#!/usr/bin/env python3
"""
WebCrawler Commander - é›†æˆæ¸¬è©¦è…³æœ¬
é©—è­‰æ ¸å¿ƒçµ„ä»¶å”åŒå·¥ä½œ

æ¸¬è©¦ç¯„åœï¼š
- é…ç½®ç®¡ç†å™¨åˆå§‹åŒ–
- æ—¥èªŒæœå‹™åŠŸèƒ½
- çˆ¬èŸ²å¼•æ“åŸºæœ¬çˆ¬å–åŠŸèƒ½
- å…¨ç³»çµ±å”åŒæ¸¬è©¦

ä½œè€…: Jerryé–‹ç™¼å·¥ä½œå®¤
ç‰ˆæœ¬: v1.0.0
"""

import os
import sys
import asyncio
import time
from pathlib import Path

# æ·»åŠ backendç›®éŒ„åˆ°Pythonè·¯å¾‘
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'backend'))

from utils.config_manager import init_config_manager, get_config_manager
from utils.logger_service import init_logger_service, get_logger
from services.crawler_engine import (
    IntelligentCrawler,
    CrawlConfig,
    AuthType,
    RequestManager,
    ResponseHandler
)


async def test_config_manager():
    """æ¸¬è©¦é…ç½®ç®¡ç†å™¨"""
    print("ğŸ”§ æ¸¬è©¦é…ç½®ç®¡ç†å™¨...")

    try:
        # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        config_manager = init_config_manager(
            config_dir="backend/config",
            environment="development",
            enable_hot_reload=False  # æ¸¬è©¦æœŸé–“ç¦ç”¨ç†±é‡è¼‰
        )

        # æ¸¬è©¦åŸºæœ¬é…ç½®ç²å–
        app_name = config_manager.get("app_name")
        assert app_name == "WebCrawler Commander", f"æœŸæœ› 'WebCrawler Commander', ç²å¾— {app_name}"

        server_port = config_manager.get("server.port")
        assert server_port == 8000, f"æœŸæœ› 8000, ç²å¾— {server_port}"

        # æ¸¬è©¦é…ç½®çµ±è¨ˆ
        stats = config_manager.get_config_stats()
        assert "load_count" in stats, "çµ±è¨ˆä¿¡æ¯ä¸å®Œæ•´"

        print("âœ… é…ç½®ç®¡ç†å™¨æ¸¬è©¦é€šé")
        return True

    except Exception as e:
        print(f"âŒ é…ç½®ç®¡ç†å™¨æ¸¬è©¦å¤±æ•—: {e}")
        return False


async def test_logger_service():
    """æ¸¬è©¦æ—¥èªŒæœå‹™"""
    print("ğŸ“ æ¸¬è©¦æ—¥èªŒæœå‹™...")

    try:
        # æ—¥èªŒæœå‹™å·²ç¶“åœ¨é…ç½®ç®¡ç†å™¨ä¸­åˆå§‹åŒ–
        logger = get_logger(__name__)

        # æ¸¬è©¦ä¸åŒç´šåˆ¥çš„æ—¥èªŒ
        logger.info("æ¸¬è©¦ä¿¡æ¯ç´šåˆ¥æ—¥èªŒ")
        logger.debug("æ¸¬è©¦èª¿è©¦ç´šåˆ¥æ—¥èªŒ", test_key="test_value")
        logger.warning("æ¸¬è©¦è­¦å‘Šç´šåˆ¥æ—¥èªŒ", warning_code=123)
        logger.error("æ¸¬è©¦éŒ¯èª¤ç´šåˆ¥æ—¥èªŒ", exception_details={"type": "ValueError", "message": "æ¸¬è©¦éŒ¯èª¤"})

        # æ¸¬è©¦è«‹æ±‚ä¸Šä¸‹æ–‡
        from utils.logger_service import set_request_context, clear_request_context
        set_request_context(request_id="test-req-001", user_id="test-user-001")
        logger.info("æ¸¬è©¦è«‹æ±‚ä¸Šä¸‹æ–‡æ—¥èªŒ", action="crawl", target="example.com")
        clear_request_context()

        print("âœ… æ—¥èªŒæœå‹™æ¸¬è©¦é€šé")
        return True

    except Exception as e:
        print(f"âŒ æ—¥èªŒæœå‹™æ¸¬è©¦å¤±æ•—: {e}")
        return False


async def test_crawler_engine():
    """æ¸¬è©¦çˆ¬èŸ²å¼•æ“"""
    print("ğŸ•·ï¸ æ¸¬è©¦çˆ¬èŸ²å¼•æ“...")

    try:
        # åˆå§‹åŒ–çˆ¬èŸ²å¼•æ“
        crawler = IntelligentCrawler()

        # æ¸¬è©¦åŸºæœ¬HTTP GETè«‹æ±‚
        config = CrawlConfig(
            url="https://httpbin.org/get",
            method="GET",
            timeout=10.0,
            max_retries=2,
            delay_between_requests=0.5,  # æ¸¬è©¦æ™‚ç¸®çŸ­å»¶é²
            respect_robots_txt=False  # httpbin.orgæ²’æœ‰robots.txté˜»æ“‹
        )

        start_time = time.time()
        result = await crawler.crawl(config)
        end_time = time.time()

        # é©—è­‰çµæœ
        assert result.success, f"çˆ¬å–å¤±æ•—: {result.error_message}"
        assert result.status_code == 200, f"æœŸæœ›ç‹€æ…‹ç¢¼200, ç²å¾— {result.status_code}"
        assert "httpbin.org" in result.url, f"URLä¸æ­£ç¢º: {result.url}"
        assert result.response_time > 0, "éŸ¿æ‡‰æ™‚é–“ä¸æ‡‰ç‚º0"
        assert result.response_time < 15.0, f"éŸ¿æ‡‰æ™‚é–“éé•·: {result.response_time}s"
        assert "headers" in result.request_headers, "è«‹æ±‚é ­ä¿¡æ¯ç¼ºå¤±"

        print(f"è«‹æ±‚æˆåŠŸã€‚ç‹€æ…‹ç¢¼: {result.status_code}, éŸ¿æ‡‰æ™‚é–“: {result.response_time:.2f}s")
        # æ¸¬è©¦User-Agentè¼ªæ›
        ua_pool = crawler.ua_pool
        ua1 = ua_pool.get_random()
        ua2 = ua_pool.get_random()
        assert len(ua1) > 10, "User-AgentåçŸ­"
        assert ua1 != ua2 or True, "User-Agentæ‡‰è©²æœ‰ä¸€å®šéš¨æ©Ÿæ€§"

        print("âœ… çˆ¬èŸ²å¼•æ“æ¸¬è©¦é€šé")
        return True

    except Exception as e:
        print(f"âŒ çˆ¬èŸ²å¼•æ“æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

    finally:
        # æ¸…ç†è³‡æº
        if 'crawler' in locals():
            await crawler.close()


async def test_request_manager():
    """æ¸¬è©¦è«‹æ±‚ç®¡ç†å™¨"""
    print("ğŸ“‹ æ¸¬è©¦è«‹æ±‚ç®¡ç†å™¨...")

    try:
        crawler = IntelligentCrawler()
        manager = RequestManager(crawler)

        # å•Ÿå‹•è™•ç†å™¨
        await manager.start_processing(num_workers=2)

        # å‰µå»ºå¤šå€‹è«‹æ±‚
        configs = [
            CrawlConfig(url="https://httpbin.org/delay/1", delay_between_requests=0.1),
            CrawlConfig(url="https://httpbin.org/status/200", delay_between_requests=0.1),
            CrawlConfig(url="https://httpbin.org/user-agent", delay_between_requests=0.1),
        ]

        # æäº¤è«‹æ±‚
        futures = []
        for config in configs:
            future = await manager.submit_request(config)
            futures.append(future)

        # ç­‰å¾…çµæœ
        results = []
        for future in futures:
            try:
                result = await asyncio.wait_for(future, timeout=30.0)
                results.append(result)
                assert result.success, f"è«‹æ±‚å¤±æ•—: {result.error_message}"
            except Exception as e:
                print(f"è«‹æ±‚ç•°å¸¸: {e}")
                results.append(None)

        # æ¸…ç†
        await manager.stop_processing()
        await crawler.close()

        # çµ±è¨ˆ
        success_count = sum(1 for r in results if r and r.success)
        print(f"âœ… è«‹æ±‚ç®¡ç†å™¨æ¸¬è©¦é€šé ({success_count}/{len(configs)} æˆåŠŸ)")

        return success_count > len(configs) // 2  # è‡³å°‘ä¸€åŠæˆåŠŸ

    except Exception as e:
        print(f"âŒ è«‹æ±‚ç®¡ç†å™¨æ¸¬è©¦å¤±æ•—: {e}")
        return False


async def test_response_handler():
    """æ¸¬è©¦éŸ¿æ‡‰è™•ç†å™¨"""
    print("ğŸ”„ æ¸¬è©¦éŸ¿æ‡‰è™•ç†å™¨...")

    try:
        from backend.services.crawler_engine import CrawlResult

        handler = ResponseHandler()

        # å‰µå»ºæ¸¬è©¦éŸ¿æ‡‰
        mock_result = CrawlResult(
            url="https://example.com",
            status_code=200,
            headers={"content-type": "text/html"},
            content=b"<html><head><title>Test Page</title></head><body><h1>Hello World</h1></body></html>",
            text="<html><head><title>Test Page</title></head><body><h1>Hello World</h1></body></html>",
            response_time=0.5,
            success=True
        )

        # æ¸¬è©¦HTMLæ•¸æ“šæå–
        extractors = [
            {
                "name": "page_title",
                "type": "css",
                "selector": "title"
            },
            {
                "name": "heading",
                "type": "css",
                "selector": "h1"
            }
        ]

        processed_data = await handler.process_response(mock_result, extractors)

        assert processed_data["success"], "éŸ¿æ‡‰è™•ç†æ¨™è¨˜ç‚ºå¤±æ•—"
        assert processed_data["status_code"] == 200, "ç‹€æ…‹ç¢¼ä¸æ­£ç¢º"
        assert "html" in processed_data["extracted_data"], "ç¼ºå°‘HTMLæ•¸æ“š"
        assert "page_title" in processed_data["extracted_data"], "ç¼ºå°‘é é¢æ¨™é¡Œæå–"
        assert "heading" in processed_data["extracted_data"], "ç¼ºå°‘æ¨™é¡Œæå–"

        # æª¢æŸ¥æå–çµæœ
        title_data = processed_data["extracted_data"]["page_title"]
        assert isinstance(title_data, list), "æ¨™é¡Œæ‡‰è©²æ˜¯åˆ—è¡¨"
        assert "Test Page" in title_data[0], "æ¨™é¡Œå…§å®¹ä¸æ­£ç¢º"

        print("âœ… éŸ¿æ‡‰è™•ç†å™¨æ¸¬è©¦é€šé")
        return True

    except Exception as e:
        print(f"âŒ éŸ¿æ‡‰è™•ç†å™¨æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_full_integration():
    """å®Œæ•´é›†æˆæ¸¬è©¦"""
    print("ğŸ”— æ¸¬è©¦å…¨ç³»çµ±é›†æˆ...")

    try:
        # 1. åˆå§‹åŒ–æ‰€æœ‰æœå‹™ (åœ¨main.pyä¸­å®Œæˆ)
        logger = get_logger(__name__)
        logger.info("é–‹å§‹å…¨ç³»çµ±é›†æˆæ¸¬è©¦")

        # 2. æ¸¬è©¦çˆ¬èŸ²å·¥ä½œæµ
        crawler = IntelligentCrawler()

        # å‰µå»ºå¤šå€‹çˆ¬å–ä»»å‹™
        tasks = [
            CrawlConfig(
                url="https://httpbin.org/json",
                respect_robots_txt=False,
                delay_between_requests=0.2
            ),
            CrawlConfig(
                url="https://httpbin.org/html",
                respect_robots_txt=False,
                delay_between_requests=0.2
            ),
        ]

        # ä¸¦ç™¼åŸ·è¡Œä»»å‹™
        results = await asyncio.gather(
            *(crawler.crawl(config) for config in tasks),
            return_exceptions=True
        )

        success_count = sum(1 for r in results if hasattr(r, 'success') and r.success)

        await crawler.close()

        # 3. æ¸¬è©¦çµ±è¨ˆä¿¡æ¯
        config_manager = get_config_manager()
        stats = config_manager.get_config_stats()
        logger.info("ç³»çµ±çµ±è¨ˆ", **stats)

        print(f"âœ… å…¨ç³»çµ±é›†æˆæ¸¬è©¦é€šé ({success_count}/{len(tasks)} æˆåŠŸ)")
        return success_count > 0

    except Exception as e:
        print(f"âŒ å…¨ç³»çµ±é›†æˆæ¸¬è©¦å¤±æ•—: {e}")
        return False


async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸš€ WebCrawler Commander - é›†æˆæ¸¬è©¦é–‹å§‹")
    print("=" * 50)

    # è¨­ç½®æ¸¬è©¦ç’°å¢ƒ
    os.environ["WEBCRAWLER_ENV"] = "testing"

    # åˆå§‹åŒ–æ ¸å¿ƒæœå‹™
    try:
        init_config_manager(
            config_dir="backend/config",
            environment="testing",
            enable_hot_reload=False
        )
        init_logger_service()
        print("âœ… æ ¸å¿ƒæœå‹™åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ ¸å¿ƒæœå‹™åˆå§‹åŒ–å¤±æ•—: {e}")
        return

    # åŸ·è¡Œæ¸¬è©¦
    tests = [
        ("é…ç½®ç®¡ç†å™¨", test_config_manager),
        ("æ—¥èªŒæœå‹™", test_logger_service),
        ("çˆ¬èŸ²å¼•æ“", test_crawler_engine),
        ("è«‹æ±‚ç®¡ç†å™¨", test_request_manager),
        ("éŸ¿æ‡‰è™•ç†å™¨", test_response_handler),
        ("å…¨ç³»çµ±é›†æˆ", test_full_integration),
    ]

    passed = 0
    total = len(tests)

    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name}æ¸¬è©¦ {'='*20}")
        try:
            result = await test_func()
            if result:
                passed += 1
            else:
                print(f"âš ï¸  {test_name}æ¸¬è©¦éƒ¨åˆ†å¤±æ•—")
        except Exception as e:
            print(f"âŒ {test_name}æ¸¬è©¦æ‹‹å‡ºç•°å¸¸: {e}")

    print("\n" + "=" * 50)
    print(f"ğŸ¯ æ¸¬è©¦å®Œæˆ: {passed}/{total} é€šé")

    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼ç³»çµ±æº–å‚™å°±ç·’ã€‚")
        return True
    elif passed >= total * 0.8:  # 80%ä»¥ä¸Šé€šé
        print("âš ï¸ å¤§éƒ¨åˆ†æ¸¬è©¦é€šéï¼Œç³»çµ±åŸºæœ¬å¯ç”¨ã€‚")
        return True
    else:
        print("âŒ æ¸¬è©¦å¤±æ•—ç‡éé«˜ï¼Œéœ€è¦æª¢æŸ¥ç³»çµ±é…ç½®ã€‚")
        return False


if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ¶ä¸­æ–·æ¸¬è©¦")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
