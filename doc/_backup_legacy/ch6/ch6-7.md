<!-- LEGACY FILE NOTICE -->
> âš ï¸ æ­¤æª”æ¡ˆç‚ºèˆŠç‰ˆå‚™ä»½ï¼Œå·²è¢«æ–°æª”å–ä»£ï¼š [ch6-7-æ€§èƒ½ä¼˜åŒ–ç­–ç•¥.md](ch6-7-æ€§èƒ½ä¼˜åŒ–ç­–ç•¥.md)\n> å‚™ä»½æ™‚é–“ï¼š2025-10-31 12:28:26\n
---

**[â† è¿”å›ç¬¬6ç« é¦–é ](ch6-index.md)**

---

### 6.7 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

#### 6.7.1 LLMè°ƒç”¨ä¼˜åŒ–

1. **ç¼“å­˜æœºåˆ¶**
   ```python
   class LLMCachingClient:
       """å¸¦ç¼“å­˜çš„LLMå®¢æˆ·ç«¯"""
       
       def __init__(self, llm_client, cache_ttl=3600):
           self.llm_client = llm_client
           self.cache = TTLCache(maxsize=1000, ttl=cache_ttl)
           self.logger = logging.getLogger(__name__)
       
       def generate(self, prompt: str) -> str:
           """ç”Ÿæˆæ–‡æœ¬ï¼Œä½¿ç”¨ç¼“å­˜"""
           # ç”Ÿæˆç¼“å­˜é”®ï¼ˆæç¤ºè¯çš„å“ˆå¸Œï¼‰
           cache_key = self._generate_cache_key(prompt)
           
           # æ£€æŸ¥ç¼“å­˜
           if cache_key in self.cache:
               self.logger.info("LLM response from cache")
               return self.cache[cache_key]
           
           # è°ƒç”¨LLM
           start_time = time.time()
           response = self.llm_client.generate(prompt)
           duration = time.time() - start_time
           
           # è®°å½•æŒ‡æ ‡
           self.logger.info("LLM call completed in %.2f seconds", duration)
           
           # ç¼“å­˜ç»“æœ
           self.cache[cache_key] = response
           
           return response
       
       def _generate_cache_key(self, prompt: str) -> str:
           """ç”Ÿæˆç¼“å­˜é”®"""
           return hashlib.md5(prompt.encode('utf-8')).hexdigest()
   ```

2. **æç¤ºè¯ä¼˜åŒ–**
   ```python
   class PromptOptimizer:
       """æç¤ºè¯ä¼˜åŒ–å™¨ï¼Œå‡å°‘tokenä½¿ç”¨"""
       
       def optimize(self, prompt: str) -> str:
           """ä¼˜åŒ–æç¤ºè¯"""
           # 1. ç§»é™¤å†—ä½™ç©ºæ ¼å’Œæ¢è¡Œ
           optimized = re.sub(r'\s+', ' ', prompt).strip()
           
           # 2. ç¼©çŸ­å¸¸è§çŸ­è¯­
           replacements = {
               "please": "plz",
               "information": "info",
               "approximately": "approx",
               "example": "ex",
               "solution": "sol"
           }
           
           for old, new in replacements.items():
               optimized = re.sub(r'\b' + old + r'\b', new, optimized, flags=re.IGNORECASE)
           
           # 3. æˆªæ–­è¿‡é•¿çš„éƒ¨åˆ†
           if len(optimized) > 2000:
               # ä¿ç•™å¼€å¤´å’Œç»“å°¾
               optimized = optimized[:1000] + "...[TRUNCATED]..." + optimized[-1000:]
           
           return optimized
   ```

3. **æ‰¹å¤„ç†è¯·æ±‚**
   ```python
   class BatchLLMClient:
       """æ‰¹å¤„ç†LLMå®¢æˆ·ç«¯"""
       
       def __init__(self, llm_client, batch_size=5, max_wait=2.0):
           self.llm_client = llm_client
           self.batch_size = batch_size
           self.max_wait = max_wait
           self.request_queue = []
           self.lock = threading.Lock()
           self.thread = threading.Thread(target=self._process_queue, daemon=True)
           self.thread.start()
       
       def _process_queue(self):
           """å¤„ç†è¯·æ±‚é˜Ÿåˆ—"""
           while True:
               with self.lock:
                   if len(self.request_queue) >= self.batch_size or (self.request_queue and time.time() - self.request_queue[0][2] > self.max_wait):
                       batch = self.request_queue[:self.batch_size]
                       self.request_queue = self.request_queue[self.batch_size:]
                   else:
                       batch = None
               
               if batch:
                   self._process_batch(batch)
               
               time.sleep(0.1)
       
       def _process_batch(self, batch):
           """å¤„ç†ä¸€æ‰¹è¯·æ±‚"""
           prompts = [item[0] for item in batch]
           callbacks = [item[1] for item in batch]
           
           try:
               # è°ƒç”¨LLMå¤„ç†æ‰¹é‡è¯·æ±‚
               responses = self.llm_client.generate_batch(prompts)
               
               # è°ƒç”¨å›è°ƒ
               for callback, response in zip(callbacks, responses):
                   callback(response)
           except Exception as e:
               for callback in callbacks:
                   callback(None, str(e))
       
       def generate(self, prompt: str, callback: Callable):
           """å¼‚æ­¥ç”Ÿæˆæ–‡æœ¬"""
           with self.lock:
               self.request_queue.append((prompt, callback, time.time()))
   ```

#### 6.7.2 ä¸Šä¸‹æ–‡ç®¡ç†ä¼˜åŒ–

1. **ä¸Šä¸‹æ–‡å‹ç¼©**
   ```python
   class ContextCompressor:
       """ä¸Šä¸‹æ–‡å‹ç¼©å™¨ï¼Œå‡å°‘ä¸Šä¸‹æ–‡tokenæ•°é‡"""
       
       def compress(self, context: Dict, max_tokens: int = 2000) -> Dict:
           """
           å‹ç¼©ä¸Šä¸‹æ–‡åˆ°æŒ‡å®štokené™åˆ¶
           
           :param context: åŸå§‹ä¸Šä¸‹æ–‡
           :param max_tokens: æœ€å¤§tokenæ•°
           :return: å‹ç¼©åçš„ä¸Šä¸‹æ–‡
           """
           # 1. è®¡ç®—å½“å‰tokenæ•°
           current_tokens = self._estimate_tokens(context)
           
           # 2. å¦‚æœä¸éœ€è¦å‹ç¼©ï¼Œç›´æ¥è¿”å›
           if current_tokens <= max_tokens:
               return context
           
           # 3. æŒ‰é‡è¦æ€§æ’åº
           important_keys = ["error_log", "user_request", "recent_messages"]
           less_important_keys = [k for k in context.keys() if k not in important_keys]
           
           # 4. ä¼˜å…ˆä¿ç•™é‡è¦ä¿¡æ¯
           compressed = {k: context[k] for k in important_keys if k in context}
           
           # 5. é€æ­¥æ·»åŠ æ¬¡è¦ä¿¡æ¯ç›´åˆ°è¾¾åˆ°tokené™åˆ¶
           remaining_tokens = max_tokens - self._estimate_tokens(compressed)
           
           for key in less_important_keys:
               if remaining_tokens <= 0:
                   break
               
               # å‹ç¼©å•ä¸ªå­—æ®µ
               compressed_value = self._compress_field(context[key], remaining_tokens)
               compressed[key] = compressed_value
               
               # æ›´æ–°å‰©ä½™token
               remaining_tokens -= self._estimate_tokens({key: compressed_value})
           
           return compressed
       
       def _compress_field(self, value: Any, max_tokens: int) -> Any:
           """å‹ç¼©å•ä¸ªå­—æ®µ"""
           if isinstance(value, str):
               # ç®€å•å®ç°ï¼šæˆªæ–­å­—ç¬¦ä¸²
               tokens = self._estimate_tokens(value)
               if tokens > max_tokens:
                   # ä¿ç•™å¼€å¤´å’Œç»“å°¾
                   return value[:max_tokens//2] + "...[TRUNCATED]..." + value[-max_tokens//2:]
               return value
           
           elif isinstance(value, list):
               # ä¿ç•™å‰Nä¸ªå…ƒç´ 
               if len(value) > 5:
                   return value[:5]
               return value
           
           elif isinstance(value, dict):
               # ä¿ç•™æœ€é‡è¦çš„å­—æ®µ
               important_fields = ["root_cause", "solutions", "error_message"]
               return {k: v for k, v in value.items() if k in important_fields}
           
           return value
       
       def _estimate_tokens(self, obj: Any) -> int:
           """ä¼°è®¡å¯¹è±¡çš„tokenæ•°é‡"""
           if isinstance(obj, str):
               # ç®€å•ä¼°è®¡ï¼šæ¯ä¸ªå­—ç¬¦çº¦0.25ä¸ªtoken
               return max(1, len(obj) // 4)
           elif isinstance(obj, list):
               return sum(self._estimate_tokens(item) for item in obj)
           elif isinstance(obj, dict):
               return sum(self._estimate_tokens(v) for v in obj.values())
           return 1
   ```

2. **ä¸Šä¸‹æ–‡æ‘˜è¦**
   ```python
   class ContextSummarizer:
       """ä¸Šä¸‹æ–‡æ‘˜è¦ç”Ÿæˆå™¨"""
       
       def __init__(self, llm_client):
           self.llm_client = llm_client
       
       def summarize(self, context: Dict) -> str:
           """
           ç”Ÿæˆä¸Šä¸‹æ–‡æ‘˜è¦
           
           :param context: åŸå§‹ä¸Šä¸‹æ–‡
           :return: ä¸Šä¸‹æ–‡æ‘˜è¦
           """
           # æ„å»ºæ‘˜è¦æç¤ºè¯
           prompt = f"""
           è¯·å°†ä»¥ä¸‹å¯¹è¯ä¸Šä¸‹æ–‡æ€»ç»“ä¸ºç®€æ´çš„æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ï¼Œä¸è¶…è¿‡100ä¸ªè¯ï¼š

           {json.dumps(context, indent=2)}

           æ‘˜è¦:
           """
           
           # ç”Ÿæˆæ‘˜è¦
           summary = self.llm_client.generate(prompt)
           
           # æ¸…ç†ç»“æœ
           return summary.strip()
   ```

#### 6.7.3 èµ„æºç®¡ç†ç­–ç•¥

1. **èµ„æºé…é¢ç®¡ç†**
   ```python
   class ResourceQuotaManager:
       """èµ„æºé…é¢ç®¡ç†å™¨"""
       
       def __init__(self, db: Database, config: Config):
           self.db = db
           self.config = config
           self.logger = logging.getLogger(__name__)
           self.quota_cache = TTLCache(maxsize=1000, ttl=300)  # 5åˆ†é’Ÿç¼“å­˜
       
       def check_quota(
           self,
           user_id: str,
           resource_type: str,
           amount: int
       ) -> Tuple[bool, str]:
           """
           æ£€æŸ¥èµ„æºé…é¢
           
           :param user_id: ç”¨æˆ·ID
           :param resource_type: èµ„æºç±»å‹ (llm_calls, processing_timeç­‰)
           :param amount: è¯·æ±‚çš„èµ„æºé‡
           :return: (æ˜¯å¦å…è®¸, æ¶ˆæ¯)
           """
           # 1. è·å–ç”¨æˆ·é…é¢
           quota = self._get_user_quota(user_id)
           
           # 2. è·å–å·²ç”¨èµ„æº
           used = self._get_used_resources(user_id, resource_type)
           
           # 3. æ£€æŸ¥æ˜¯å¦è¶…å‡ºé…é¢
           if used + amount > quota[resource_type]:
               return False, f"è¶…å‡º{resource_type}é…é¢ ({used}/{quota[resource_type]})"
           
           # 4. é¢„æ‰£èµ„æº
           self._reserve_resources(user_id, resource_type, amount)
           
           return True, f"å·²é¢„ç•™{amount}å•ä½{resource_type}"
       
       def _get_user_quota(self, user_id: str) -> Dict:
           """è·å–ç”¨æˆ·é…é¢"""
           # ä»ç¼“å­˜è·å–
           cache_key = f"{user_id}:quota"
           if cache_key in self.quota_cache:
               return self.quota_cache[cache_key]
           
           # ä»æ•°æ®åº“è·å–
           sql = """
           SELECT llm_calls, processing_time, storage 
           FROM user_quotas 
           WHERE user_id = %(user_id)s
           """
           row = self.db.fetchone(sql, {"user_id": user_id})
           
           if not row:
               # é»˜è®¤é…é¢
               quota = {
                   "llm_calls": self.config.default_llm_calls,
                   "processing_time": self.config.default_processing_time,
                   "storage": self.config.default_storage
               }
           else:
               quota = {
                   "llm_calls": row["llm_calls"],
                   "processing_time": row["processing_time"],
                   "storage": row["storage"]
               }
           
           # ç¼“å­˜ç»“æœ
           self.quota_cache[cache_key] = quota
           return quota
       
       def _get_used_resources(self, user_id: str, resource_type: str) -> int:
           """è·å–å·²ç”¨èµ„æº"""
           # å®ç°èµ„æºä½¿ç”¨ç»Ÿè®¡
           # è¿™é‡Œç®€åŒ–ä¸ºè¿”å›0
           return 0
       
       def _reserve_resources(
           self,
           user_id: str,
           resource_type: str,
           amount: int
       ):
           """é¢„æ‰£èµ„æº"""
           # å®ç°èµ„æºé¢„ç•™
           pass
   ```

---

## ğŸ“‘ ç›¸å…³ç« èŠ‚

| å‰åº | å½“å‰ | åç»­ |
|-----|------|------|
| [6.6](ch6-6.md) | **6.7** | [6.8](ch6-8.md) |

**å¿«é€Ÿé“¾æ¥ï¼š**
- [â† è¿”å›ç¬¬6ç« é¦–é ](ch6-index.md)
