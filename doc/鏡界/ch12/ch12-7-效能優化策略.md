# ç¬¬12ç« ï¼šæ•¸æ“šè³ªé‡ç®¡ç†ä¸­å¿ƒ

## 12.7 æ•ˆèƒ½å„ªåŒ–ç­–ç•¥

**[â† è¿”å›ç¬¬12ç« é¦–é ](ch12-index.md)**

---

## ğŸš€ æ€§èƒ½å„ªåŒ–æ¦‚è¦½

æ•¸æ“šè³ªé‡ç®¡ç†ä¸­å¿ƒéœ€è¦è™•ç†å¤§è¦æ¨¡æ•¸æ“šï¼Œæ€§èƒ½å„ªåŒ–è‡³é—œé‡è¦ã€‚æœ¬ç« ç¯€è©³ç´°ä»‹ç´¹å„å±¤é¢çš„å„ªåŒ–ç­–ç•¥ã€‚

```
æ€§èƒ½å„ªåŒ–ç¶­åº¦
â”œâ”€â”€ 1. æ•¸æ“šè™•ç†å„ªåŒ–
â”‚   â”œâ”€â”€ æ‰¹é‡è™•ç†å„ªåŒ–
â”‚   â”œâ”€â”€ æµå¼è™•ç†å„ªåŒ–
â”‚   â””â”€â”€ åˆ†å¸ƒå¼è¨ˆç®—
â”œâ”€â”€ 2. å­˜å„²å„ªåŒ–
â”‚   â”œâ”€â”€ æ•¸æ“šåˆ†å€
â”‚   â”œâ”€â”€ ç´¢å¼•å„ªåŒ–
â”‚   â””â”€â”€ ç·©å­˜ç­–ç•¥
â”œâ”€â”€ 3. æŸ¥è©¢å„ªåŒ–
â”‚   â”œâ”€â”€ SQL å„ªåŒ–
â”‚   â”œâ”€â”€ æ™‚åºæŸ¥è©¢å„ªåŒ–
â”‚   â””â”€â”€ èšåˆå„ªåŒ–
â”œâ”€â”€ 4. ä¸¦ç™¼å„ªåŒ–
â”‚   â”œâ”€â”€ ç•°æ­¥è™•ç†
â”‚   â”œâ”€â”€ ä»»å‹™éšŠåˆ—
â”‚   â””â”€â”€ ä¸¦è¡Œè¨ˆç®—
â””â”€â”€ 5. è³‡æºå„ªåŒ–
    â”œâ”€â”€ å…§å­˜ç®¡ç†
    â”œâ”€â”€ CPU å„ªåŒ–
    â””â”€â”€ I/O å„ªåŒ–
```

---

## 1ï¸âƒ£ æ•¸æ“šè™•ç†å„ªåŒ–

### 1.1 æ‰¹é‡è™•ç†å„ªåŒ–

#### å‹•æ…‹æ‰¹æ¬¡å¤§å°

```python
# app/quality_center/optimizer/batch_optimizer.py

class DynamicBatchProcessor:
    """å‹•æ…‹æ‰¹æ¬¡è™•ç†å™¨"""
    
    def __init__(self, initial_batch_size=1000):
        self.batch_size = initial_batch_size
        self.min_batch_size = 100
        self.max_batch_size = 10000
        self.performance_history = []
    
    def process_batches(self, data_iterator):
        """è™•ç†æ‰¹æ¬¡ï¼Œå‹•æ…‹èª¿æ•´æ‰¹æ¬¡å¤§å°"""
        for batch in self._get_batches(data_iterator):
            start_time = time.time()
            
            # è™•ç†æ‰¹æ¬¡
            result = self._process_batch(batch)
            
            # è¨˜éŒ„æ€§èƒ½
            duration = time.time() - start_time
            throughput = len(batch) / duration
            
            self.performance_history.append({
                'batch_size': len(batch),
                'duration': duration,
                'throughput': throughput
            })
            
            # å‹•æ…‹èª¿æ•´æ‰¹æ¬¡å¤§å°
            self._adjust_batch_size()
            
            yield result
    
    def _adjust_batch_size(self):
        """æ ¹æ“šæ€§èƒ½æ­·å²èª¿æ•´æ‰¹æ¬¡å¤§å°"""
        if len(self.performance_history) < 5:
            return
        
        recent_history = self.performance_history[-5:]
        avg_throughput = sum(h['throughput'] for h in recent_history) / 5
        
        # å¦‚æœååé‡ä¸‹é™ï¼Œæ¸›å°æ‰¹æ¬¡
        if avg_throughput < self.target_throughput * 0.8:
            self.batch_size = max(
                self.min_batch_size,
                int(self.batch_size * 0.8)
            )
        # å¦‚æœååé‡ç©©å®šï¼Œå˜—è©¦å¢å¤§æ‰¹æ¬¡
        elif avg_throughput > self.target_throughput * 0.95:
            self.batch_size = min(
                self.max_batch_size,
                int(self.batch_size * 1.2)
            )
        
        logger.info(f"Adjusted batch size to {self.batch_size}")
    
    def _get_batches(self, iterator):
        """ç”Ÿæˆæ‰¹æ¬¡"""
        batch = []
        for item in iterator:
            batch.append(item)
            if len(batch) >= self.batch_size:
                yield batch
                batch = []
        
        if batch:
            yield batch
```

#### æ‰¹é‡ SQL æ“ä½œ

```python
# å„ªåŒ–å‰ï¼šé€æ¢æ’å…¥
for record in records:
    db.execute(
        "INSERT INTO quality_checks (data_source, score) VALUES (%s, %s)",
        (record['source'], record['score'])
    )

# å„ªåŒ–å¾Œï¼šæ‰¹é‡æ’å…¥
def batch_insert(records, batch_size=1000):
    """æ‰¹é‡æ’å…¥å„ªåŒ–"""
    from psycopg2.extras import execute_values
    
    conn = get_db_connection()
    cursor = conn.cursor()
    
    values = [
        (r['source'], r['score']) 
        for r in records
    ]
    
    execute_values(
        cursor,
        "INSERT INTO quality_checks (data_source, score) VALUES %s",
        values,
        page_size=batch_size
    )
    
    conn.commit()
```

### 1.2 æµå¼è™•ç†å„ªåŒ–

#### Flink æµå¼è³ªé‡æª¢æŸ¥å„ªåŒ–

```python
# pyflink_quality_checker.py

from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.window import TumblingProcessingTimeWindows
from pyflink.common import Time

def create_optimized_stream_pipeline():
    """å‰µå»ºå„ªåŒ–çš„æµå¼è™•ç†ç®¡é“"""
    env = StreamExecutionEnvironment.get_execution_environment()
    
    # 1. ä¸¦è¡Œåº¦å„ªåŒ–
    env.set_parallelism(8)  # æ ¹æ“š CPU æ ¸å¿ƒæ•¸è¨­ç½®
    
    # 2. æª¢æŸ¥é»å„ªåŒ–
    env.enable_checkpointing(60000)  # æ¯åˆ†é˜æª¢æŸ¥é»
    env.get_checkpoint_config().set_min_pause_between_checkpoints(30000)
    env.get_checkpoint_config().set_checkpoint_timeout(300000)
    
    # 3. æ•¸æ“šæµè™•ç†
    data_stream = (env
        .add_source(kafka_source)
        # è¨­ç½®æ°´å°ç­–ç•¥
        .assign_timestamps_and_watermarks(
            WatermarkStrategy
                .for_bounded_out_of_orderness(Duration.of_seconds(10))
                .with_timestamp_assigner(lambda e: e['timestamp'])
        )
        # éµæ§åˆ†å€ï¼ˆæé«˜ä¸¦è¡Œåº¦ï¼‰
        .key_by(lambda x: x['data_source'])
        # çª—å£æ“ä½œ
        .window(TumblingProcessingTimeWindows.of(Time.minutes(5)))
        # èšåˆè¨ˆç®—
        .aggregate(QualityAggregateFunction())
        # è³ªé‡æª¢æŸ¥
        .process(QualityCheckProcessFunction())
    )
    
    return data_stream


class QualityAggregateFunction(AggregateFunction):
    """å„ªåŒ–çš„èšåˆå‡½æ•¸"""
    
    def create_accumulator(self):
        return {
            'count': 0,
            'valid': 0,
            'invalid': 0,
            'null_count': 0
        }
    
    def add(self, value, accumulator):
        accumulator['count'] += 1
        if self.is_valid(value):
            accumulator['valid'] += 1
        else:
            accumulator['invalid'] += 1
        
        accumulator['null_count'] += self.count_nulls(value)
        return accumulator
    
    def get_result(self, accumulator):
        return {
            'total': accumulator['count'],
            'valid_rate': accumulator['valid'] / accumulator['count'],
            'null_rate': accumulator['null_count'] / (
                accumulator['count'] * expected_fields
            )
        }
    
    def merge(self, acc1, acc2):
        return {
            'count': acc1['count'] + acc2['count'],
            'valid': acc1['valid'] + acc2['valid'],
            'invalid': acc1['invalid'] + acc2['invalid'],
            'null_count': acc1['null_count'] + acc2['null_count']
        }
```

### 1.3 åˆ†å¸ƒå¼è¨ˆç®—å„ªåŒ–

#### Spark å„ªåŒ–é…ç½®

```python
# spark_quality_processor.py

from pyspark.sql import SparkSession
from pyspark.sql.functions import *

def create_optimized_spark_session():
    """å‰µå»ºå„ªåŒ–çš„ Spark æœƒè©±"""
    return (SparkSession.builder
        .appName("QualityManagement")
        # å…§å­˜é…ç½®
        .config("spark.executor.memory", "8g")
        .config("spark.driver.memory", "4g")
        .config("spark.memory.fraction", "0.8")  # 80% ç”¨æ–¼åŸ·è¡Œå’Œå­˜å„²
        .config("spark.memory.storageFraction", "0.3")  # 30% ç”¨æ–¼ç·©å­˜
        # ä¸¦è¡Œåº¦é…ç½®
        .config("spark.default.parallelism", "200")
        .config("spark.sql.shuffle.partitions", "200")
        # åºåˆ—åŒ–å„ªåŒ–
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
        .config("spark.kryoserializer.buffer.max", "512m")
        # å»£æ’­å„ªåŒ–
        .config("spark.sql.autoBroadcastJoinThreshold", "50MB")
        # å‹•æ…‹åˆ†é…
        .config("spark.dynamicAllocation.enabled", "true")
        .config("spark.dynamicAllocation.minExecutors", "2")
        .config("spark.dynamicAllocation.maxExecutors", "20")
        # å£“ç¸®
        .config("spark.sql.inMemoryColumnarStorage.compressed", "true")
        .config("spark.rdd.compress", "true")
        .getOrCreate()
    )


def optimized_quality_check(spark, data_path):
    """å„ªåŒ–çš„è³ªé‡æª¢æŸ¥è™•ç†"""
    
    # 1. è®€å–æ•¸æ“šï¼ˆä½¿ç”¨åˆ†å€ï¼‰
    df = (spark.read
        .format("parquet")
        .option("mergeSchema", "false")  # é¿å… schema åˆä½µé–‹éŠ·
        .load(data_path)
    )
    
    # 2. ç·©å­˜ç†±æ•¸æ“š
    df.cache()
    
    # 3. åˆ†å€å„ªåŒ–
    df = df.repartition(200, "data_source")  # æ ¹æ“šæ•¸æ“šæºé‡åˆ†å€
    
    # 4. è³ªé‡æª¢æŸ¥ï¼ˆå‘é‡åŒ–æ“ä½œï¼‰
    quality_df = df.select(
        "data_source",
        "record_id",
        # å®Œæ•´æ€§æª¢æŸ¥
        (
            col("field1").isNotNull().cast("int") +
            col("field2").isNotNull().cast("int") +
            col("field3").isNotNull().cast("int")
        ).alias("completeness_score"),
        # æ ¼å¼æª¢æŸ¥
        when(col("email").rlike(r"^[\w\.-]+@[\w\.-]+\.\w+$"), 1)
            .otherwise(0).alias("email_valid"),
        # ç¯„åœæª¢æŸ¥
        when((col("age") >= 0) & (col("age") <= 150), 1)
            .otherwise(0).alias("age_valid")
    )
    
    # 5. èšåˆçµ±è¨ˆï¼ˆä½¿ç”¨ Catalyst å„ªåŒ–å™¨ï¼‰
    stats = quality_df.groupBy("data_source").agg(
        count("*").alias("total_records"),
        avg("completeness_score").alias("avg_completeness"),
        sum("email_valid").alias("valid_emails"),
        sum("age_valid").alias("valid_ages")
    )
    
    # 6. å¯«å…¥çµæœï¼ˆåˆ†å€å¯«å…¥ï¼‰
    stats.write \
        .mode("overwrite") \
        .partitionBy("data_source") \
        .parquet("output/quality_stats")
    
    return stats
```

---

## 2ï¸âƒ£ å­˜å„²å„ªåŒ–

### 2.1 æ•¸æ“šåˆ†å€ç­–ç•¥

#### PostgreSQL åˆ†å€è¡¨

```sql
-- æŒ‰æ™‚é–“åˆ†å€ï¼ˆç¯„åœåˆ†å€ï¼‰
CREATE TABLE quality_checks (
    id BIGSERIAL,
    check_id UUID,
    data_source VARCHAR(200),
    started_at TIMESTAMP NOT NULL,
    quality_score NUMERIC(5, 2),
    -- other columns...
    PRIMARY KEY (id, started_at)
) PARTITION BY RANGE (started_at);

-- å‰µå»ºæœˆåº¦åˆ†å€
CREATE TABLE quality_checks_y2025m01 PARTITION OF quality_checks
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

CREATE TABLE quality_checks_y2025m02 PARTITION OF quality_checks
    FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');

-- è‡ªå‹•å‰µå»ºåˆ†å€çš„å‡½æ•¸
CREATE OR REPLACE FUNCTION create_monthly_partitions(
    start_date DATE,
    end_date DATE
)
RETURNS VOID AS $$
DECLARE
    current_date DATE := start_date;
    next_date DATE;
    partition_name TEXT;
BEGIN
    WHILE current_date < end_date LOOP
        next_date := current_date + INTERVAL '1 month';
        partition_name := 'quality_checks_y' || 
                         TO_CHAR(current_date, 'YYYY') || 'm' || 
                         TO_CHAR(current_date, 'MM');
        
        EXECUTE format(
            'CREATE TABLE IF NOT EXISTS %I PARTITION OF quality_checks
             FOR VALUES FROM (%L) TO (%L)',
            partition_name,
            current_date,
            next_date
        );
        
        current_date := next_date;
    END LOOP;
END;
$$ LANGUAGE plpgsql;

-- å‰µå»ºæœªä¾†12å€‹æœˆçš„åˆ†å€
SELECT create_monthly_partitions(CURRENT_DATE, CURRENT_DATE + INTERVAL '12 months');


-- æŒ‰æ•¸æ“šæºå’Œæ™‚é–“çµ„åˆåˆ†å€ï¼ˆåˆ—è¡¨+ç¯„åœï¼‰
CREATE TABLE anomaly_records (
    id BIGSERIAL,
    anomaly_id UUID,
    data_source VARCHAR(200) NOT NULL,
    detected_at TIMESTAMP NOT NULL,
    -- other columns...
    PRIMARY KEY (id, data_source, detected_at)
) PARTITION BY LIST (data_source);

-- ç‚ºæ¯å€‹æ•¸æ“šæºå‰µå»ºå­åˆ†å€
CREATE TABLE anomaly_records_ch8_crawler PARTITION OF anomaly_records
    FOR VALUES IN ('ch8_crawler')
    PARTITION BY RANGE (detected_at);

CREATE TABLE anomaly_records_ch8_crawler_2025m01 
    PARTITION OF anomaly_records_ch8_crawler
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');
```

### 2.2 ç´¢å¼•å„ªåŒ–

#### çµ„åˆç´¢å¼•ç­–ç•¥

```sql
-- 1. å¸¸ç”¨æŸ¥è©¢æ¨¡å¼åˆ†æ
-- æŸ¥è©¢1ï¼šæŒ‰æ•¸æ“šæºå’Œæ™‚é–“ç¯„åœæŸ¥è©¢
SELECT * FROM quality_checks 
WHERE data_source = 'ch8_crawler' 
  AND started_at BETWEEN '2025-10-01' AND '2025-10-31';

-- å‰µå»ºçµ„åˆç´¢å¼•ï¼ˆé †åºå¾ˆé‡è¦ï¼ï¼‰
CREATE INDEX idx_quality_checks_source_time 
ON quality_checks (data_source, started_at DESC);

-- æŸ¥è©¢2ï¼šæŒ‰è³ªé‡åˆ†æ•¸éæ¿¾
SELECT * FROM quality_checks 
WHERE quality_score < 80 
ORDER BY started_at DESC;

-- å‰µå»ºéƒ¨åˆ†ç´¢å¼•ï¼ˆåªç´¢å¼•éœ€è¦é—œæ³¨çš„ä½åˆ†æ•¸æ“šï¼‰
CREATE INDEX idx_quality_checks_low_score 
ON quality_checks (quality_score, started_at DESC)
WHERE quality_score < 80;

-- æŸ¥è©¢3ï¼šJSON å­—æ®µæŸ¥è©¢
SELECT * FROM quality_rules 
WHERE rule_definition->>'field' = 'email';

-- å‰µå»º GIN ç´¢å¼•æ”¯æŒ JSON æŸ¥è©¢
CREATE INDEX idx_quality_rules_definition_gin 
ON quality_rules USING GIN (rule_definition);

-- æˆ–å‰µå»ºè¡¨é”å¼ç´¢å¼•
CREATE INDEX idx_quality_rules_definition_field 
ON quality_rules ((rule_definition->>'field'));


-- 2. BRIN ç´¢å¼•ï¼ˆç”¨æ–¼å¤§è¡¨ã€é †åºæ•¸æ“šï¼‰
-- é©ç”¨æ–¼æ™‚åºæ•¸æ“šï¼Œç´¢å¼•å¤§å°é å°æ–¼ B-tree
CREATE INDEX idx_quality_checks_time_brin 
ON quality_checks USING BRIN (started_at) 
WITH (pages_per_range = 128);


-- 3. ä¸¦ç™¼å‰µå»ºç´¢å¼•
CREATE INDEX CONCURRENTLY idx_quality_checks_concurrent 
ON quality_checks (data_source);


-- 4. ç´¢å¼•ç¶­è­·
-- é‡å»ºç´¢å¼•ï¼ˆæ¶ˆé™¤ç¢ç‰‡ï¼‰
REINDEX TABLE quality_checks;

-- æˆ–ä½¿ç”¨ CONCURRENTLY é¿å…é–è¡¨
REINDEX INDEX CONCURRENTLY idx_quality_checks_source_time;


-- 5. ç›£æ§ç´¢å¼•ä½¿ç”¨æƒ…æ³
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan as index_scans,
    idx_tup_read as tuples_read,
    idx_tup_fetch as tuples_fetched,
    pg_size_pretty(pg_relation_size(indexrelid)) as index_size
FROM pg_stat_user_indexes
WHERE schemaname = 'public'
ORDER BY idx_scan ASC;

-- æŸ¥æ‰¾æœªä½¿ç”¨çš„ç´¢å¼•
SELECT 
    schemaname,
    tablename,
    indexname,
    pg_size_pretty(pg_relation_size(indexrelid)) as index_size
FROM pg_stat_user_indexes
WHERE idx_scan = 0
  AND indexrelname NOT LIKE 'pg_toast%';
```

### 2.3 ç·©å­˜ç­–ç•¥

#### å¤šç´šç·©å­˜æ¶æ§‹

```python
# app/quality_center/cache/cache_manager.py

from typing import Any, Optional
import redis
import pickle
from functools import wraps
import hashlib

class CacheManager:
    """å¤šç´šç·©å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        # L1 ç·©å­˜ï¼šæœ¬åœ°å…§å­˜ï¼ˆæœ€å¿«ï¼‰
        self.l1_cache = {}
        self.l1_max_size = 1000
        
        # L2 ç·©å­˜ï¼šRedisï¼ˆå¿«é€Ÿå…±äº«ï¼‰
        self.redis_client = redis.Redis(
            host='localhost',
            port=6379,
            decode_responses=False
        )
        
        # L3 ç·©å­˜ï¼šæ•¸æ“šåº«ç‰©åŒ–è¦–åœ–ï¼ˆè¼ƒæ…¢ä½†å¯é ï¼‰
    
    def get(self, key: str) -> Optional[Any]:
        """ç²å–ç·©å­˜ï¼ˆå¤šç´šæŸ¥æ‰¾ï¼‰"""
        # L1: å…§å­˜ç·©å­˜
        if key in self.l1_cache:
            logger.debug(f"L1 cache hit: {key}")
            return self.l1_cache[key]
        
        # L2: Redis ç·©å­˜
        redis_value = self.redis_client.get(key)
        if redis_value:
            logger.debug(f"L2 cache hit: {key}")
            value = pickle.loads(redis_value)
            # å¯«å…¥ L1
            self._set_l1(key, value)
            return value
        
        logger.debug(f"Cache miss: {key}")
        return None
    
    def set(self, key: str, value: Any, ttl: int = 3600):
        """è¨­ç½®ç·©å­˜ï¼ˆå¯«å…¥æ‰€æœ‰å±¤ç´šï¼‰"""
        # L1: å…§å­˜ç·©å­˜
        self._set_l1(key, value)
        
        # L2: Redis ç·©å­˜
        self.redis_client.setex(
            key,
            ttl,
            pickle.dumps(value)
        )
    
    def _set_l1(self, key: str, value: Any):
        """è¨­ç½® L1 ç·©å­˜ï¼ˆLRU æ·˜æ±°ï¼‰"""
        if len(self.l1_cache) >= self.l1_max_size:
            # ç°¡å–®çš„ LRUï¼šåˆªé™¤æœ€è€çš„ä¸€åŠ
            keys_to_remove = list(self.l1_cache.keys())[:self.l1_max_size // 2]
            for k in keys_to_remove:
                del self.l1_cache[k]
        
        self.l1_cache[key] = value
    
    def delete(self, key: str):
        """åˆªé™¤ç·©å­˜"""
        # L1
        if key in self.l1_cache:
            del self.l1_cache[key]
        
        # L2
        self.redis_client.delete(key)
    
    def clear_pattern(self, pattern: str):
        """æ¸…é™¤åŒ¹é…æ¨¡å¼çš„ç·©å­˜"""
        # L1
        keys_to_delete = [k for k in self.l1_cache if pattern in k]
        for key in keys_to_delete:
            del self.l1_cache[key]
        
        # L2
        cursor = 0
        while True:
            cursor, keys = self.redis_client.scan(
                cursor=cursor,
                match=pattern,
                count=100
            )
            if keys:
                self.redis_client.delete(*keys)
            if cursor == 0:
                break


# ç·©å­˜è£é£¾å™¨
def cached(ttl=3600, key_prefix=""):
    """ç·©å­˜è£é£¾å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ç”Ÿæˆç·©å­˜éµ
            cache_key = _generate_cache_key(
                key_prefix or func.__name__,
                args,
                kwargs
            )
            
            # å˜—è©¦å¾ç·©å­˜ç²å–
            cache_manager = CacheManager()
            cached_value = cache_manager.get(cache_key)
            
            if cached_value is not None:
                return cached_value
            
            # åŸ·è¡Œå‡½æ•¸
            result = func(*args, **kwargs)
            
            # å¯«å…¥ç·©å­˜
            cache_manager.set(cache_key, result, ttl)
            
            return result
        
        return wrapper
    return decorator


def _generate_cache_key(prefix: str, args: tuple, kwargs: dict) -> str:
    """ç”Ÿæˆç·©å­˜éµ"""
    # åºåˆ—åŒ–åƒæ•¸
    key_data = f"{prefix}:{args}:{sorted(kwargs.items())}"
    
    # ç”Ÿæˆå“ˆå¸Œ
    return hashlib.md5(key_data.encode()).hexdigest()


# ä½¿ç”¨ç¤ºä¾‹
@cached(ttl=300, key_prefix="quality_score")
def get_quality_score(data_source: str, date: str) -> float:
    """ç²å–è³ªé‡åˆ†æ•¸ï¼ˆå¸¶ç·©å­˜ï¼‰"""
    # å¾æ•¸æ“šåº«æŸ¥è©¢
    return db.query(
        "SELECT AVG(quality_score) FROM quality_checks "
        "WHERE data_source = %s AND DATE(started_at) = %s",
        (data_source, date)
    ).scalar()
```

---

## 3ï¸âƒ£ æŸ¥è©¢å„ªåŒ–

### 3.1 SQL å„ªåŒ–

#### æŸ¥è©¢é‡å¯«å„ªåŒ–

```sql
-- å„ªåŒ–å‰ï¼šå­æŸ¥è©¢
SELECT *
FROM quality_checks qc
WHERE qc.quality_score < (
    SELECT AVG(quality_score)
    FROM quality_checks
    WHERE data_source = qc.data_source
);

-- å„ªåŒ–å¾Œï¼šCTE + JOIN
WITH avg_scores AS (
    SELECT 
        data_source,
        AVG(quality_score) as avg_score
    FROM quality_checks
    GROUP BY data_source
)
SELECT qc.*
FROM quality_checks qc
JOIN avg_scores a ON qc.data_source = a.data_source
WHERE qc.quality_score < a.avg_score;


-- å„ªåŒ–å‰ï¼šN+1 æŸ¥è©¢å•é¡Œ
-- åœ¨æ‡‰ç”¨å±¤å¾ªç’°æŸ¥è©¢
for rule_id in rule_ids:
    db.query("SELECT * FROM quality_rules WHERE id = %s", (rule_id,))

-- å„ªåŒ–å¾Œï¼šå–®æ¬¡æ‰¹é‡æŸ¥è©¢
SELECT * FROM quality_rules WHERE id = ANY(%s);


-- å„ªåŒ–å‰ï¼šDISTINCT å»é‡
SELECT DISTINCT data_source FROM quality_checks;

-- å„ªåŒ–å¾Œï¼šGROUP BYï¼ˆé€šå¸¸æ›´å¿«ï¼‰
SELECT data_source FROM quality_checks GROUP BY data_source;


-- å„ªåŒ–å‰ï¼šOR æ¢ä»¶
SELECT * FROM quality_checks
WHERE data_source = 'ch8_crawler' OR data_source = 'ch1_api';

-- å„ªåŒ–å¾Œï¼šIN æ“ä½œç¬¦
SELECT * FROM quality_checks
WHERE data_source IN ('ch8_crawler', 'ch1_api');


-- å„ªåŒ–å‰ï¼šå‡½æ•¸ä½œç”¨æ–¼ç´¢å¼•åˆ—
SELECT * FROM quality_checks
WHERE DATE(started_at) = '2025-10-31';

-- å„ªåŒ–å¾Œï¼šç¯„åœæŸ¥è©¢
SELECT * FROM quality_checks
WHERE started_at >= '2025-10-31'
  AND started_at < '2025-11-01';
```

### 3.2 æ™‚åºæ•¸æ“šæŸ¥è©¢å„ªåŒ–

#### InfluxDB æŸ¥è©¢å„ªåŒ–

```python
# influxdb_optimizer.py

from influxdb_client import InfluxDBClient

class InfluxQueryOptimizer:
    """InfluxDB æŸ¥è©¢å„ªåŒ–å™¨"""
    
    def __init__(self, client: InfluxDBClient):
        self.client = client
        self.query_api = client.query_api()
    
    def optimized_query(self, measurement: str, field: str, 
                       start: str, end: str, data_source: str):
        """å„ªåŒ–çš„æŸ¥è©¢"""
        
        # 1. ä½¿ç”¨ pushdown éæ¿¾ï¼ˆåœ¨å­˜å„²å±¤éæ¿¾ï¼‰
        # 2. ä½¿ç”¨èšåˆæ¸›å°‘æ•¸æ“šé‡
        # 3. é™åˆ¶è¿”å›å­—æ®µ
        
        query = f'''
        from(bucket: "quality_metrics")
          |> range(start: {start}, stop: {end})
          |> filter(fn: (r) => r["_measurement"] == "{measurement}")
          |> filter(fn: (r) => r["_field"] == "{field}")
          |> filter(fn: (r) => r["data_source"] == "{data_source}")
          |> aggregateWindow(every: 5m, fn: mean)
          |> yield(name: "mean")
        '''
        
        return self.query_api.query(query)
    
    def optimized_downsampling(self, measurement: str, 
                               start: str, end: str):
        """é™æ¡æ¨£æŸ¥è©¢ï¼ˆæ¸›å°‘æ•¸æ“šé»ï¼‰"""
        
        query = f'''
        from(bucket: "quality_metrics")
          |> range(start: {start}, stop: {end})
          |> filter(fn: (r) => r["_measurement"] == "{measurement}")
          |> aggregateWindow(every: 1h, fn: mean, createEmpty: false)
          |> yield(name: "hourly_avg")
        '''
        
        return self.query_api.query(query)
    
    def parallel_query(self, measurements: list):
        """ä¸¦è¡ŒæŸ¥è©¢å¤šå€‹ measurement"""
        from concurrent.futures import ThreadPoolExecutor
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [
                executor.submit(self._query_measurement, m)
                for m in measurements
            ]
            
            results = [f.result() for f in futures]
        
        return results
```

---

## 4ï¸âƒ£ ä¸¦ç™¼å„ªåŒ–

### 4.1 ç•°æ­¥è™•ç†

```python
# async_processor.py

import asyncio
import aiohttp
from typing import List

class AsyncQualityChecker:
    """ç•°æ­¥è³ªé‡æª¢æŸ¥å™¨"""
    
    async def check_multiple_sources(self, sources: List[str]):
        """ä¸¦ç™¼æª¢æŸ¥å¤šå€‹æ•¸æ“šæº"""
        tasks = [
            self.check_source(source)
            for source in sources
        ]
        
        # ä¸¦ç™¼åŸ·è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return results
    
    async def check_source(self, source: str):
        """æª¢æŸ¥å–®å€‹æ•¸æ“šæº"""
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"http://api/quality/check",
                json={"data_source": source}
            ) as response:
                return await response.json()
    
    async def stream_process(self, data_stream):
        """æµå¼ç•°æ­¥è™•ç†"""
        async for data in data_stream:
            # éé˜»å¡è™•ç†
            await self.process_data(data)
    
    async def process_data(self, data):
        """è™•ç†å–®æ¢æ•¸æ“š"""
        # CPU å¯†é›†å‹æ“ä½œä½¿ç”¨é€²ç¨‹æ± 
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            process_pool_executor,
            cpu_intensive_check,
            data
        )
        return result
```

### 4.2 ä»»å‹™éšŠåˆ—å„ªåŒ–

```python
# celery_tasks.py

from celery import Celery, group, chord
from celery.result import allow_join_result

app = Celery('quality_tasks', broker='redis://localhost:6379/0')

# é…ç½®å„ªåŒ–
app.conf.update(
    task_serializer='json',
    result_serializer='json',
    accept_content=['json'],
    timezone='UTC',
    enable_utc=True,
    
    # ä¸¦ç™¼å„ªåŒ–
    worker_prefetch_multiplier=4,
    worker_max_tasks_per_child=1000,
    
    # çµæœå¾Œç«¯å„ªåŒ–
    result_backend='redis://localhost:6379/1',
    result_expires=3600,
    result_compression='gzip',
    
    # ä»»å‹™è·¯ç”±
    task_routes={
        'quality_tasks.check_quality': {'queue': 'quality'},
        'quality_tasks.clean_data': {'queue': 'cleaning'},
        'quality_tasks.detect_anomaly': {'queue': 'detection'},
    }
)


@app.task(bind=True, max_retries=3)
def check_quality_task(self, data_source: str):
    """è³ªé‡æª¢æŸ¥ä»»å‹™"""
    try:
        result = perform_quality_check(data_source)
        return result
    except Exception as exc:
        # æŒ‡æ•¸é€€é¿é‡è©¦
        raise self.retry(exc=exc, countdown=2 ** self.request.retries)


@app.task
def aggregate_results(results):
    """èšåˆçµæœ"""
    return {
        'total': len(results),
        'avg_score': sum(r['score'] for r in results) / len(results)
    }


def parallel_quality_checks(data_sources: List[str]):
    """ä¸¦è¡Œè³ªé‡æª¢æŸ¥ï¼ˆä½¿ç”¨ Celeryï¼‰"""
    
    # æ–¹å¼1ï¼šä½¿ç”¨ groupï¼ˆä¸¦è¡ŒåŸ·è¡Œï¼‰
    job = group(
        check_quality_task.s(source)
        for source in data_sources
    )
    result = job.apply_async()
    
    # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
    with allow_join_result():
        results = result.get(timeout=300)
    
    return results


def pipeline_quality_check(data_sources: List[str]):
    """ç®¡é“å¼è³ªé‡æª¢æŸ¥ï¼ˆä½¿ç”¨ chordï¼‰"""
    
    # chord: ä¸¦è¡ŒåŸ·è¡Œ header ä»»å‹™ï¼Œçµæœèšåˆåˆ° callback
    job = chord(
        check_quality_task.s(source)
        for source in data_sources
    )(aggregate_results.s())
    
    result = job.get(timeout=300)
    return result
```

---

## 5ï¸âƒ£ è³‡æºå„ªåŒ–

### 5.1 å…§å­˜ç®¡ç†

```python
# memory_optimizer.py

import gc
import sys
from memory_profiler import profile

class MemoryOptimizedProcessor:
    """å…§å­˜å„ªåŒ–è™•ç†å™¨"""
    
    @profile
    def process_large_dataset(self, file_path: str):
        """è™•ç†å¤§æ•¸æ“šé›†ï¼ˆå…§å­˜å„ªåŒ–ï¼‰"""
        
        # 1. ä½¿ç”¨ç”Ÿæˆå™¨ï¼ˆæƒ°æ€§åŠ è¼‰ï¼‰
        for chunk in self._read_chunks(file_path, chunk_size=10000):
            self._process_chunk(chunk)
            
            # 2. åŠæ™‚æ¸…ç†
            del chunk
            gc.collect()
    
    def _read_chunks(self, file_path: str, chunk_size: int):
        """åˆ†å¡Šè®€å–ï¼ˆç”Ÿæˆå™¨ï¼‰"""
        import pandas as pd
        
        for chunk in pd.read_csv(
            file_path,
            chunksize=chunk_size,
            # 3. æŒ‡å®šæ•¸æ“šé¡å‹ï¼ˆæ¸›å°‘å…§å­˜ï¼‰
            dtype={
                'id': 'int32',
                'score': 'float32'
            },
            # 4. åªè®€å–éœ€è¦çš„åˆ—
            usecols=['id', 'score', 'source']
        ):
            yield chunk
    
    def optimize_dataframe_memory(self, df):
        """å„ªåŒ– DataFrame å…§å­˜ä½¿ç”¨"""
        
        for col in df.columns:
            col_type = df[col].dtype
            
            # å„ªåŒ–æ•¸å€¼é¡å‹
            if col_type == 'int64':
                df[col] = pd.to_numeric(df[col], downcast='integer')
            elif col_type == 'float64':
                df[col] = pd.to_numeric(df[col], downcast='float')
            
            # å„ªåŒ–å­—ç¬¦ä¸²é¡å‹
            elif col_type == 'object':
                num_unique = df[col].nunique()
                num_total = len(df[col])
                
                # å¦‚æœå”¯ä¸€å€¼å°‘ï¼Œä½¿ç”¨ category
                if num_unique / num_total < 0.5:
                    df[col] = df[col].astype('category')
        
        return df


# å…§å­˜ç›£æ§
def monitor_memory():
    """ç›£æ§å…§å­˜ä½¿ç”¨"""
    import psutil
    
    process = psutil.Process()
    memory_info = process.memory_info()
    
    print(f"RSS: {memory_info.rss / 1024 / 1024:.2f} MB")
    print(f"VMS: {memory_info.vms / 1024 / 1024:.2f} MB")
    
    # ç³»çµ±å…§å­˜
    sys_memory = psutil.virtual_memory()
    print(f"System Memory: {sys_memory.percent}% used")
```

### 5.2 CPU å„ªåŒ–

```python
# cpu_optimizer.py

from multiprocessing import Pool, cpu_count
from concurrent.futures import ProcessPoolExecutor, as_completed
import numpy as np

class CPUOptimizer:
    """CPU å„ªåŒ–å™¨"""
    
    def __init__(self):
        self.num_workers = cpu_count()
    
    def parallel_process(self, data_list: list):
        """ä¸¦è¡Œè™•ç†ï¼ˆå¤šé€²ç¨‹ï¼‰"""
        
        # ä½¿ç”¨é€²ç¨‹æ± 
        with Pool(processes=self.num_workers) as pool:
            results = pool.map(
                self._process_item,
                data_list,
                chunksize=100  # æ‰¹æ¬¡å¤§å°
            )
        
        return results
    
    def parallel_with_progress(self, data_list: list):
        """ä¸¦è¡Œè™•ç†ï¼ˆå¸¶é€²åº¦ï¼‰"""
        
        results = []
        
        with ProcessPoolExecutor(max_workers=self.num_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_data = {
                executor.submit(self._process_item, data): data
                for data in data_list
            }
            
            # é€å€‹ç²å–çµæœ
            for future in as_completed(future_to_data):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as exc:
                    logger.error(f"Task generated exception: {exc}")
        
        return results
    
    @staticmethod
    def _process_item(data):
        """è™•ç†å–®å€‹é …ç›®"""
        # CPU å¯†é›†å‹æ“ä½œ
        return heavy_computation(data)


# å‘é‡åŒ–å„ªåŒ–ï¼ˆä½¿ç”¨ NumPyï¼‰
def vectorized_quality_check(data: np.ndarray):
    """å‘é‡åŒ–è³ªé‡æª¢æŸ¥ï¼ˆæ¯”å¾ªç’°å¿«10-100å€ï¼‰"""
    
    # å„ªåŒ–å‰ï¼šPython å¾ªç’°
    # results = []
    # for value in data:
    #     if 0 <= value <= 100:
    #         results.append(True)
    #     else:
    #         results.append(False)
    
    # å„ªåŒ–å¾Œï¼šå‘é‡åŒ–æ“ä½œ
    results = (data >= 0) & (data <= 100)
    
    return results


# Numba JIT ç·¨è­¯å„ªåŒ–
from numba import jit, prange

@jit(nopython=True, parallel=True)
def numba_quality_check(data):
    """ä½¿ç”¨ Numba åŠ é€Ÿè³ªé‡æª¢æŸ¥"""
    n = len(data)
    results = np.empty(n, dtype=np.bool_)
    
    for i in prange(n):
        results[i] = 0 <= data[i] <= 100
    
    return results
```

### 5.3 I/O å„ªåŒ–

```python
# io_optimizer.py

import aiofiles
import asyncio
from pathlib import Path

class IOOptimizer:
    """I/O å„ªåŒ–å™¨"""
    
    async def async_read_files(self, file_paths: list):
        """ç•°æ­¥è®€å–å¤šå€‹æ–‡ä»¶"""
        
        tasks = [
            self._async_read_file(path)
            for path in file_paths
        ]
        
        contents = await asyncio.gather(*tasks)
        return contents
    
    async def _async_read_file(self, file_path: str):
        """ç•°æ­¥è®€å–å–®å€‹æ–‡ä»¶"""
        async with aiofiles.open(file_path, 'r') as f:
            content = await f.read()
        return content
    
    def buffered_write(self, data_stream, output_file: str, buffer_size=10000):
        """ç·©è¡å¯«å…¥"""
        buffer = []
        
        with open(output_file, 'w', buffering=buffer_size * 1024) as f:
            for data in data_stream:
                buffer.append(data)
                
                if len(buffer) >= buffer_size:
                    f.write('\n'.join(buffer) + '\n')
                    buffer = []
            
            # å¯«å…¥å‰©é¤˜æ•¸æ“š
            if buffer:
                f.write('\n'.join(buffer) + '\n')
    
    def memory_mapped_read(self, file_path: str):
        """å…§å­˜æ˜ å°„è®€å–ï¼ˆå¤§æ–‡ä»¶ï¼‰"""
        import mmap
        
        with open(file_path, 'r+b') as f:
            with mmap.mmap(f.fileno(), 0) as mmapped_file:
                # å¯ä»¥åƒæ™®é€šæ–‡ä»¶ä¸€æ¨£è®€å–
                content = mmapped_file.read()
        
        return content
```

---

## ğŸ“Š æ€§èƒ½ç›£æ§

### ç›£æ§æŒ‡æ¨™

```python
# performance_monitor.py

from prometheus_client import Counter, Histogram, Gauge
import time

# å®šç¾©æŒ‡æ¨™
quality_check_total = Counter(
    'quality_check_total',
    'Total quality checks',
    ['data_source', 'status']
)

quality_check_duration = Histogram(
    'quality_check_duration_seconds',
    'Quality check duration',
    ['data_source']
)

processing_queue_size = Gauge(
    'processing_queue_size',
    'Current processing queue size'
)


def monitored_quality_check(data_source: str):
    """å¸¶ç›£æ§çš„è³ªé‡æª¢æŸ¥"""
    
    start_time = time.time()
    
    try:
        result = perform_quality_check(data_source)
        
        # è¨˜éŒ„æˆåŠŸ
        quality_check_total.labels(
            data_source=data_source,
            status='success'
        ).inc()
        
        return result
    
    except Exception as e:
        # è¨˜éŒ„å¤±æ•—
        quality_check_total.labels(
            data_source=data_source,
            status='failure'
        ).inc()
        
        raise
    
    finally:
        # è¨˜éŒ„è€—æ™‚
        duration = time.time() - start_time
        quality_check_duration.labels(
            data_source=data_source
        ).observe(duration)
```

---

## ğŸ“‘ ç›¸é—œç« ç¯€

| å‰åº | ç•¶å‰ | å¾ŒçºŒ |
|-----|------|------|
| [12.6 APIè©³ç´°è¦ç¯„](ch12-6-APIè©³ç´°è¦ç¯„.md) | **12.7 æ•ˆèƒ½å„ªåŒ–ç­–ç•¥** | [12.8 å®‰å…¨è€ƒæ…®](ch12-8-å®‰å…¨è€ƒæ…®.md) |

**å¿«é€Ÿéˆæ¥ï¼š**
- [12.1 æ¨¡çµ„æ¦‚è¿°](ch12-1-æ¨¡çµ„æ¦‚è¿°.md)
- [12.6 APIè©³ç´°è¦ç¯„](ch12-6-APIè©³ç´°è¦ç¯„.md)
- [12.8 å®‰å…¨è€ƒæ…®](ch12-8-å®‰å…¨è€ƒæ…®.md)
- [â† è¿”å›ç¬¬12ç« é¦–é ](ch12-index.md)

---

**æœ€å¾Œæ›´æ–°**: 2025-10-31  
**ç‰ˆæœ¬**: 1.0

