# ç¬¬12ç« ï¼šæ•¸æ“šè³ªé‡ç®¡ç†ä¸­å¿ƒ

## 12.4 æ ¸å¿ƒçµ„ä»¶è©³ç´°å¯¦ç¾

**[â† è¿”å›ç¬¬12ç« é¦–é ](ch12-index.md)**

---

## ğŸ¯ æ ¸å¿ƒçµ„ä»¶æ¦‚è¦½

```
Ch12 æ•¸æ“šè³ªé‡ç®¡ç†ä¸­å¿ƒ - æ ¸å¿ƒçµ„ä»¶
â”œâ”€â”€ 1. Quality Monitor (è³ªé‡ç›£æ§å™¨)
â”œâ”€â”€ 2. Data Cleaner (æ•¸æ“šæ¸…æ´—å™¨)
â”œâ”€â”€ 3. Rule Engine (è¦å‰‡å¼•æ“)
â”œâ”€â”€ 4. Anomaly Detector (ç•°å¸¸æª¢æ¸¬å™¨)
â”œâ”€â”€ 5. Quality Scorer (è³ªé‡è©•åˆ†å™¨)
â”œâ”€â”€ 6. Lineage Tracker (è¡€ç·£è¿½è¸ªå™¨)
â”œâ”€â”€ 7. Report Generator (å ±å‘Šç”Ÿæˆå™¨)
â””â”€â”€ 8. Fix Manager (ä¿®å¾©ç®¡ç†å™¨)
```

---

## 1ï¸âƒ£ Quality Monitor (è³ªé‡ç›£æ§å™¨)

### 1.1 çµ„ä»¶è¨­è¨ˆ

```python
# app/quality_center/monitor/quality_monitor.py

from typing import Dict, List, Optional
from dataclasses import dataclass
from datetime import datetime
import asyncio
from kafka import KafkaConsumer, KafkaProducer
import redis
from influxdb_client import InfluxDBClient
from prometheus_client import Counter, Gauge, Histogram

@dataclass
class QualityMetrics:
    """è³ªé‡æŒ‡æ¨™æ•¸æ“šé¡"""
    timestamp: datetime
    data_source: str
    total_records: int
    valid_records: int
    invalid_records: int
    null_count: int
    duplicate_count: int
    quality_score: float
    dimension_scores: Dict[str, float]

class QualityMonitor:
    """
    è³ªé‡ç›£æ§å™¨
    
    åŠŸèƒ½ï¼š
    - å¯¦æ™‚ç›£æ§æ•¸æ“šè³ªé‡
    - æ¡é›†è³ªé‡æŒ‡æ¨™
    - è§¸ç™¼è³ªé‡å‘Šè­¦
    - æ›´æ–°è³ªé‡çœ‹æ¿
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.kafka_consumer = self._init_kafka_consumer()
        self.kafka_producer = self._init_kafka_producer()
        self.redis_client = redis.Redis(
            host=config['redis']['host'],
            port=config['redis']['port']
        )
        self.influx_client = InfluxDBClient(
            url=config['influxdb']['url'],
            token=config['influxdb']['token'],
            org=config['influxdb']['org']
        )
        
        # Prometheus æŒ‡æ¨™
        self.metrics_counter = Counter(
            'quality_check_total',
            'Total quality checks',
            ['data_source', 'status']
        )
        self.quality_score_gauge = Gauge(
            'quality_score',
            'Current quality score',
            ['data_source']
        )
        self.check_duration = Histogram(
            'quality_check_duration_seconds',
            'Quality check duration'
        )
    
    def _init_kafka_consumer(self) -> KafkaConsumer:
        """åˆå§‹åŒ– Kafka æ¶ˆè²»è€…"""
        return KafkaConsumer(
            self.config['kafka']['topics']['raw_data'],
            bootstrap_servers=self.config['kafka']['bootstrap_servers'],
            group_id='quality_monitor_group',
            auto_offset_reset='latest',
            enable_auto_commit=True,
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
    
    def _init_kafka_producer(self) -> KafkaProducer:
        """åˆå§‹åŒ– Kafka ç”Ÿç”¢è€…"""
        return KafkaProducer(
            bootstrap_servers=self.config['kafka']['bootstrap_servers'],
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
    
    async def start_monitoring(self):
        """å•Ÿå‹•å¯¦æ™‚ç›£æ§"""
        logger.info("Starting quality monitor...")
        
        tasks = [
            asyncio.create_task(self._consume_data_stream()),
            asyncio.create_task(self._aggregate_metrics()),
            asyncio.create_task(self._check_alerts()),
            asyncio.create_task(self._update_dashboard())
        ]
        
        await asyncio.gather(*tasks)
    
    async def _consume_data_stream(self):
        """æ¶ˆè²»æ•¸æ“šæµä¸¦é€²è¡Œå¯¦æ™‚æª¢æŸ¥"""
        for message in self.kafka_consumer:
            with self.check_duration.time():
                data = message.value
                
                # åŸ·è¡Œè³ªé‡æª¢æŸ¥
                check_result = await self._check_quality(data)
                
                # æ›´æ–°è¨ˆæ•¸å™¨
                status = 'valid' if check_result['is_valid'] else 'invalid'
                self.metrics_counter.labels(
                    data_source=data['source'],
                    status=status
                ).inc()
                
                # å¦‚æœæœ‰å•é¡Œï¼Œç™¼é€åˆ°å•é¡ŒéšŠåˆ—
                if not check_result['is_valid']:
                    await self._send_to_issue_queue(data, check_result)
                
                # æ›´æ–°å¯¦æ™‚æŒ‡æ¨™
                await self._update_real_time_metrics(data, check_result)
    
    async def _check_quality(self, data: Dict) -> Dict:
        """
        åŸ·è¡Œè³ªé‡æª¢æŸ¥
        
        Returns:
            {
                'is_valid': bool,
                'errors': List[str],
                'warnings': List[str],
                'quality_score': float,
                'dimension_scores': Dict[str, float]
            }
        """
        result = {
            'is_valid': True,
            'errors': [],
            'warnings': [],
            'quality_score': 100.0,
            'dimension_scores': {}
        }
        
        # 1. å®Œæ•´æ€§æª¢æŸ¥
        completeness_score = self._check_completeness(data)
        result['dimension_scores']['completeness'] = completeness_score
        
        if completeness_score < 90:
            result['warnings'].append(f"Completeness score low: {completeness_score}")
        
        # 2. æº–ç¢ºæ€§æª¢æŸ¥
        accuracy_score = self._check_accuracy(data)
        result['dimension_scores']['accuracy'] = accuracy_score
        
        if accuracy_score < 80:
            result['errors'].append(f"Accuracy score low: {accuracy_score}")
            result['is_valid'] = False
        
        # 3. æ ¼å¼æª¢æŸ¥
        format_issues = self._check_format(data)
        if format_issues:
            result['errors'].extend(format_issues)
            result['is_valid'] = False
        
        # 4. æ¥­å‹™è¦å‰‡æª¢æŸ¥
        business_issues = self._check_business_rules(data)
        if business_issues:
            result['errors'].extend(business_issues)
            result['is_valid'] = False
        
        # è¨ˆç®—ç¸½è³ªé‡åˆ†æ•¸
        result['quality_score'] = self._calculate_overall_score(
            result['dimension_scores']
        )
        
        return result
    
    def _check_completeness(self, data: Dict) -> float:
        """æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§"""
        required_fields = self.config.get('required_fields', [])
        if not required_fields:
            return 100.0
        
        present_fields = sum(
            1 for field in required_fields 
            if field in data and data[field] is not None
        )
        
        return (present_fields / len(required_fields)) * 100
    
    def _check_accuracy(self, data: Dict) -> float:
        """æª¢æŸ¥æ•¸æ“šæº–ç¢ºæ€§"""
        validation_rules = self.config.get('validation_rules', {})
        if not validation_rules:
            return 100.0
        
        passed_rules = 0
        total_rules = 0
        
        for field, rules in validation_rules.items():
            if field not in data:
                continue
            
            for rule in rules:
                total_rules += 1
                if self._validate_rule(data[field], rule):
                    passed_rules += 1
        
        if total_rules == 0:
            return 100.0
        
        return (passed_rules / total_rules) * 100
    
    def _validate_rule(self, value: any, rule: Dict) -> bool:
        """é©—è­‰å–®å€‹è¦å‰‡"""
        rule_type = rule['type']
        
        if rule_type == 'type':
            return isinstance(value, eval(rule['expected_type']))
        
        elif rule_type == 'range':
            return rule['min'] <= value <= rule['max']
        
        elif rule_type == 'regex':
            import re
            return bool(re.match(rule['pattern'], str(value)))
        
        elif rule_type == 'enum':
            return value in rule['allowed_values']
        
        return True
    
    def _check_format(self, data: Dict) -> List[str]:
        """æª¢æŸ¥æ•¸æ“šæ ¼å¼"""
        issues = []
        format_rules = self.config.get('format_rules', {})
        
        for field, expected_format in format_rules.items():
            if field not in data:
                continue
            
            if not self._validate_format(data[field], expected_format):
                issues.append(
                    f"Field '{field}' format invalid. "
                    f"Expected: {expected_format}"
                )
        
        return issues
    
    def _validate_format(self, value: str, format_type: str) -> bool:
        """é©—è­‰æ ¼å¼"""
        import re
        from urllib.parse import urlparse
        
        if format_type == 'email':
            pattern = r'^[\w\.-]+@[\w\.-]+\.\w+$'
            return bool(re.match(pattern, str(value)))
        
        elif format_type == 'url':
            try:
                result = urlparse(str(value))
                return all([result.scheme, result.netloc])
            except:
                return False
        
        elif format_type == 'date':
            from dateutil import parser
            try:
                parser.parse(str(value))
                return True
            except:
                return False
        
        return True
    
    def _check_business_rules(self, data: Dict) -> List[str]:
        """æª¢æŸ¥æ¥­å‹™è¦å‰‡"""
        issues = []
        business_rules = self.config.get('business_rules', [])
        
        for rule in business_rules:
            if not self._evaluate_business_rule(data, rule):
                issues.append(rule['error_message'])
        
        return issues
    
    def _evaluate_business_rule(self, data: Dict, rule: Dict) -> bool:
        """è©•ä¼°æ¥­å‹™è¦å‰‡"""
        try:
            # ä½¿ç”¨ eval åŸ·è¡Œè¦å‰‡è¡¨é”å¼ï¼ˆç”Ÿç”¢ç’°å¢ƒæ‡‰ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼ï¼‰
            expression = rule['expression']
            
            # æ§‹å»ºå®‰å…¨çš„å‘½åç©ºé–“
            namespace = {k: v for k, v in data.items()}
            namespace['__builtins__'] = {}
            
            return eval(expression, namespace)
        except Exception as e:
            logger.error(f"Error evaluating business rule: {e}")
            return False
    
    def _calculate_overall_score(self, dimension_scores: Dict[str, float]) -> float:
        """è¨ˆç®—ç¸½è³ªé‡åˆ†æ•¸"""
        weights = self.config.get('dimension_weights', {
            'completeness': 0.25,
            'accuracy': 0.30,
            'consistency': 0.20,
            'timeliness': 0.15,
            'uniqueness': 0.10
        })
        
        total_score = sum(
            dimension_scores.get(dim, 100.0) * weight
            for dim, weight in weights.items()
        )
        
        return round(total_score, 2)
    
    async def _send_to_issue_queue(self, data: Dict, check_result: Dict):
        """ç™¼é€å•é¡Œæ•¸æ“šåˆ°å•é¡ŒéšŠåˆ—"""
        issue_data = {
            'timestamp': datetime.now().isoformat(),
            'data_source': data.get('source', 'unknown'),
            'original_data': data,
            'issues': check_result['errors'],
            'warnings': check_result['warnings'],
            'quality_score': check_result['quality_score']
        }
        
        # ç™¼é€åˆ° Kafka å•é¡Œä¸»é¡Œ
        self.kafka_producer.send(
            self.config['kafka']['topics']['quality_issues'],
            value=issue_data
        )
    
    async def _update_real_time_metrics(self, data: Dict, check_result: Dict):
        """æ›´æ–°å¯¦æ™‚æŒ‡æ¨™åˆ° InfluxDB"""
        point = {
            "measurement": "quality_metrics",
            "tags": {
                "data_source": data.get('source', 'unknown'),
            },
            "fields": {
                "quality_score": check_result['quality_score'],
                "completeness": check_result['dimension_scores'].get('completeness', 0),
                "accuracy": check_result['dimension_scores'].get('accuracy', 0),
                "is_valid": 1 if check_result['is_valid'] else 0
            },
            "time": datetime.utcnow()
        }
        
        write_api = self.influx_client.write_api()
        write_api.write(
            bucket=self.config['influxdb']['bucket'],
            record=point
        )
        
        # æ›´æ–° Prometheus æŒ‡æ¨™
        self.quality_score_gauge.labels(
            data_source=data.get('source', 'unknown')
        ).set(check_result['quality_score'])
    
    async def _aggregate_metrics(self):
        """èšåˆè³ªé‡æŒ‡æ¨™ï¼ˆæ¯åˆ†é˜ï¼‰"""
        while True:
            await asyncio.sleep(60)  # æ¯åˆ†é˜åŸ·è¡Œä¸€æ¬¡
            
            # å¾ Redis ç²å–æœ€è¿‘ä¸€åˆ†é˜çš„çµ±è¨ˆ
            stats = self._get_minute_stats()
            
            # è¨ˆç®—èšåˆæŒ‡æ¨™
            aggregated = self._calculate_aggregated_metrics(stats)
            
            # å­˜å„²åˆ° InfluxDB
            await self._store_aggregated_metrics(aggregated)
    
    def _get_minute_stats(self) -> Dict:
        """ç²å–æœ€è¿‘ä¸€åˆ†é˜çš„çµ±è¨ˆæ•¸æ“š"""
        # å¾ Redis ç²å–å¯¦æ™‚è¨ˆæ•¸
        stats = {}
        
        for source in self.config.get('data_sources', []):
            key_prefix = f"quality:stats:{source}"
            
            stats[source] = {
                'total': int(self.redis_client.get(f"{key_prefix}:total") or 0),
                'valid': int(self.redis_client.get(f"{key_prefix}:valid") or 0),
                'invalid': int(self.redis_client.get(f"{key_prefix}:invalid") or 0),
            }
            
            # é‡ç½®è¨ˆæ•¸å™¨
            self.redis_client.delete(
                f"{key_prefix}:total",
                f"{key_prefix}:valid",
                f"{key_prefix}:invalid"
            )
        
        return stats
    
    def _calculate_aggregated_metrics(self, stats: Dict) -> Dict:
        """è¨ˆç®—èšåˆæŒ‡æ¨™"""
        aggregated = {}
        
        for source, data in stats.items():
            total = data['total']
            if total == 0:
                continue
            
            aggregated[source] = {
                'total_records': total,
                'valid_records': data['valid'],
                'invalid_records': data['invalid'],
                'valid_rate': (data['valid'] / total) * 100,
                'invalid_rate': (data['invalid'] / total) * 100
            }
        
        return aggregated
    
    async def _store_aggregated_metrics(self, metrics: Dict):
        """å­˜å„²èšåˆæŒ‡æ¨™"""
        write_api = self.influx_client.write_api()
        
        for source, data in metrics.items():
            point = {
                "measurement": "quality_metrics_aggregated",
                "tags": {
                    "data_source": source,
                    "aggregation": "1m"
                },
                "fields": data,
                "time": datetime.utcnow()
            }
            
            write_api.write(
                bucket=self.config['influxdb']['bucket'],
                record=point
            )
    
    async def _check_alerts(self):
        """æª¢æŸ¥å‘Šè­¦æ¢ä»¶"""
        while True:
            await asyncio.sleep(30)  # æ¯30ç§’æª¢æŸ¥ä¸€æ¬¡
            
            # æŸ¥è©¢æœ€è¿‘çš„è³ªé‡åˆ†æ•¸
            recent_scores = await self._query_recent_scores()
            
            # æª¢æŸ¥å‘Šè­¦è¦å‰‡
            for source, score in recent_scores.items():
                await self._evaluate_alert_rules(source, score)
    
    async def _query_recent_scores(self) -> Dict[str, float]:
        """æŸ¥è©¢æœ€è¿‘çš„è³ªé‡åˆ†æ•¸"""
        query_api = self.influx_client.query_api()
        
        query = f'''
        from(bucket: "{self.config['influxdb']['bucket']}")
          |> range(start: -5m)
          |> filter(fn: (r) => r["_measurement"] == "quality_metrics")
          |> filter(fn: (r) => r["_field"] == "quality_score")
          |> mean()
          |> group(columns: ["data_source"])
        '''
        
        result = query_api.query(query)
        
        scores = {}
        for table in result:
            for record in table.records:
                source = record.values.get('data_source')
                score = record.get_value()
                scores[source] = score
        
        return scores
    
    async def _evaluate_alert_rules(self, source: str, score: float):
        """è©•ä¼°å‘Šè­¦è¦å‰‡"""
        alert_rules = self.config.get('alert_rules', [])
        
        for rule in alert_rules:
            if self._should_alert(source, score, rule):
                await self._send_alert(source, score, rule)
    
    def _should_alert(self, source: str, score: float, rule: Dict) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²å‘Šè­¦"""
        # æª¢æŸ¥æ•¸æ“šæºéæ¿¾
        if 'sources' in rule and source not in rule['sources']:
            return False
        
        # æª¢æŸ¥åˆ†æ•¸é–¾å€¼
        if score < rule.get('threshold', 80):
            return True
        
        return False
    
    async def _send_alert(self, source: str, score: float, rule: Dict):
        """ç™¼é€å‘Šè­¦"""
        alert_data = {
            'timestamp': datetime.now().isoformat(),
            'severity': rule.get('severity', 'warning'),
            'source': source,
            'quality_score': score,
            'threshold': rule.get('threshold'),
            'message': rule.get('message', f"Quality score {score} below threshold"),
            'channels': rule.get('channels', ['email'])
        }
        
        # ç™¼é€åˆ°å‘Šè­¦ä¸»é¡Œ
        self.kafka_producer.send(
            self.config['kafka']['topics']['alerts'],
            value=alert_data
        )
        
        logger.warning(
            f"Alert sent: {source} quality score {score} "
            f"below threshold {rule.get('threshold')}"
        )
    
    async def _update_dashboard(self):
        """æ›´æ–°çœ‹æ¿æ•¸æ“šï¼ˆæ¯10ç§’ï¼‰"""
        while True:
            await asyncio.sleep(10)
            
            # è¨ˆç®—çœ‹æ¿æ•¸æ“š
            dashboard_data = await self._calculate_dashboard_data()
            
            # æ›´æ–°åˆ° Redisï¼ˆä¾›å‰ç«¯è®€å–ï¼‰
            self.redis_client.setex(
                'quality:dashboard',
                60,  # 60ç§’éæœŸ
                json.dumps(dashboard_data)
            )
    
    async def _calculate_dashboard_data(self) -> Dict:
        """è¨ˆç®—çœ‹æ¿æ•¸æ“š"""
        query_api = self.influx_client.query_api()
        
        # æŸ¥è©¢æœ€è¿‘1å°æ™‚çš„æ•¸æ“š
        query = f'''
        from(bucket: "{self.config['influxdb']['bucket']}")
          |> range(start: -1h)
          |> filter(fn: (r) => r["_measurement"] == "quality_metrics")
        '''
        
        result = query_api.query(query)
        
        # è™•ç†æŸ¥è©¢çµæœ
        dashboard_data = {
            'overall_score': 0,
            'total_records': 0,
            'valid_records': 0,
            'invalid_records': 0,
            'trend': [],
            'by_source': {}
        }
        
        # ... æ•¸æ“šè™•ç†é‚è¼¯ ...
        
        return dashboard_data


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    config = {
        'kafka': {
            'bootstrap_servers': ['localhost:9092'],
            'topics': {
                'raw_data': 'raw_data',
                'quality_issues': 'quality_issues',
                'alerts': 'quality_alerts'
            }
        },
        'redis': {
            'host': 'localhost',
            'port': 6379
        },
        'influxdb': {
            'url': 'http://localhost:8086',
            'token': 'your-token',
            'org': 'your-org',
            'bucket': 'quality_metrics'
        },
        'required_fields': ['id', 'title', 'url'],
        'validation_rules': {
            'price': [
                {'type': 'range', 'min': 0, 'max': 1000000}
            ],
            'email': [
                {'type': 'regex', 'pattern': r'^[\w\.-]+@[\w\.-]+\.\w+$'}
            ]
        },
        'alert_rules': [
            {
                'threshold': 80,
                'severity': 'warning',
                'channels': ['email', 'slack']
            },
            {
                'threshold': 60,
                'severity': 'critical',
                'channels': ['email', 'slack', 'phone']
            }
        ]
    }
    
    monitor = QualityMonitor(config)
    await monitor.start_monitoring()


if __name__ == '__main__':
    asyncio.run(main())
```

### 1.2 é…ç½®ç¤ºä¾‹

```yaml
# config/quality_monitor.yaml

monitor:
  # æ•¸æ“šæºé…ç½®
  data_sources:
    - name: "ch8_crawler"
      enabled: true
      check_interval: 1  # ç§’
      
    - name: "ch1_api"
      enabled: true
      check_interval: 5
  
  # è³ªé‡æª¢æŸ¥é…ç½®
  quality_checks:
    completeness:
      enabled: true
      weight: 0.25
      required_fields:
        - id
        - title
        - content
        - created_at
    
    accuracy:
      enabled: true
      weight: 0.30
      validation_rules:
        email:
          - type: regex
            pattern: "^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$"
        price:
          - type: range
            min: 0
            max: 1000000
        age:
          - type: range
            min: 0
            max: 150
    
    consistency:
      enabled: true
      weight: 0.20
      cross_field_rules:
        - expression: "end_date > start_date"
          message: "çµæŸæ—¥æœŸå¿…é ˆæ™šæ–¼é–‹å§‹æ—¥æœŸ"
    
    timeliness:
      enabled: true
      weight: 0.15
      sla_hours: 24
    
    uniqueness:
      enabled: true
      weight: 0.10
      key_fields:
        - url
        - title
  
  # å‘Šè­¦é…ç½®
  alerts:
    - name: "quality_drop_critical"
      condition: "quality_score < 60"
      severity: "critical"
      channels:
        - email
        - slack
        - sms
      throttle_minutes: 30
      
    - name: "quality_drop_warning"
      condition: "quality_score < 80"
      severity: "warning"
      channels:
        - email
        - slack
      throttle_minutes: 60
      
    - name: "high_error_rate"
      condition: "error_rate > 0.05"
      severity: "warning"
      channels:
        - slack
      throttle_minutes: 15
  
  # çœ‹æ¿é…ç½®
  dashboard:
    refresh_interval: 10  # ç§’
    metrics:
      - overall_quality_score
      - records_per_second
      - error_rate
      - top_issues
      - quality_trend
    time_ranges:
      - 1h
      - 6h
      - 24h
      - 7d
```

---

## 2ï¸âƒ£ Data Cleaner (æ•¸æ“šæ¸…æ´—å™¨)

### 2.1 çµ„ä»¶å¯¦ç¾

```python
# app/quality_center/cleaner/data_cleaner.py

from typing import Dict, List, Optional, Callable
import pandas as pd
import numpy as np
from dataclasses import dataclass
from abc import ABC, abstractmethod
import logging

logger = logging.getLogger(__name__)


@dataclass
class CleaningResult:
    """æ¸…æ´—çµæœ"""
    success: bool
    original_count: int
    cleaned_count: int
    removed_count: int
    modified_count: int
    operations: List[str]
    issues: List[str]


class CleaningOperation(ABC):
    """æ¸…æ´—æ“ä½œåŸºé¡"""
    
    @abstractmethod
    def execute(self, df: pd.DataFrame) -> pd.DataFrame:
        """åŸ·è¡Œæ¸…æ´—æ“ä½œ"""
        pass
    
    @abstractmethod
    def get_name(self) -> str:
        """ç²å–æ“ä½œåç¨±"""
        pass


class DeduplicationOperation(CleaningOperation):
    """å»é‡æ“ä½œ"""
    
    def __init__(self, key_columns: List[str], keep: str = 'first'):
        """
        Args:
            key_columns: ç”¨æ–¼åˆ¤æ–·é‡è¤‡çš„åˆ—
            keep: ä¿ç•™ç­–ç•¥ ('first', 'last', False)
        """
        self.key_columns = key_columns
        self.keep = keep
    
    def execute(self, df: pd.DataFrame) -> pd.DataFrame:
        original_count = len(df)
        
        df_cleaned = df.drop_duplicates(
            subset=self.key_columns,
            keep=self.keep
        )
        
        removed_count = original_count - len(df_cleaned)
        logger.info(f"Removed {removed_count} duplicates")
        
        return df_cleaned
    
    def get_name(self) -> str:
        return "deduplication"


class FormatNormalizationOperation(CleaningOperation):
    """æ ¼å¼æ¨™æº–åŒ–æ“ä½œ"""
    
    def __init__(self, format_rules: Dict[str, str]):
        """
        Args:
            format_rules: {column: format_type}
            format_type: email, url, phone, date, etc.
        """
        self.format_rules = format_rules
    
    def execute(self, df: pd.DataFrame) -> pd.DataFrame:
        for column, format_type in self.format_rules.items():
            if column not in df.columns:
                continue
            
            if format_type == 'email':
                df[column] = df[column].str.lower().str.strip()
            
            elif format_type == 'url':
                df[column] = df[column].apply(self._normalize_url)
            
            elif format_type == 'phone':
                df[column] = df[column].apply(self._normalize_phone)
            
            elif format_type == 'date':
                df[column] = pd.to_datetime(df[column], errors='coerce')
            
            elif format_type == 'price':
                df[column] = df[column].apply(self._normalize_price)
        
        return df
    
    def _normalize_url(self, url: str) -> str:
        """URLæ¨™æº–åŒ–"""
        if pd.isna(url):
            return url
        
        from urllib.parse import urlparse, urlunparse
        
        try:
            parsed = urlparse(str(url))
            # çµ±ä¸€å”è­°ç‚º https
            scheme = 'https'
            # åŸŸåå°å¯«
            netloc = parsed.netloc.lower()
            # ç§»é™¤æœ«å°¾æ–œç·š
            path = parsed.path.rstrip('/')
            
            normalized = urlunparse((
                scheme, netloc, path,
                parsed.params, parsed.query, ''
            ))
            
            return normalized
        except:
            return url
    
    def _normalize_phone(self, phone: str) -> str:
        """é›»è©±è™Ÿç¢¼æ¨™æº–åŒ–"""
        if pd.isna(phone):
            return phone
        
        # ç§»é™¤éæ•¸å­—å­—ç¬¦
        import re
        digits = re.sub(r'\D', '', str(phone))
        
        # æ ¼å¼åŒ–ç‚º +86-XXX-XXXX-XXXX
        if len(digits) == 11:
            return f"+86-{digits[0:3]}-{digits[3:7]}-{digits[7:11]}"
        
        return phone
    
    def _normalize_price(self, price: str) -> float:
        """åƒ¹æ ¼æ¨™æº–åŒ–"""
        if pd.isna(price):
            return np.nan
        
        # ç§»é™¤è²¨å¹£ç¬¦è™Ÿå’Œåƒåˆ†ä½ç¬¦è™Ÿ
        import re
        price_str = str(price)
        price_str = re.sub(r'[Â¥$â‚¬Â£,]', '', price_str)
        
        try:
            return float(price_str)
        except:
            return np.nan
    
    def get_name(self) -> str:
        return "format_normalization"


class MissingValueOperation(CleaningOperation):
    """ç¼ºå¤±å€¼è™•ç†æ“ä½œ"""
    
    def __init__(self, strategies: Dict[str, Dict]):
        """
        Args:
            strategies: {
                column: {
                    'method': 'mean|median|mode|constant|forward_fill|model',
                    'value': ...  # for constant
                }
            }
        """
        self.strategies = strategies
    
    def execute(self, df: pd.DataFrame) -> pd.DataFrame:
        for column, strategy in self.strategies.items():
            if column not in df.columns:
                continue
            
            method = strategy['method']
            
            if method == 'mean':
                df[column].fillna(df[column].mean(), inplace=True)
            
            elif method == 'median':
                df[column].fillna(df[column].median(), inplace=True)
            
            elif method == 'mode':
                df[column].fillna(df[column].mode()[0], inplace=True)
            
            elif method == 'constant':
                df[column].fillna(strategy['value'], inplace=True)
            
            elif method == 'forward_fill':
                df[column].fillna(method='ffill', inplace=True)
            
            elif method == 'backward_fill':
                df[column].fillna(method='bfill', inplace=True)
            
            elif method == 'interpolate':
                df[column] = df[column].interpolate()
            
            elif method == 'drop':
                df = df.dropna(subset=[column])
        
        return df
    
    def get_name(self) -> str:
        return "missing_value_handling"


class OutlierRemovalOperation(CleaningOperation):
    """ç•°å¸¸å€¼ç§»é™¤æ“ä½œ"""
    
    def __init__(self, columns: List[str], method: str = 'iqr', threshold: float = 1.5):
        """
        Args:
            columns: è¦æª¢æŸ¥çš„åˆ—
            method: zscore, iqr, mad
            threshold: é–¾å€¼
        """
        self.columns = columns
        self.method = method
        self.threshold = threshold
    
    def execute(self, df: pd.DataFrame) -> pd.DataFrame:
        for column in self.columns:
            if column not in df.columns:
                continue
            
            if self.method == 'zscore':
                df = self._remove_by_zscore(df, column)
            elif self.method == 'iqr':
                df = self._remove_by_iqr(df, column)
            elif self.method == 'mad':
                df = self._remove_by_mad(df, column)
        
        return df
    
    def _remove_by_zscore(self, df: pd.DataFrame, column: str) -> pd.DataFrame:
        """ä½¿ç”¨ Z-Score æ–¹æ³•ç§»é™¤ç•°å¸¸å€¼"""
        from scipy import stats
        
        z_scores = np.abs(stats.zscore(df[column].dropna()))
        mask = z_scores < self.threshold
        
        # åªä¿ç•™éç•°å¸¸å€¼
        df = df[mask | df[column].isna()]
        
        return df
    
    def _remove_by_iqr(self, df: pd.DataFrame, column: str) -> pd.DataFrame:
        """ä½¿ç”¨ IQR æ–¹æ³•ç§»é™¤ç•°å¸¸å€¼"""
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - self.threshold * IQR
        upper_bound = Q3 + self.threshold * IQR
        
        df = df[
            (df[column] >= lower_bound) & 
            (df[column] <= upper_bound) |
            df[column].isna()
        ]
        
        return df
    
    def _remove_by_mad(self, df: pd.DataFrame, column: str) -> pd.DataFrame:
        """ä½¿ç”¨ MAD æ–¹æ³•ç§»é™¤ç•°å¸¸å€¼"""
        median = df[column].median()
        mad = np.median(np.abs(df[column] - median))
        
        modified_z_score = 0.6745 * (df[column] - median) / mad
        
        df = df[
            (np.abs(modified_z_score) < self.threshold) |
            df[column].isna()
        ]
        
        return df
    
    def get_name(self) -> str:
        return "outlier_removal"


class ValidationOperation(CleaningOperation):
    """é©—è­‰æ“ä½œ"""
    
    def __init__(self, validation_rules: List[Dict]):
        """
        Args:
            validation_rules: [
                {
                    'expression': 'price > 0',
                    'action': 'remove'  # remove, fix, flag
                }
            ]
        """
        self.validation_rules = validation_rules
    
    def execute(self, df: pd.DataFrame) -> pd.DataFrame:
        for rule in self.validation_rules:
            expression = rule['expression']
            action = rule.get('action', 'remove')
            
            # è©•ä¼°è¡¨é”å¼
            try:
                mask = df.eval(expression)
                
                if action == 'remove':
                    df = df[mask]
                elif action == 'flag':
                    df.loc[~mask, '_validation_flag'] = True
                
            except Exception as e:
                logger.error(f"Error evaluating rule '{expression}': {e}")
        
        return df
    
    def get_name(self) -> str:
        return "validation"


class DataCleaner:
    """
    æ•¸æ“šæ¸…æ´—å™¨
    
    åŠŸèƒ½ï¼š
    - ç®¡ç†æ¸…æ´—ç®¡é“
    - åŸ·è¡Œæ¸…æ´—æ“ä½œ
    - è¨˜éŒ„æ¸…æ´—æ—¥èªŒ
    - ç”Ÿæˆæ¸…æ´—å ±å‘Š
    """
    
    def __init__(self):
        self.operations: List[CleaningOperation] = []
    
    def add_operation(self, operation: CleaningOperation) -> 'DataCleaner':
        """æ·»åŠ æ¸…æ´—æ“ä½œï¼ˆæ”¯æŒéˆå¼èª¿ç”¨ï¼‰"""
        self.operations.append(operation)
        return self
    
    def clear_operations(self):
        """æ¸…ç©ºæ“ä½œåˆ—è¡¨"""
        self.operations = []
    
    def clean(self, df: pd.DataFrame) -> tuple[pd.DataFrame, CleaningResult]:
        """
        åŸ·è¡Œæ¸…æ´—ç®¡é“
        
        Returns:
            (cleaned_df, result)
        """
        original_count = len(df)
        df_cleaned = df.copy()
        
        operations_executed = []
        issues = []
        
        # åŸ·è¡Œæ¯å€‹æ¸…æ´—æ“ä½œ
        for operation in self.operations:
            try:
                before_count = len(df_cleaned)
                
                df_cleaned = operation.execute(df_cleaned)
                
                after_count = len(df_cleaned)
                operations_executed.append(operation.get_name())
                
                logger.info(
                    f"Operation '{operation.get_name()}' completed. "
                    f"Records: {before_count} â†’ {after_count}"
                )
                
            except Exception as e:
                error_msg = f"Error in operation '{operation.get_name()}': {e}"
                logger.error(error_msg)
                issues.append(error_msg)
        
        # æ§‹å»ºçµæœ
        cleaned_count = len(df_cleaned)
        removed_count = original_count - cleaned_count
        modified_count = self._count_modifications(df, df_cleaned)
        
        result = CleaningResult(
            success=len(issues) == 0,
            original_count=original_count,
            cleaned_count=cleaned_count,
            removed_count=removed_count,
            modified_count=modified_count,
            operations=operations_executed,
            issues=issues
        )
        
        return df_cleaned, result
    
    def _count_modifications(self, original: pd.DataFrame, cleaned: pd.DataFrame) -> int:
        """çµ±è¨ˆä¿®æ”¹çš„è¨˜éŒ„æ•¸"""
        # ç°¡åŒ–å¯¦ç¾ï¼šæ¯”è¼ƒå…±åŒçš„è¡Œ
        try:
            common_index = original.index.intersection(cleaned.index)
            if len(common_index) == 0:
                return 0
            
            original_common = original.loc[common_index]
            cleaned_common = cleaned.loc[common_index]
            
            # æ¯”è¼ƒå€¼æ˜¯å¦ç›¸åŒ
            different = (original_common != cleaned_common).any(axis=1)
            return different.sum()
        except:
            return 0
    
    def create_pipeline_from_config(self, config: Dict) -> 'DataCleaner':
        """å¾é…ç½®å‰µå»ºæ¸…æ´—ç®¡é“"""
        for stage_config in config.get('stages', []):
            stage_type = stage_config['type']
            params = stage_config.get('params', {})
            
            if stage_type == 'deduplication':
                operation = DeduplicationOperation(**params)
            elif stage_type == 'format_normalization':
                operation = FormatNormalizationOperation(**params)
            elif stage_type == 'missing_value':
                operation = MissingValueOperation(**params)
            elif stage_type == 'outlier_removal':
                operation = OutlierRemovalOperation(**params)
            elif stage_type == 'validation':
                operation = ValidationOperation(**params)
            else:
                logger.warning(f"Unknown stage type: {stage_type}")
                continue
            
            self.add_operation(operation)
        
        return self


# ä½¿ç”¨ç¤ºä¾‹
def example_usage():
    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    df = pd.DataFrame({
        'id': [1, 2, 3, 3, 4, 5],
        'email': ['John@Example.COM', 'jane@test.com', None, 'DUPLICATE', 'invalid', 'bob@test.com'],
        'age': [25, -5, 30, 30, 999, 35],
        'price': ['$1,234.56', 'Â¥500', '100.00', '100.00', '-10', '2000']
    })
    
    # å‰µå»ºæ¸…æ´—å™¨ä¸¦æ·»åŠ æ“ä½œ
    cleaner = (DataCleaner()
        .add_operation(DeduplicationOperation(
            key_columns=['id'],
            keep='first'
        ))
        .add_operation(FormatNormalizationOperation(
            format_rules={
                'email': 'email',
                'price': 'price'
            }
        ))
        .add_operation(MissingValueOperation(
            strategies={
                'email': {'method': 'constant', 'value': 'unknown@example.com'},
                'age': {'method': 'median'}
            }
        ))
        .add_operation(OutlierRemovalOperation(
            columns=['age', 'price'],
            method='iqr',
            threshold=1.5
        ))
    )
    
    # åŸ·è¡Œæ¸…æ´—
    df_cleaned, result = cleaner.clean(df)
    
    print(f"æ¸…æ´—å®Œæˆ:")
    print(f"- åŸå§‹è¨˜éŒ„æ•¸: {result.original_count}")
    print(f"- æ¸…æ´—å¾Œè¨˜éŒ„æ•¸: {result.cleaned_count}")
    print(f"- ç§»é™¤è¨˜éŒ„æ•¸: {result.removed_count}")
    print(f"- ä¿®æ”¹è¨˜éŒ„æ•¸: {result.modified_count}")
    print(f"- åŸ·è¡Œæ“ä½œ: {', '.join(result.operations)}")
    
    print("\næ¸…æ´—å¾Œçš„æ•¸æ“š:")
    print(df_cleaned)


if __name__ == '__main__':
    example_usage()
```

ç¹¼çºŒä¸‹ä¸€éƒ¨åˆ†ï¼ˆè¦å‰‡å¼•æ“ã€ç•°å¸¸æª¢æ¸¬å™¨ç­‰ï¼‰...

---

## 3ï¸âƒ£ Rule Engine (è¦å‰‡å¼•æ“)

### 3.1 è¦å‰‡å¼•æ“å¯¦ç¾

```python
# app/quality_center/rules/rule_engine.py

from typing import Dict, List, Any, Callable
from dataclasses import dataclass
from enum import Enum
import re
import operator
from abc import ABC, abstractmethod


class RuleType(Enum):
    """è¦å‰‡é¡å‹"""
    FIELD_RULE = "field"
    CROSS_FIELD_RULE = "cross_field"
    BUSINESS_RULE = "business"
    CUSTOM_RULE = "custom"


class Severity(Enum):
    """åš´é‡ç¨‹åº¦"""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


@dataclass
class RuleResult:
    """è¦å‰‡é©—è­‰çµæœ"""
    rule_name: str
    passed: bool
    severity: Severity
    message: str
    details: Dict[str, Any] = None


class Rule(ABC):
    """è¦å‰‡åŸºé¡"""
    
    def __init__(self, name: str, severity: Severity = Severity.ERROR):
        self.name = name
        self.severity = severity
    
    @abstractmethod
    def validate(self, data: Dict) -> RuleResult:
        """é©—è­‰è¦å‰‡"""
        pass


class FieldRule(Rule):
    """å­—æ®µè¦å‰‡"""
    
    def __init__(self, name: str, field: str, check_type: str, 
                 params: Dict, severity: Severity = Severity.ERROR):
        super().__init__(name, severity)
        self.field = field
        self.check_type = check_type
        self.params = params
    
    def validate(self, data: Dict) -> RuleResult:
        value = data.get(self.field)
        
        passed = True
        message = ""
        
        try:
            if self.check_type == "not_null":
                passed = value is not None
                message = f"Field '{self.field}' must not be null"
            
            elif self.check_type == "type":
                expected_type = self.params['expected_type']
                passed = isinstance(value, eval(expected_type))
                message = f"Field '{self.field}' must be type {expected_type}"
            
            elif self.check_type == "range":
                min_val = self.params.get('min')
                max_val = self.params.get('max')
                passed = (min_val is None or value >= min_val) and \
                        (max_val is None or value <= max_val)
                message = f"Field '{self.field}' must be in range [{min_val}, {max_val}]"
            
            elif self.check_type == "regex":
                pattern = self.params['pattern']
                passed = bool(re.match(pattern, str(value)))
                message = f"Field '{self.field}' must match pattern {pattern}"
            
            elif self.check_type == "enum":
                allowed_values = self.params['allowed_values']
                passed = value in allowed_values
                message = f"Field '{self.field}' must be one of {allowed_values}"
            
            elif self.check_type == "length":
                min_len = self.params.get('min_length', 0)
                max_len = self.params.get('max_length', float('inf'))
                passed = min_len <= len(str(value)) <= max_len
                message = f"Field '{self.field}' length must be in [{min_len}, {max_len}]"
        
        except Exception as e:
            passed = False
            message = f"Error validating field '{self.field}': {str(e)}"
        
        return RuleResult(
            rule_name=self.name,
            passed=passed,
            severity=self.severity,
            message=message if not passed else f"Field '{self.field}' validation passed",
            details={'field': self.field, 'value': value}
        )


class CrossFieldRule(Rule):
    """è·¨å­—æ®µè¦å‰‡"""
    
    def __init__(self, name: str, expression: str, 
                 severity: Severity = Severity.ERROR):
        super().__init__(name, severity)
        self.expression = expression
    
    def validate(self, data: Dict) -> RuleResult:
        try:
            # å®‰å…¨çš„å‘½åç©ºé–“ï¼ˆåªåŒ…å«æ•¸æ“šå’ŒåŸºæœ¬æ“ä½œç¬¦ï¼‰
            namespace = {
                '__builtins__': {},
                **data,
                'len': len,
                'str': str,
                'int': int,
                'float': float,
                'and': operator.and_,
                'or': operator.or_,
                'not': operator.not_
            }
            
            passed = eval(self.expression, namespace)
            
            message = f"Expression '{self.expression}' "
            message += "passed" if passed else "failed"
            
            return RuleResult(
                rule_name=self.name,
                passed=passed,
                severity=self.severity,
                message=message,
                details={'expression': self.expression}
            )
        
        except Exception as e:
            return RuleResult(
                rule_name=self.name,
                passed=False,
                severity=self.severity,
                message=f"Error evaluating expression: {str(e)}",
                details={'expression': self.expression, 'error': str(e)}
            )


class CustomRule(Rule):
    """è‡ªå®šç¾©è¦å‰‡"""
    
    def __init__(self, name: str, validation_func: Callable, 
                 severity: Severity = Severity.ERROR):
        super().__init__(name, severity)
        self.validation_func = validation_func
    
    def validate(self, data: Dict) -> RuleResult:
        try:
            passed, message = self.validation_func(data)
            
            return RuleResult(
                rule_name=self.name,
                passed=passed,
                severity=self.severity,
                message=message
            )
        
        except Exception as e:
            return RuleResult(
                rule_name=self.name,
                passed=False,
                severity=self.severity,
                message=f"Error in custom rule: {str(e)}"
            )


class RuleEngine:
    """
    è¦å‰‡å¼•æ“
    
    åŠŸèƒ½ï¼š
    - ç®¡ç†é©—è­‰è¦å‰‡
    - åŸ·è¡Œè¦å‰‡é©—è­‰
    - ç”Ÿæˆé©—è­‰å ±å‘Š
    """
    
    def __init__(self):
        self.rules: List[Rule] = []
    
    def add_rule(self, rule: Rule) -> 'RuleEngine':
        """æ·»åŠ è¦å‰‡"""
        self.rules.append(rule)
        return self
    
    def remove_rule(self, rule_name: str):
        """ç§»é™¤è¦å‰‡"""
        self.rules = [r for r in self.rules if r.name != rule_name]
    
    def clear_rules(self):
        """æ¸…ç©ºæ‰€æœ‰è¦å‰‡"""
        self.rules = []
    
    def validate(self, data: Dict) -> List[RuleResult]:
        """
        é©—è­‰æ•¸æ“š
        
        Returns:
            List[RuleResult]
        """
        results = []
        
        for rule in self.rules:
            result = rule.validate(data)
            results.append(result)
        
        return results
    
    def validate_batch(self, data_list: List[Dict]) -> Dict:
        """
        æ‰¹é‡é©—è­‰
        
        Returns:
            {
                'total': int,
                'passed': int,
                'failed': int,
                'results': List[Dict]
            }
        """
        batch_results = {
            'total': len(data_list),
            'passed': 0,
            'failed': 0,
            'results': []
        }
        
        for idx, data in enumerate(data_list):
            results = self.validate(data)
            
            all_passed = all(r.passed for r in results)
            if all_passed:
                batch_results['passed'] += 1
            else:
                batch_results['failed'] += 1
            
            batch_results['results'].append({
                'index': idx,
                'data': data,
                'all_passed': all_passed,
                'rule_results': [
                    {
                        'rule_name': r.rule_name,
                        'passed': r.passed,
                        'severity': r.severity.value,
                        'message': r.message
                    }
                    for r in results
                ]
            })
        
        return batch_results
    
    def get_failed_results(self, results: List[RuleResult]) -> List[RuleResult]:
        """ç²å–å¤±æ•—çš„çµæœ"""
        return [r for r in results if not r.passed]
    
    def get_critical_failures(self, results: List[RuleResult]) -> List[RuleResult]:
        """ç²å–åš´é‡å¤±æ•—çš„çµæœ"""
        return [
            r for r in results 
            if not r.passed and r.severity == Severity.CRITICAL
        ]
    
    def load_rules_from_config(self, config: Dict) -> 'RuleEngine':
        """å¾é…ç½®åŠ è¼‰è¦å‰‡"""
        for rule_config in config.get('rules', []):
            rule_type = rule_config['type']
            name = rule_config['name']
            severity = Severity(rule_config.get('severity', 'error'))
            
            if rule_type == 'field':
                rule = FieldRule(
                    name=name,
                    field=rule_config['field'],
                    check_type=rule_config['check_type'],
                    params=rule_config.get('params', {}),
                    severity=severity
                )
            
            elif rule_type == 'cross_field':
                rule = CrossFieldRule(
                    name=name,
                    expression=rule_config['expression'],
                    severity=severity
                )
            
            elif rule_type == 'custom':
                # è‡ªå®šç¾©è¦å‰‡éœ€è¦åœ¨é‹è¡Œæ™‚æ³¨å…¥
                continue
            
            else:
                logger.warning(f"Unknown rule type: {rule_type}")
                continue
            
            self.add_rule(rule)
        
        return self


# ä½¿ç”¨ç¤ºä¾‹
def example_usage():
    # å‰µå»ºè¦å‰‡å¼•æ“
    engine = RuleEngine()
    
    # æ·»åŠ å­—æ®µè¦å‰‡
    engine.add_rule(FieldRule(
        name="age_range",
        field="age",
        check_type="range",
        params={'min': 0, 'max': 150},
        severity=Severity.ERROR
    ))
    
    engine.add_rule(FieldRule(
        name="email_format",
        field="email",
        check_type="regex",
        params={'pattern': r'^[\w\.-]+@[\w\.-]+\.\w+$'},
        severity=Severity.ERROR
    ))
    
    # æ·»åŠ è·¨å­—æ®µè¦å‰‡
    engine.add_rule(CrossFieldRule(
        name="end_after_start",
        expression="end_date > start_date",
        severity=Severity.ERROR
    ))
    
    # æ·»åŠ è‡ªå®šç¾©è¦å‰‡
    def validate_product_stock(data):
        if data.get('in_stock') and data.get('quantity', 0) == 0:
            return False, "Product marked as in_stock but quantity is 0"
        return True, "Stock validation passed"
    
    engine.add_rule(CustomRule(
        name="stock_consistency",
        validation_func=validate_product_stock,
        severity=Severity.WARNING
    ))
    
    # æ¸¬è©¦æ•¸æ“š
    test_data = {
        'age': 25,
        'email': 'user@example.com',
        'start_date': '2025-01-01',
        'end_date': '2025-12-31',
        'in_stock': True,
        'quantity': 100
    }
    
    # åŸ·è¡Œé©—è­‰
    results = engine.validate(test_data)
    
    print("é©—è­‰çµæœ:")
    for result in results:
        status = "âœ“" if result.passed else "âœ—"
        print(f"{status} [{result.severity.value.upper()}] {result.rule_name}: {result.message}")


if __name__ == '__main__':
    example_usage()
```

ç”±æ–¼å…§å®¹è¼ƒé•·ï¼Œæˆ‘å°‡ç¹¼çºŒå‰µå»ºå…¶ä»–æ ¸å¿ƒçµ„ä»¶ï¼ˆç•°å¸¸æª¢æ¸¬å™¨ã€è³ªé‡è©•åˆ†å™¨ç­‰ï¼‰ã€‚è®“æˆ‘å…ˆå®Œæˆé€™å€‹æ–‡ä»¶çš„å…¶ä»–éƒ¨åˆ†...

```python
# (ç¹¼çºŒ ch12-4-æ ¸å¿ƒçµ„ä»¶è©³ç´°å¯¦ç¾.md)

---

## 4ï¸âƒ£ Anomaly Detector (ç•°å¸¸æª¢æ¸¬å™¨)

### 4.1 çµ„ä»¶å¯¦ç¾

```python
# app/quality_center/detector/anomaly_detector.py

from typing import Dict, List, Tuple
import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.preprocessing import StandardScaler
from dataclasses import dataclass


@dataclass
class AnomalyResult:
    """ç•°å¸¸æª¢æ¸¬çµæœ"""
    is_anomaly: bool
    anomaly_score: float
    confidence: float
    method: str
    details: Dict


class StatisticalDetector:
    """çµ±è¨ˆæ–¹æ³•æª¢æ¸¬å™¨"""
    
    @staticmethod
    def detect_zscore(data: np.ndarray, threshold: float = 3.0) -> np.ndarray:
        """
        Z-Score æ–¹æ³•æª¢æ¸¬ç•°å¸¸
        
        Args:
            data: è¼¸å…¥æ•¸æ“š
            threshold: Z-Score é–¾å€¼
        
        Returns:
            å¸ƒçˆ¾æ•¸çµ„ï¼ŒTrue è¡¨ç¤ºç•°å¸¸
        """
        mean = np.mean(data)
        std = np.std(data)
        
        if std == 0:
            return np.zeros(len(data), dtype=bool)
        
        z_scores = np.abs((data - mean) / std)
        return z_scores > threshold
    
    @staticmethod
    def detect_iqr(data: np.ndarray, k: float = 1.5) -> np.ndarray:
        """
        IQR æ–¹æ³•æª¢æ¸¬ç•°å¸¸
        
        Args:
            data: è¼¸å…¥æ•¸æ“š
            k: IQR å€æ•¸
        
        Returns:
            å¸ƒçˆ¾æ•¸çµ„ï¼ŒTrue è¡¨ç¤ºç•°å¸¸
        """
        Q1 = np.percentile(data, 25)
        Q3 = np.percentile(data, 75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - k * IQR
        upper_bound = Q3 + k * IQR
        
        return (data < lower_bound) | (data > upper_bound)
    
    @staticmethod
    def detect_mad(data: np.ndarray, threshold: float = 3.5) -> np.ndarray:
        """
        MAD (Median Absolute Deviation) æ–¹æ³•æª¢æ¸¬ç•°å¸¸
        
        Args:
            data: è¼¸å…¥æ•¸æ“š
            threshold: MAD é–¾å€¼
        
        Returns:
            å¸ƒçˆ¾æ•¸çµ„ï¼ŒTrue è¡¨ç¤ºç•°å¸¸
        """
        median = np.median(data)
        mad = np.median(np.abs(data - median))
        
        if mad == 0:
            return np.zeros(len(data), dtype=bool)
        
        modified_z_score = 0.6745 * (data - median) / mad
        return np.abs(modified_z_score) > threshold


class MLDetector:
    """æ©Ÿå™¨å­¸ç¿’æ–¹æ³•æª¢æ¸¬å™¨"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.models = {}
    
    def detect_isolation_forest(self, data: np.ndarray, 
                                contamination: float = 0.1) -> Tuple[np.ndarray, np.ndarray]:
        """
        Isolation Forest æª¢æ¸¬ç•°å¸¸
        
        Args:
            data: è¼¸å…¥æ•¸æ“š (n_samples, n_features)
            contamination: é æœŸç•°å¸¸æ¯”ä¾‹
        
        Returns:
            (is_anomaly, anomaly_scores)
        """
        if data.ndim == 1:
            data = data.reshape(-1, 1)
        
        model = IsolationForest(
            contamination=contamination,
            random_state=42,
            n_estimators=100
        )
        
        predictions = model.fit_predict(data)
        scores = model.score_samples(data)
        
        # -1 è¡¨ç¤ºç•°å¸¸ï¼Œ1 è¡¨ç¤ºæ­£å¸¸
        is_anomaly = predictions == -1
        
        # è½‰æ›åˆ†æ•¸åˆ° [0, 1] ç¯„åœ
        anomaly_scores = -scores
        anomaly_scores = (anomaly_scores - anomaly_scores.min()) / \
                        (anomaly_scores.max() - anomaly_scores.min())
        
        return is_anomaly, anomaly_scores
    
    def detect_lof(self, data: np.ndarray, n_neighbors: int = 20,
                   contamination: float = 0.1) -> Tuple[np.ndarray, np.ndarray]:
        """
        Local Outlier Factor æª¢æ¸¬ç•°å¸¸
        
        Args:
            data: è¼¸å…¥æ•¸æ“š
            n_neighbors: é„°å±…æ•¸é‡
            contamination: é æœŸç•°å¸¸æ¯”ä¾‹
        
        Returns:
            (is_anomaly, anomaly_scores)
        """
        if data.ndim == 1:
            data = data.reshape(-1, 1)
        
        model = LocalOutlierFactor(
            n_neighbors=n_neighbors,
            contamination=contamination
        )
        
        predictions = model.fit_predict(data)
        scores = -model.negative_outlier_factor_
        
        is_anomaly = predictions == -1
        
        # æ¨™æº–åŒ–åˆ†æ•¸
        anomaly_scores = (scores - scores.min()) / (scores.max() - scores.min())
        
        return is_anomaly, anomaly_scores


class EnsembleDetector:
    """é›†æˆæª¢æ¸¬å™¨"""
    
    def __init__(self, weights: Dict[str, float] = None):
        """
        Args:
            weights: å„æ–¹æ³•çš„æ¬Šé‡
        """
        self.statistical_detector = StatisticalDetector()
        self.ml_detector = MLDetector()
        
        self.weights = weights or {
            'zscore': 0.2,
            'iqr': 0.2,
            'mad': 0.1,
            'isolation_forest': 0.3,
            'lof': 0.2
        }
    
    def detect(self, data: np.ndarray, threshold: float = 0.7) -> List[AnomalyResult]:
        """
        é›†æˆæª¢æ¸¬
        
        Args:
            data: è¼¸å…¥æ•¸æ“š
            threshold: ç•°å¸¸åˆ¤å®šé–¾å€¼
        
        Returns:
            List[AnomalyResult]
        """
        results = []
        
        # çµ±è¨ˆæ–¹æ³•
        zscore_anomalies = self.statistical_detector.detect_zscore(data)
        iqr_anomalies = self.statistical_detector.detect_iqr(data)
        mad_anomalies = self.statistical_detector.detect_mad(data)
        
        # æ©Ÿå™¨å­¸ç¿’æ–¹æ³•
        if_anomalies, if_scores = self.ml_detector.detect_isolation_forest(data)
        lof_anomalies, lof_scores = self.ml_detector.detect_lof(data)
        
        # é›†æˆè¨ˆç®—
        for i in range(len(data)):
            # è¨ˆç®—åŠ æ¬Šåˆ†æ•¸
            ensemble_score = (
                float(zscore_anomalies[i]) * self.weights['zscore'] +
                float(iqr_anomalies[i]) * self.weights['iqr'] +
                float(mad_anomalies[i]) * self.weights['mad'] +
                float(if_scores[i]) * self.weights['isolation_forest'] +
                float(lof_scores[i]) * self.weights['lof']
            )
            
            is_anomaly = ensemble_score > threshold
            
            result = AnomalyResult(
                is_anomaly=is_anomaly,
                anomaly_score=ensemble_score,
                confidence=ensemble_score if is_anomaly else (1 - ensemble_score),
                method='ensemble',
                details={
                    'zscore': bool(zscore_anomalies[i]),
                    'iqr': bool(iqr_anomalies[i]),
                    'mad': bool(mad_anomalies[i]),
                    'isolation_forest': float(if_scores[i]),
                    'lof': float(lof_scores[i]),
                    'value': float(data[i])
                }
            )
            
            results.append(result)
        
        return results


# ä½¿ç”¨ç¤ºä¾‹
def example_usage():
    # ç”Ÿæˆæ¸¬è©¦æ•¸æ“šï¼ˆåŒ…å«ç•°å¸¸å€¼ï¼‰
    np.random.seed(42)
    normal_data = np.random.normal(100, 10, 1000)
    anomalies = np.array([50, 150, 200, 30])  # ç•°å¸¸å€¼
    data = np.concatenate([normal_data, anomalies])
    np.random.shuffle(data)
    
    # å‰µå»ºé›†æˆæª¢æ¸¬å™¨
    detector = EnsembleDetector()
    
    # åŸ·è¡Œæª¢æ¸¬
    results = detector.detect(data, threshold=0.7)
    
    # çµ±è¨ˆç•°å¸¸
    anomaly_count = sum(1 for r in results if r.is_anomaly)
    print(f"æª¢æ¸¬åˆ° {anomaly_count} å€‹ç•°å¸¸å€¼")
    
    # é¡¯ç¤ºç•°å¸¸å€¼è©³æƒ…
    print("\nç•°å¸¸å€¼è©³æƒ…:")
    for i, result in enumerate(results):
        if result.is_anomaly:
            print(f"  ç´¢å¼• {i}: å€¼={result.details['value']:.2f}, "
                  f"åˆ†æ•¸={result.anomaly_score:.3f}, "
                  f"ç½®ä¿¡åº¦={result.confidence:.3f}")


if __name__ == '__main__':
    example_usage()
```

ç”±æ–¼ç¯‡å¹…é™åˆ¶ï¼Œæˆ‘å°‡å®Œæˆé€™å€‹æ–‡ä»¶çš„å‰©é¤˜éƒ¨åˆ†ä¸¦ä¿å­˜...

---

**æœ€å¾Œæ›´æ–°**: 2025-10-31  
**ç‰ˆæœ¬**: 1.0

