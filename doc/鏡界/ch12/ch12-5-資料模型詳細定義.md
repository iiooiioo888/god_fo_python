# ç¬¬12ç« ï¼šæ•¸æ“šè³ªé‡ç®¡ç†ä¸­å¿ƒ

## 12.5 è³‡æ–™æ¨¡å‹è©³ç´°å®šç¾©

**[â† è¿”å›ç¬¬12ç« é¦–é ](ch12-index.md)**

---

## ğŸ“Š æ•¸æ“šæ¨¡å‹æ¦‚è¦½

```
Ch12 æ•¸æ“šè³ªé‡ç®¡ç†ä¸­å¿ƒ - æ•¸æ“šæ¨¡å‹
â”œâ”€â”€ 1. è³ªé‡å…ƒæ•¸æ“šæ¨¡å‹
â”‚   â”œâ”€â”€ è³ªé‡è¦å‰‡ (Quality Rules)
â”‚   â”œâ”€â”€ è³ªé‡æª¢æŸ¥ (Quality Checks)
â”‚   â””â”€â”€ è³ªé‡æŒ‡æ¨™ (Quality Metrics)
â”œâ”€â”€ 2. æ¸…æ´—é…ç½®æ¨¡å‹
â”‚   â”œâ”€â”€ æ¸…æ´—è¦å‰‡ (Cleaning Rules)
â”‚   â”œâ”€â”€ æ¸…æ´—ç®¡é“ (Cleaning Pipelines)
â”‚   â””â”€â”€ æ¸…æ´—æ­·å² (Cleaning History)
â”œâ”€â”€ 3. ç•°å¸¸æª¢æ¸¬æ¨¡å‹
â”‚   â”œâ”€â”€ ç•°å¸¸è¨˜éŒ„ (Anomaly Records)
â”‚   â”œâ”€â”€ æª¢æ¸¬æ¨¡å‹ (Detection Models)
â”‚   â””â”€â”€ ç•°å¸¸å‘Šè­¦ (Anomaly Alerts)
â”œâ”€â”€ 4. æ•¸æ“šè¡€ç·£æ¨¡å‹
â”‚   â”œâ”€â”€ æ•¸æ“šå¯¦é«” (Data Entities)
â”‚   â”œâ”€â”€ è¡€ç·£é—œä¿‚ (Lineage Relations)
â”‚   â””â”€â”€ è½‰æ›é‚è¼¯ (Transformations)
â””â”€â”€ 5. è³ªé‡å ±å‘Šæ¨¡å‹
    â”œâ”€â”€ è³ªé‡å ±å‘Š (Quality Reports)
    â”œâ”€â”€ å•é¡Œè¿½è¹¤ (Issue Tracking)
    â””â”€â”€ ä¿®å¾©è¨˜éŒ„ (Fix Records)
```

---

## 1ï¸âƒ£ è³ªé‡å…ƒæ•¸æ“šæ¨¡å‹

### 1.1 è³ªé‡è¦å‰‡ (Quality Rules)

```sql
-- PostgreSQL Schema

CREATE TABLE quality_rules (
    id BIGSERIAL PRIMARY KEY,
    name VARCHAR(200) NOT NULL UNIQUE,
    description TEXT,
    rule_type VARCHAR(50) NOT NULL,  -- field, cross_field, business, custom
    category VARCHAR(100),            -- completeness, accuracy, consistency, etc.
    
    -- è¦å‰‡å®šç¾©
    rule_definition JSONB NOT NULL,
    /*
    {
        "field": "email",
        "check_type": "regex",
        "params": {
            "pattern": "^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$"
        }
    }
    */
    
    -- åŸ·è¡Œé…ç½®
    severity VARCHAR(20) NOT NULL,    -- info, warning, error, critical
    enabled BOOLEAN DEFAULT TRUE,
    priority INTEGER DEFAULT 0,
    execution_mode VARCHAR(20) DEFAULT 'sync',  -- sync, async
    
    -- é©ç”¨ç¯„åœ
    data_sources TEXT[],              -- é©ç”¨çš„æ•¸æ“šæº
    tables TEXT[],                    -- é©ç”¨çš„è¡¨
    conditions JSONB,                 -- æ‡‰ç”¨æ¢ä»¶
    
    -- ç•°å¸¸è™•ç†
    on_failure VARCHAR(20) DEFAULT 'reject',  -- reject, warn, fix, ignore
    auto_fix_enabled BOOLEAN DEFAULT FALSE,
    fix_strategy JSONB,
    
    -- å…ƒæ•¸æ“š
    version VARCHAR(20) DEFAULT '1.0',
    created_by VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_executed_at TIMESTAMP,
    
    -- çµ±è¨ˆä¿¡æ¯
    execution_count BIGINT DEFAULT 0,
    success_count BIGINT DEFAULT 0,
    failure_count BIGINT DEFAULT 0,
    
    -- ç´¢å¼•
    CONSTRAINT valid_rule_type CHECK (rule_type IN ('field', 'cross_field', 'business', 'custom')),
    CONSTRAINT valid_severity CHECK (severity IN ('info', 'warning', 'error', 'critical'))
);

-- ç´¢å¼•
CREATE INDEX idx_quality_rules_type ON quality_rules(rule_type);
CREATE INDEX idx_quality_rules_category ON quality_rules(category);
CREATE INDEX idx_quality_rules_enabled ON quality_rules(enabled);
CREATE INDEX idx_quality_rules_severity ON quality_rules(severity);

-- è¨»é‡‹
COMMENT ON TABLE quality_rules IS 'è³ªé‡è¦å‰‡å®šç¾©è¡¨';
COMMENT ON COLUMN quality_rules.rule_definition IS 'è¦å‰‡å®šç¾©ï¼ˆJSONBæ ¼å¼ï¼‰';
COMMENT ON COLUMN quality_rules.fix_strategy IS 'è‡ªå‹•ä¿®å¾©ç­–ç•¥ï¼ˆJSONBæ ¼å¼ï¼‰';
```

### 1.2 è³ªé‡æª¢æŸ¥ (Quality Checks)

```sql
CREATE TABLE quality_checks (
    id BIGSERIAL PRIMARY KEY,
    check_id UUID UNIQUE DEFAULT gen_random_uuid(),
    
    -- æª¢æŸ¥ä¿¡æ¯
    data_source VARCHAR(200) NOT NULL,
    table_name VARCHAR(200),
    check_type VARCHAR(50) NOT NULL,  -- real_time, batch, scheduled
    
    -- åŸ·è¡Œä¿¡æ¯
    started_at TIMESTAMP NOT NULL,
    completed_at TIMESTAMP,
    duration_seconds NUMERIC(10, 2),
    status VARCHAR(20) NOT NULL,      -- pending, running, completed, failed
    
    -- æª¢æŸ¥çµæœ
    total_records BIGINT,
    valid_records BIGINT,
    invalid_records BIGINT,
    
    -- è³ªé‡åˆ†æ•¸
    quality_score NUMERIC(5, 2),
    completeness_score NUMERIC(5, 2),
    accuracy_score NUMERIC(5, 2),
    consistency_score NUMERIC(5, 2),
    timeliness_score NUMERIC(5, 2),
    uniqueness_score NUMERIC(5, 2),
    
    -- å•é¡Œçµ±è¨ˆ
    null_count BIGINT DEFAULT 0,
    duplicate_count BIGINT DEFAULT 0,
    format_error_count BIGINT DEFAULT 0,
    range_error_count BIGINT DEFAULT 0,
    business_rule_error_count BIGINT DEFAULT 0,
    
    -- è©³ç´°çµæœ
    issues JSONB,
    /*
    [
        {
            "rule_name": "email_format",
            "severity": "error",
            "affected_records": 150,
            "sample_records": [...]
        }
    ]
    */
    
    statistics JSONB,
    /*
    {
        "rules_checked": 25,
        "rules_passed": 23,
        "rules_failed": 2
    }
    */
    
    -- å…ƒæ•¸æ“š
    triggered_by VARCHAR(100),        -- scheduler, manual, api, event
    executed_by VARCHAR(100),
    
    -- ç´¢å¼•
    CONSTRAINT valid_check_type CHECK (check_type IN ('real_time', 'batch', 'scheduled')),
    CONSTRAINT valid_status CHECK (status IN ('pending', 'running', 'completed', 'failed'))
);

-- ç´¢å¼•
CREATE INDEX idx_quality_checks_source ON quality_checks(data_source);
CREATE INDEX idx_quality_checks_started_at ON quality_checks(started_at DESC);
CREATE INDEX idx_quality_checks_status ON quality_checks(status);
CREATE INDEX idx_quality_checks_score ON quality_checks(quality_score);

-- åˆ†å€ï¼ˆæŒ‰æœˆï¼‰
CREATE TABLE quality_checks_y2025m01 PARTITION OF quality_checks
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

COMMENT ON TABLE quality_checks IS 'è³ªé‡æª¢æŸ¥åŸ·è¡Œè¨˜éŒ„è¡¨';
```

### 1.3 è³ªé‡æŒ‡æ¨™ (Quality Metrics)

```sql
-- ä½¿ç”¨ InfluxDB å­˜å„²æ™‚åºæŒ‡æ¨™

-- InfluxDB Line Protocol ç¤ºä¾‹

-- å¯¦æ™‚è³ªé‡æŒ‡æ¨™
quality_metrics,source=ch8_crawler,table=products quality_score=95.2,completeness=98.0,accuracy=96.0 1698710400000000000

-- èšåˆæŒ‡æ¨™ï¼ˆæ¯åˆ†é˜ï¼‰
quality_metrics_1m,source=ch8_crawler,aggregation=1m total_records=10000,valid_records=9500,invalid_records=500 1698710400000000000

-- èšåˆæŒ‡æ¨™ï¼ˆæ¯å°æ™‚ï¼‰
quality_metrics_1h,source=ch8_crawler,aggregation=1h avg_quality_score=94.5,min_quality_score=88.0,max_quality_score=98.5 1698710400000000000

-- PostgreSQL è¦–åœ–ï¼ˆç”¨æ–¼æŸ¥è©¢åŒ¯ç¸½æ•¸æ“šï¼‰
CREATE MATERIALIZED VIEW quality_metrics_summary AS
SELECT 
    data_source,
    DATE_TRUNC('day', started_at) AS date,
    COUNT(*) AS check_count,
    AVG(quality_score) AS avg_quality_score,
    MIN(quality_score) AS min_quality_score,
    MAX(quality_score) AS max_quality_score,
    AVG(completeness_score) AS avg_completeness,
    AVG(accuracy_score) AS avg_accuracy,
    AVG(consistency_score) AS avg_consistency,
    AVG(timeliness_score) AS avg_timeliness,
    AVG(uniqueness_score) AS avg_uniqueness,
    SUM(total_records) AS total_records_checked,
    SUM(invalid_records) AS total_invalid_records
FROM quality_checks
WHERE status = 'completed'
GROUP BY data_source, DATE_TRUNC('day', started_at);

CREATE UNIQUE INDEX ON quality_metrics_summary(data_source, date);

-- å®šæœŸåˆ·æ–°ç‰©åŒ–è¦–åœ–ï¼ˆæ¯å°æ™‚ï¼‰
-- ä½¿ç”¨ pg_cron æˆ–å¤–éƒ¨èª¿åº¦å™¨
```

---

## 2ï¸âƒ£ æ¸…æ´—é…ç½®æ¨¡å‹

### 2.1 æ¸…æ´—è¦å‰‡ (Cleaning Rules)

```sql
CREATE TABLE cleaning_rules (
    id BIGSERIAL PRIMARY KEY,
    name VARCHAR(200) NOT NULL UNIQUE,
    description TEXT,
    operation_type VARCHAR(50) NOT NULL,  -- deduplication, normalization, missing_value, outlier, validation
    
    -- è¦å‰‡é…ç½®
    config JSONB NOT NULL,
    /*
    ç¤ºä¾‹é…ç½®ï¼š
    {
        "key_columns": ["url", "title"],
        "keep": "first",
        "similarity_threshold": 0.85
    }
    */
    
    -- é©ç”¨æ¢ä»¶
    data_sources TEXT[],
    tables TEXT[],
    conditions JSONB,
    
    -- åŸ·è¡Œé…ç½®
    enabled BOOLEAN DEFAULT TRUE,
    priority INTEGER DEFAULT 0,
    execution_order INTEGER,
    
    -- æ€§èƒ½é…ç½®
    batch_size INTEGER DEFAULT 1000,
    parallel_enabled BOOLEAN DEFAULT FALSE,
    max_workers INTEGER DEFAULT 4,
    
    -- å…ƒæ•¸æ“š
    version VARCHAR(20) DEFAULT '1.0',
    created_by VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- çµ±è¨ˆä¿¡æ¯
    execution_count BIGINT DEFAULT 0,
    total_records_processed BIGINT DEFAULT 0,
    total_records_modified BIGINT DEFAULT 0,
    avg_processing_time_ms NUMERIC(10, 2),
    
    CONSTRAINT valid_operation_type CHECK (operation_type IN (
        'deduplication', 'normalization', 'missing_value', 'outlier', 'validation', 'custom'
    ))
);

CREATE INDEX idx_cleaning_rules_type ON cleaning_rules(operation_type);
CREATE INDEX idx_cleaning_rules_enabled ON cleaning_rules(enabled);

COMMENT ON TABLE cleaning_rules IS 'æ•¸æ“šæ¸…æ´—è¦å‰‡è¡¨';
```

### 2.2 æ¸…æ´—ç®¡é“ (Cleaning Pipelines)

```sql
CREATE TABLE cleaning_pipelines (
    id BIGSERIAL PRIMARY KEY,
    name VARCHAR(200) NOT NULL UNIQUE,
    description TEXT,
    
    -- ç®¡é“é…ç½®
    stages JSONB NOT NULL,
    /*
    [
        {
            "order": 1,
            "rule_id": 101,
            "enabled": true
        },
        {
            "order": 2,
            "rule_id": 102,
            "enabled": true
        }
    ]
    */
    
    -- é©ç”¨ç¯„åœ
    data_sources TEXT[],
    tables TEXT[],
    
    -- åŸ·è¡Œç­–ç•¥
    execution_mode VARCHAR(20) DEFAULT 'sequential',  -- sequential, parallel
    continue_on_error BOOLEAN DEFAULT FALSE,
    rollback_on_failure BOOLEAN DEFAULT TRUE,
    
    -- ç‹€æ…‹
    enabled BOOLEAN DEFAULT TRUE,
    is_default BOOLEAN DEFAULT FALSE,
    
    -- å…ƒæ•¸æ“š
    version VARCHAR(20) DEFAULT '1.0',
    created_by VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- çµ±è¨ˆä¿¡æ¯
    execution_count BIGINT DEFAULT 0,
    success_count BIGINT DEFAULT 0,
    failure_count BIGINT DEFAULT 0,
    avg_duration_seconds NUMERIC(10, 2)
);

CREATE INDEX idx_cleaning_pipelines_enabled ON cleaning_pipelines(enabled);
CREATE INDEX idx_cleaning_pipelines_default ON cleaning_pipelines(is_default);

COMMENT ON TABLE cleaning_pipelines IS 'æ•¸æ“šæ¸…æ´—ç®¡é“é…ç½®è¡¨';
```

### 2.3 æ¸…æ´—æ­·å² (Cleaning History)

```sql
CREATE TABLE cleaning_history (
    id BIGSERIAL PRIMARY KEY,
    execution_id UUID UNIQUE DEFAULT gen_random_uuid(),
    
    -- åŸ·è¡Œä¿¡æ¯
    pipeline_id BIGINT REFERENCES cleaning_pipelines(id),
    data_source VARCHAR(200) NOT NULL,
    table_name VARCHAR(200),
    
    -- æ™‚é–“ä¿¡æ¯
    started_at TIMESTAMP NOT NULL,
    completed_at TIMESTAMP,
    duration_seconds NUMERIC(10, 2),
    
    -- ç‹€æ…‹
    status VARCHAR(20) NOT NULL,  -- pending, running, completed, failed, rolled_back
    
    -- è™•ç†çµæœ
    original_count BIGINT,
    cleaned_count BIGINT,
    removed_count BIGINT,
    modified_count BIGINT,
    
    -- è©³ç´°çµæœ
    operations_executed JSONB,
    /*
    [
        {
            "operation": "deduplication",
            "rule_id": 101,
            "records_affected": 1500,
            "duration_ms": 2350
        }
    ]
    */
    
    issues JSONB,
    
    -- å›æ»¾ä¿¡æ¯
    backup_location VARCHAR(500),
    rollbackable BOOLEAN DEFAULT TRUE,
    rolled_back_at TIMESTAMP,
    
    -- å…ƒæ•¸æ“š
    triggered_by VARCHAR(100),
    executed_by VARCHAR(100),
    
    CONSTRAINT valid_status CHECK (status IN (
        'pending', 'running', 'completed', 'failed', 'rolled_back'
    ))
);

CREATE INDEX idx_cleaning_history_pipeline ON cleaning_history(pipeline_id);
CREATE INDEX idx_cleaning_history_source ON cleaning_history(data_source);
CREATE INDEX idx_cleaning_history_started_at ON cleaning_history(started_at DESC);
CREATE INDEX idx_cleaning_history_status ON cleaning_history(status);

-- åˆ†å€ï¼ˆæŒ‰æœˆï¼‰
CREATE TABLE cleaning_history_y2025m01 PARTITION OF cleaning_history
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

COMMENT ON TABLE cleaning_history IS 'æ•¸æ“šæ¸…æ´—åŸ·è¡Œæ­·å²è¡¨';
```

---

## 3ï¸âƒ£ ç•°å¸¸æª¢æ¸¬æ¨¡å‹

### 3.1 ç•°å¸¸è¨˜éŒ„ (Anomaly Records)

```sql
CREATE TABLE anomaly_records (
    id BIGSERIAL PRIMARY KEY,
    anomaly_id UUID UNIQUE DEFAULT gen_random_uuid(),
    
    -- æ•¸æ“šæºä¿¡æ¯
    data_source VARCHAR(200) NOT NULL,
    table_name VARCHAR(200),
    record_id VARCHAR(200),
    
    -- ç•°å¸¸ä¿¡æ¯
    anomaly_type VARCHAR(50) NOT NULL,  -- outlier, pattern, trend, spike
    detection_method VARCHAR(50) NOT NULL,  -- zscore, iqr, isolation_forest, etc.
    
    -- ç•°å¸¸åˆ†æ•¸
    anomaly_score NUMERIC(5, 3) NOT NULL,  -- 0.000 - 1.000
    confidence NUMERIC(5, 3),
    severity VARCHAR(20),  -- low, medium, high, critical
    
    -- æ•¸æ“šè©³æƒ…
    original_data JSONB,
    affected_fields TEXT[],
    expected_value JSONB,
    actual_value JSONB,
    
    -- ä¸Šä¸‹æ–‡ä¿¡æ¯
    context JSONB,
    /*
    {
        "statistical_features": {
            "mean": 100,
            "std": 10,
            "z_score": 3.5
        },
        "temporal_context": {
            "time_window": "1h",
            "historical_avg": 95
        }
    }
    */
    
    -- æª¢æ¸¬æ™‚é–“
    detected_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    
    -- è™•ç†ç‹€æ…‹
    status VARCHAR(20) DEFAULT 'pending',  -- pending, reviewed, resolved, ignored
    reviewed_by VARCHAR(100),
    reviewed_at TIMESTAMP,
    resolution TEXT,
    
    -- å‘Šè­¦ä¿¡æ¯
    alert_sent BOOLEAN DEFAULT FALSE,
    alert_sent_at TIMESTAMP,
    
    CONSTRAINT valid_anomaly_type CHECK (anomaly_type IN (
        'outlier', 'pattern', 'trend', 'spike', 'dip', 'missing'
    )),
    CONSTRAINT valid_status CHECK (status IN (
        'pending', 'reviewed', 'resolved', 'ignored'
    ))
);

CREATE INDEX idx_anomaly_records_source ON anomaly_records(data_source);
CREATE INDEX idx_anomaly_records_detected_at ON anomaly_records(detected_at DESC);
CREATE INDEX idx_anomaly_records_score ON anomaly_records(anomaly_score DESC);
CREATE INDEX idx_anomaly_records_status ON anomaly_records(status);
CREATE INDEX idx_anomaly_records_severity ON anomaly_records(severity);

-- åˆ†å€ï¼ˆæŒ‰æœˆï¼‰
CREATE TABLE anomaly_records_y2025m01 PARTITION OF anomaly_records
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

COMMENT ON TABLE anomaly_records IS 'ç•°å¸¸è¨˜éŒ„è¡¨';
```

### 3.2 æª¢æ¸¬æ¨¡å‹ (Detection Models)

```sql
CREATE TABLE detection_models (
    id BIGSERIAL PRIMARY KEY,
    name VARCHAR(200) NOT NULL UNIQUE,
    description TEXT,
    
    -- æ¨¡å‹é¡å‹
    model_type VARCHAR(50) NOT NULL,  -- statistical, ml, dl, ensemble
    algorithm VARCHAR(100) NOT NULL,  -- zscore, isolation_forest, autoencoder, etc.
    
    -- æ¨¡å‹é…ç½®
    hyperparameters JSONB,
    /*
    {
        "contamination": 0.1,
        "n_estimators": 100,
        "max_samples": "auto"
    }
    */
    
    -- è¨“ç·´ä¿¡æ¯
    training_data_source VARCHAR(200),
    training_start_date DATE,
    training_end_date DATE,
    training_samples BIGINT,
    
    -- æ¨¡å‹æ€§èƒ½
    metrics JSONB,
    /*
    {
        "precision": 0.92,
        "recall": 0.88,
        "f1_score": 0.90,
        "auc_roc": 0.95
    }
    */
    
    -- æ¨¡å‹æ–‡ä»¶
    model_path VARCHAR(500),
    model_size_mb NUMERIC(10, 2),
    
    -- ç‰ˆæœ¬ä¿¡æ¯
    version VARCHAR(20),
    parent_model_id BIGINT REFERENCES detection_models(id),
    
    -- ç‹€æ…‹
    status VARCHAR(20) DEFAULT 'draft',  -- draft, training, active, archived
    enabled BOOLEAN DEFAULT FALSE,
    
    -- å…ƒæ•¸æ“š
    created_by VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    trained_at TIMESTAMP,
    deployed_at TIMESTAMP,
    
    -- ä½¿ç”¨çµ±è¨ˆ
    detection_count BIGINT DEFAULT 0,
    true_positive_count BIGINT DEFAULT 0,
    false_positive_count BIGINT DEFAULT 0,
    
    CONSTRAINT valid_model_type CHECK (model_type IN (
        'statistical', 'ml', 'dl', 'ensemble', 'rule_based'
    )),
    CONSTRAINT valid_status CHECK (status IN (
        'draft', 'training', 'active', 'archived'
    ))
);

CREATE INDEX idx_detection_models_type ON detection_models(model_type);
CREATE INDEX idx_detection_models_status ON detection_models(status);
CREATE INDEX idx_detection_models_enabled ON detection_models(enabled);

COMMENT ON TABLE detection_models IS 'ç•°å¸¸æª¢æ¸¬æ¨¡å‹é…ç½®è¡¨';
```

### 3.3 ç•°å¸¸å‘Šè­¦ (Anomaly Alerts)

```sql
CREATE TABLE anomaly_alerts (
    id BIGSERIAL PRIMARY KEY,
    alert_id UUID UNIQUE DEFAULT gen_random_uuid(),
    
    -- é—œè¯ç•°å¸¸
    anomaly_id UUID REFERENCES anomaly_records(anomaly_id),
    
    -- å‘Šè­¦ä¿¡æ¯
    alert_type VARCHAR(50) NOT NULL,  -- real_time, batch, threshold
    severity VARCHAR(20) NOT NULL,    -- low, medium, high, critical
    title VARCHAR(500) NOT NULL,
    message TEXT NOT NULL,
    
    -- å½±éŸ¿ç¯„åœ
    affected_systems TEXT[],
    affected_users TEXT[],
    estimated_impact VARCHAR(20),  -- low, medium, high
    
    -- å‘Šè­¦ç‹€æ…‹
    status VARCHAR(20) DEFAULT 'open',  -- open, acknowledged, investigating, resolved, closed
    
    -- è™•ç†ä¿¡æ¯
    assigned_to VARCHAR(100),
    acknowledged_by VARCHAR(100),
    acknowledged_at TIMESTAMP,
    resolved_by VARCHAR(100),
    resolved_at TIMESTAMP,
    resolution_time_minutes INTEGER,
    
    -- é€šçŸ¥ä¿¡æ¯
    notification_channels TEXT[],  -- email, slack, sms, webhook
    notified_at TIMESTAMP,
    notification_status JSONB,
    
    -- æ™‚é–“ä¿¡æ¯
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- å‡ç´šä¿¡æ¯
    escalated BOOLEAN DEFAULT FALSE,
    escalated_at TIMESTAMP,
    escalation_level INTEGER DEFAULT 0,
    
    CONSTRAINT valid_severity CHECK (severity IN (
        'low', 'medium', 'high', 'critical'
    )),
    CONSTRAINT valid_status CHECK (status IN (
        'open', 'acknowledged', 'investigating', 'resolved', 'closed'
    ))
);

CREATE INDEX idx_anomaly_alerts_anomaly_id ON anomaly_alerts(anomaly_id);
CREATE INDEX idx_anomaly_alerts_status ON anomaly_alerts(status);
CREATE INDEX idx_anomaly_alerts_severity ON anomaly_alerts(severity);
CREATE INDEX idx_anomaly_alerts_created_at ON anomaly_alerts(created_at DESC);

COMMENT ON TABLE anomaly_alerts IS 'ç•°å¸¸å‘Šè­¦è¡¨';
```

---

## 4ï¸âƒ£ æ•¸æ“šè¡€ç·£æ¨¡å‹

### 4.1 æ•¸æ“šå¯¦é«” (Data Entities)

```sql
CREATE TABLE data_entities (
    id BIGSERIAL PRIMARY KEY,
    entity_id UUID UNIQUE DEFAULT gen_random_uuid(),
    
    -- å¯¦é«”ä¿¡æ¯
    name VARCHAR(200) NOT NULL,
    qualified_name VARCHAR(500) UNIQUE NOT NULL,  -- database.schema.table
    entity_type VARCHAR(50) NOT NULL,  -- table, view, file, api, stream
    
    -- ä½ç½®ä¿¡æ¯
    database_name VARCHAR(200),
    schema_name VARCHAR(200),
    table_name VARCHAR(200),
    location VARCHAR(500),
    
    -- Schema ä¿¡æ¯
    columns JSONB,
    /*
    [
        {
            "name": "id",
            "type": "bigint",
            "nullable": false,
            "description": "ä¸»éµ"
        }
    ]
    */
    
    -- å…ƒæ•¸æ“š
    description TEXT,
    owner VARCHAR(100),
    tags TEXT[],
    properties JSONB,
    
    -- æ•¸æ“šçµ±è¨ˆ
    row_count BIGINT,
    size_bytes BIGINT,
    last_modified_at TIMESTAMP,
    
    -- è³ªé‡ä¿¡æ¯
    quality_score NUMERIC(5, 2),
    quality_last_checked_at TIMESTAMP,
    
    -- ç‹€æ…‹
    status VARCHAR(20) DEFAULT 'active',  -- active, deprecated, archived
    
    -- æ™‚é–“æˆ³
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    CONSTRAINT valid_entity_type CHECK (entity_type IN (
        'table', 'view', 'file', 'api', 'stream', 'topic'
    ))
);

CREATE INDEX idx_data_entities_type ON data_entities(entity_type);
CREATE INDEX idx_data_entities_qualified_name ON data_entities(qualified_name);
CREATE INDEX idx_data_entities_owner ON data_entities(owner);
CREATE INDEX idx_data_entities_status ON data_entities(status);

COMMENT ON TABLE data_entities IS 'æ•¸æ“šå¯¦é«”å®šç¾©è¡¨';
```

### 4.2 è¡€ç·£é—œä¿‚ (Lineage Relations)

```sql
CREATE TABLE lineage_relations (
    id BIGSERIAL PRIMARY KEY,
    
    -- æºå’Œç›®æ¨™
    source_entity_id UUID REFERENCES data_entities(entity_id),
    target_entity_id UUID REFERENCES data_entities(entity_id),
    
    -- é—œä¿‚é¡å‹
    relation_type VARCHAR(50) NOT NULL,  -- direct, derived, aggregated, joined
    
    -- è½‰æ›ä¿¡æ¯
    transformation_id BIGINT,  -- é—œè¯åˆ° transformations è¡¨
    
    -- å­—æ®µç´šè¡€ç·£
    field_mapping JSONB,
    /*
    {
        "source_fields": ["user_id", "email"],
        "target_fields": ["customer_id", "contact_email"],
        "mapping": [
            {"source": "user_id", "target": "customer_id"},
            {"source": "email", "target": "contact_email"}
        ]
    }
    */
    
    -- ä¾è³´ä¿¡æ¯
    dependency_type VARCHAR(20),  -- strong, weak
    is_critical BOOLEAN DEFAULT FALSE,
    
    -- å…ƒæ•¸æ“š
    created_by VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- é©—è­‰ä¿¡æ¯
    last_verified_at TIMESTAMP,
    verification_status VARCHAR(20),  -- verified, unverified, broken
    
    CONSTRAINT valid_relation_type CHECK (relation_type IN (
        'direct', 'derived', 'aggregated', 'joined', 'filtered', 'transformed'
    )),
    CONSTRAINT no_self_reference CHECK (source_entity_id != target_entity_id)
);

CREATE INDEX idx_lineage_relations_source ON lineage_relations(source_entity_id);
CREATE INDEX idx_lineage_relations_target ON lineage_relations(target_entity_id);
CREATE INDEX idx_lineage_relations_type ON lineage_relations(relation_type);

COMMENT ON TABLE lineage_relations IS 'æ•¸æ“šè¡€ç·£é—œä¿‚è¡¨';
```

### 4.3 è½‰æ›é‚è¼¯ (Transformations)

```sql
CREATE TABLE transformations (
    id BIGSERIAL PRIMARY KEY,
    transformation_id UUID UNIQUE DEFAULT gen_random_uuid(),
    
    -- è½‰æ›ä¿¡æ¯
    name VARCHAR(200) NOT NULL,
    description TEXT,
    transformation_type VARCHAR(50) NOT NULL,  -- etl, cleaning, aggregation, join
    
    -- è½‰æ›é‚è¼¯
    logic_type VARCHAR(20) NOT NULL,  -- sql, python, spark, custom
    logic_code TEXT,
    logic_file_path VARCHAR(500),
    
    -- è¼¸å…¥è¼¸å‡º
    input_entities UUID[],
    output_entities UUID[],
    
    -- åŸ·è¡Œä¿¡æ¯
    execution_engine VARCHAR(50),  -- spark, pandas, dask, flink
    execution_config JSONB,
    
    -- èª¿åº¦ä¿¡æ¯
    schedule_type VARCHAR(20),  -- manual, cron, event_driven
    schedule_expression VARCHAR(100),
    
    -- å…ƒæ•¸æ“š
    owner VARCHAR(100),
    tags TEXT[],
    version VARCHAR(20) DEFAULT '1.0',
    
    -- ç‹€æ…‹
    enabled BOOLEAN DEFAULT TRUE,
    
    -- æ™‚é–“æˆ³
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- çµ±è¨ˆä¿¡æ¯
    execution_count BIGINT DEFAULT 0,
    success_count BIGINT DEFAULT 0,
    failure_count BIGINT DEFAULT 0,
    avg_duration_seconds NUMERIC(10, 2),
    
    CONSTRAINT valid_transformation_type CHECK (transformation_type IN (
        'etl', 'cleaning', 'aggregation', 'join', 'filter', 'enrichment'
    ))
);

CREATE INDEX idx_transformations_type ON transformations(transformation_type);
CREATE INDEX idx_transformations_owner ON transformations(owner);
CREATE INDEX idx_transformations_enabled ON transformations(enabled);

COMMENT ON TABLE transformations IS 'æ•¸æ“šè½‰æ›é‚è¼¯è¡¨';
```

---

## 5ï¸âƒ£ è³ªé‡å ±å‘Šæ¨¡å‹

### 5.1 è³ªé‡å ±å‘Š (Quality Reports)

```sql
CREATE TABLE quality_reports (
    id BIGSERIAL PRIMARY KEY,
    report_id UUID UNIQUE DEFAULT gen_random_uuid(),
    
    -- å ±å‘Šä¿¡æ¯
    title VARCHAR(500) NOT NULL,
    report_type VARCHAR(50) NOT NULL,  -- daily, weekly, monthly, ad_hoc
    period_start DATE NOT NULL,
    period_end DATE NOT NULL,
    
    -- å ±å‘Šå…§å®¹
    summary JSONB,
    /*
    {
        "overall_score": 95.2,
        "total_records": 10000000,
        "quality_improvement": 2.3,
        "top_issues": [...]
    }
    */
    
    detailed_metrics JSONB,
    dimension_analysis JSONB,
    trend_analysis JSONB,
    recommendations TEXT[],
    
    -- éæ¿¾æ¢ä»¶
    filters JSONB,
    /*
    {
        "data_sources": ["ch8_crawler", "ch1_api"],
        "quality_threshold": 80
    }
    */
    
    -- å ±å‘Šæ–‡ä»¶
    report_format VARCHAR(20),  -- html, pdf, excel, json
    report_path VARCHAR(500),
    file_size_mb NUMERIC(10, 2),
    
    -- ç”Ÿæˆä¿¡æ¯
    generated_by VARCHAR(100),
    generated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    generation_duration_seconds NUMERIC(10, 2),
    
    -- åˆ†ç™¼ä¿¡æ¯
    recipients TEXT[],
    delivery_channels TEXT[],
    delivered_at TIMESTAMP,
    
    -- ç‹€æ…‹
    status VARCHAR(20) DEFAULT 'draft',  -- draft, generating, completed, failed
    
    CONSTRAINT valid_report_type CHECK (report_type IN (
        'daily', 'weekly', 'monthly', 'quarterly', 'ad_hoc'
    ))
);

CREATE INDEX idx_quality_reports_type ON quality_reports(report_type);
CREATE INDEX idx_quality_reports_period ON quality_reports(period_start, period_end);
CREATE INDEX idx_quality_reports_generated_at ON quality_reports(generated_at DESC);

COMMENT ON TABLE quality_reports IS 'è³ªé‡å ±å‘Šè¡¨';
```

### 5.2 å•é¡Œè¿½è¹¤ (Issue Tracking)

```sql
CREATE TABLE quality_issues (
    id BIGSERIAL PRIMARY KEY,
    issue_id UUID UNIQUE DEFAULT gen_random_uuid(),
    
    -- å•é¡Œä¿¡æ¯
    title VARCHAR(500) NOT NULL,
    description TEXT,
    issue_type VARCHAR(50) NOT NULL,  -- data_quality, system, process
    category VARCHAR(100),  -- format_error, missing_value, duplicate, etc.
    
    -- åš´é‡ç¨‹åº¦
    severity VARCHAR(20) NOT NULL,  -- low, medium, high, critical
    priority VARCHAR(20) DEFAULT 'medium',
    
    -- æ•¸æ“šæºä¿¡æ¯
    data_source VARCHAR(200) NOT NULL,
    table_name VARCHAR(200),
    affected_records BIGINT,
    affected_fields TEXT[],
    
    -- æª¢æ¸¬ä¿¡æ¯
    detected_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    detection_method VARCHAR(100),
    detection_rule_id BIGINT,
    
    -- æ ¹å› åˆ†æ
    root_cause_category VARCHAR(100),
    root_cause_description TEXT,
    evidence JSONB,
    
    -- å½±éŸ¿åˆ†æ
    business_impact TEXT,
    downstream_systems TEXT[],
    estimated_loss NUMERIC(15, 2),
    
    -- ç‹€æ…‹
    status VARCHAR(20) DEFAULT 'open',  -- open, assigned, in_progress, resolved, closed
    
    -- è™•ç†ä¿¡æ¯
    assigned_to VARCHAR(100),
    assigned_at TIMESTAMP,
    started_work_at TIMESTAMP,
    resolved_at TIMESTAMP,
    resolution_time_hours NUMERIC(10, 2),
    
    -- è§£æ±ºæ–¹æ¡ˆ
    resolution_type VARCHAR(50),  -- fixed, workaround, accepted, duplicate
    resolution_description TEXT,
    fix_record_id BIGINT,
    
    -- é é˜²æªæ–½
    prevention_actions TEXT[],
    prevention_implemented BOOLEAN DEFAULT FALSE,
    
    -- æ™‚é–“æˆ³
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    closed_at TIMESTAMP,
    
    CONSTRAINT valid_severity CHECK (severity IN ('low', 'medium', 'high', 'critical')),
    CONSTRAINT valid_status CHECK (status IN (
        'open', 'assigned', 'in_progress', 'resolved', 'closed', 'cancelled'
    ))
);

CREATE INDEX idx_quality_issues_source ON quality_issues(data_source);
CREATE INDEX idx_quality_issues_status ON quality_issues(status);
CREATE INDEX idx_quality_issues_severity ON quality_issues(severity);
CREATE INDEX idx_quality_issues_detected_at ON quality_issues(detected_at DESC);
CREATE INDEX idx_quality_issues_assigned_to ON quality_issues(assigned_to);

COMMENT ON TABLE quality_issues IS 'è³ªé‡å•é¡Œè¿½è¹¤è¡¨';
```

### 5.3 ä¿®å¾©è¨˜éŒ„ (Fix Records)

```sql
CREATE TABLE fix_records (
    id BIGSERIAL PRIMARY KEY,
    fix_id UUID UNIQUE DEFAULT gen_random_uuid(),
    
    -- é—œè¯å•é¡Œ
    issue_id UUID REFERENCES quality_issues(issue_id),
    
    -- ä¿®å¾©ä¿¡æ¯
    fix_type VARCHAR(20) NOT NULL,  -- auto, manual, hybrid
    fix_method VARCHAR(100) NOT NULL,
    
    -- ä¿®å¾©é…ç½®
    fix_config JSONB,
    /*
    {
        "method": "regex_replace",
        "params": {
            "pattern": "...",
            "replacement": "..."
        }
    }
    */
    
    -- åŸ·è¡Œä¿¡æ¯
    started_at TIMESTAMP NOT NULL,
    completed_at TIMESTAMP,
    duration_seconds NUMERIC(10, 2),
    
    -- ç‹€æ…‹
    status VARCHAR(20) DEFAULT 'pending',  -- pending, running, completed, failed, rolled_back
    
    -- è™•ç†çµæœ
    affected_records BIGINT,
    success_records BIGINT,
    failed_records BIGINT,
    
    -- é©—è­‰çµæœ
    validation_passed BOOLEAN,
    quality_score_before NUMERIC(5, 2),
    quality_score_after NUMERIC(5, 2),
    improvement NUMERIC(5, 2),
    
    -- å›æ»¾ä¿¡æ¯
    backup_location VARCHAR(500),
    rollbackable BOOLEAN DEFAULT TRUE,
    rolled_back_at TIMESTAMP,
    rollback_reason TEXT,
    
    -- åŸ·è¡Œè€…
    executed_by VARCHAR(100),
    
    -- è©³ç´°æ—¥èªŒ
    execution_log TEXT,
    errors JSONB,
    
    CONSTRAINT valid_fix_type CHECK (fix_type IN ('auto', 'manual', 'hybrid')),
    CONSTRAINT valid_status CHECK (status IN (
        'pending', 'running', 'completed', 'failed', 'rolled_back'
    ))
);

CREATE INDEX idx_fix_records_issue_id ON fix_records(issue_id);
CREATE INDEX idx_fix_records_type ON fix_records(fix_type);
CREATE INDEX idx_fix_records_status ON fix_records(status);
CREATE INDEX idx_fix_records_started_at ON fix_records(started_at DESC);

COMMENT ON TABLE fix_records IS 'å•é¡Œä¿®å¾©è¨˜éŒ„è¡¨';
```

---

## ğŸ“‘ ç›¸é—œç« ç¯€

| å‰åº | ç•¶å‰ | å¾ŒçºŒ |
|-----|------|------|
| [12.4 æ ¸å¿ƒçµ„ä»¶è©³ç´°å¯¦ç¾](ch12-4-æ ¸å¿ƒçµ„ä»¶è©³ç´°å¯¦ç¾.md) | **12.5 è³‡æ–™æ¨¡å‹è©³ç´°å®šç¾©** | [12.6 APIè©³ç´°è¦ç¯„](ch12-6-APIè©³ç´°è¦ç¯„.md) |

**å¿«é€Ÿéˆæ¥ï¼š**
- [12.1 æ¨¡çµ„æ¦‚è¿°](ch12-1-æ¨¡çµ„æ¦‚è¿°.md)
- [12.4 æ ¸å¿ƒçµ„ä»¶è©³ç´°å¯¦ç¾](ch12-4-æ ¸å¿ƒçµ„ä»¶è©³ç´°å¯¦ç¾.md)
- [12.6 APIè©³ç´°è¦ç¯„](ch12-6-APIè©³ç´°è¦ç¯„.md)
- [â† è¿”å›ç¬¬12ç« é¦–é ](ch12-index.md)

---

**æœ€å¾Œæ›´æ–°**: 2025-10-31  
**ç‰ˆæœ¬**: 1.0

