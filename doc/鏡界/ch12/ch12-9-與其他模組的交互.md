# ç¬¬12ç« ï¼šæ•¸æ“šè³ªé‡ç®¡ç†ä¸­å¿ƒ

## 12.9 èˆ‡å…¶ä»–æ¨¡çµ„çš„äº¤äº’

**[â† è¿”å›ç¬¬12ç« é¦–é ](ch12-index.md)**

---

## ğŸ”— æ¨¡çµ„äº¤äº’æ¦‚è¦½

æ•¸æ“šè³ªé‡ç®¡ç†ä¸­å¿ƒæ˜¯æ•´å€‹é¡ç•Œå¹³å°çš„è³ªé‡ä¿éšœä¸­æ¨ï¼Œèˆ‡å¤šå€‹æ¨¡çµ„æœ‰ç·Šå¯†çš„äº¤äº’é—œä¿‚ã€‚

```
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   Ch12 æ•¸æ“š    â”‚
                     â”‚   è³ªé‡ç®¡ç†ä¸­å¿ƒ â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                 â”‚                 â”‚
    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚ Ch1 æ•¸æ“šæºâ”‚     â”‚Ch8 çˆ¬èŸ² â”‚      â”‚Ch10 å¯¦æ™‚â”‚
    â”‚   è¨»å†Š    â”‚     â”‚æ•¸æ“šæ¡é›† â”‚      â”‚  è™•ç†   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                 â”‚                 â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                     â”‚ Ch4 æ•¸æ“š   â”‚
                     â”‚  è™•ç†å¼•æ“  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                     â”‚ Ch11 èª¿åº¦  â”‚
                     â”‚   ä¸­å¿ƒ     â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1ï¸âƒ£ èˆ‡ Ch1 æ•¸æ“šæºè¨»å†Šä¸­å¿ƒçš„äº¤äº’

### 1.1 è³ªé‡å…ƒæ•¸æ“šåŒæ­¥

```python
# app/quality_center/integration/datasource_integration.py

from typing import Dict, List
import requests

class DataSourceIntegration:
    """èˆ‡æ•¸æ“šæºè¨»å†Šä¸­å¿ƒçš„é›†æˆ"""
    
    def __init__(self, datasource_api_url: str):
        self.api_url = datasource_api_url
    
    def register_quality_metrics(self, data_source_id: str, metrics: Dict):
        """
        å‘æ•¸æ“šæºè¨»å†Šä¸­å¿ƒä¸Šå ±è³ªé‡æŒ‡æ¨™
        
        Args:
            data_source_id: æ•¸æ“šæºID
            metrics: è³ªé‡æŒ‡æ¨™
        """
        endpoint = f"{self.api_url}/datasources/{data_source_id}/quality-metrics"
        
        payload = {
            'quality_score': metrics['overall_score'],
            'completeness': metrics['completeness'],
            'accuracy': metrics['accuracy'],
            'consistency': metrics['consistency'],
            'timeliness': metrics['timeliness'],
            'uniqueness': metrics['uniqueness'],
            'last_checked_at': metrics['timestamp'],
            'issues_count': metrics.get('issues_count', 0)
        }
        
        response = requests.post(endpoint, json=payload)
        response.raise_for_status()
        
        return response.json()
    
    def get_datasource_schema(self, data_source_id: str) -> Dict:
        """
        å¾æ•¸æ“šæºè¨»å†Šä¸­å¿ƒç²å– Schema
        
        Returns:
            æ•¸æ“šæºçš„ Schema ä¿¡æ¯
        """
        endpoint = f"{self.api_url}/datasources/{data_source_id}/schema"
        response = requests.get(endpoint)
        response.raise_for_status()
        
        return response.json()
    
    def sync_quality_rules(self, data_source_id: str):
        """
        åŒæ­¥æ•¸æ“šæºçš„è³ªé‡è¦å‰‡
        
        å¾æ•¸æ“šæºè¨»å†Šä¸­å¿ƒç²å–æ•¸æ“šæºçš„è³ªé‡è¦æ±‚ï¼Œ
        è‡ªå‹•å‰µå»ºæˆ–æ›´æ–°è³ªé‡è¦å‰‡
        """
        # ç²å–æ•¸æ“šæºé…ç½®
        datasource = self.get_datasource_config(data_source_id)
        
        # æå–è³ªé‡è¦æ±‚
        quality_requirements = datasource.get('quality_requirements', {})
        
        # å‰µå»ºè³ªé‡è¦å‰‡
        for field, requirements in quality_requirements.items():
            if requirements.get('required'):
                # å‰µå»ºéç©ºè¦å‰‡
                self._create_rule({
                    'name': f"{data_source_id}_{field}_not_null",
                    'rule_type': 'field',
                    'field': field,
                    'check_type': 'not_null',
                    'data_sources': [data_source_id]
                })
            
            if 'format' in requirements:
                # å‰µå»ºæ ¼å¼è¦å‰‡
                self._create_rule({
                    'name': f"{data_source_id}_{field}_format",
                    'rule_type': 'field',
                    'field': field,
                    'check_type': 'regex',
                    'params': {'pattern': requirements['format']},
                    'data_sources': [data_source_id]
                })
    
    def update_datasource_status(self, data_source_id: str, 
                                 quality_status: str):
        """
        æ›´æ–°æ•¸æ“šæºçš„è³ªé‡ç‹€æ…‹
        
        Args:
            data_source_id: æ•¸æ“šæºID
            quality_status: good, warning, critical
        """
        endpoint = f"{self.api_url}/datasources/{data_source_id}/status"
        
        payload = {
            'quality_status': quality_status,
            'updated_at': datetime.utcnow().isoformat()
        }
        
        response = requests.patch(endpoint, json=payload)
        response.raise_for_status()


# ä½¿ç”¨ç¤ºä¾‹
integration = DataSourceIntegration('http://datasource-api:8000/api/v1')

# ä¸Šå ±è³ªé‡æŒ‡æ¨™
integration.register_quality_metrics(
    data_source_id='ch8_crawler_001',
    metrics={
        'overall_score': 95.2,
        'completeness': 98.0,
        'accuracy': 96.0,
        'consistency': 94.0,
        'timeliness': 92.0,
        'uniqueness': 97.0,
        'timestamp': '2025-10-31T10:00:00Z',
        'issues_count': 5
    }
)
```

### 1.2 äº‹ä»¶é©…å‹•é›†æˆ

```python
# app/quality_center/integration/event_handlers.py

from kafka import KafkaConsumer, KafkaProducer
import json

class QualityEventHandler:
    """è³ªé‡äº‹ä»¶è™•ç†å™¨"""
    
    def __init__(self):
        self.consumer = KafkaConsumer(
            'datasource.events',
            bootstrap_servers=['localhost:9092'],
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
        
        self.producer = KafkaProducer(
            bootstrap_servers=['localhost:9092'],
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
    
    def handle_datasource_registered(self, event: Dict):
        """
        è™•ç†æ•¸æ“šæºè¨»å†Šäº‹ä»¶
        
        ç•¶æ–°æ•¸æ“šæºè¨»å†Šæ™‚ï¼Œè‡ªå‹•å‰µå»ºé»˜èªè³ªé‡è¦å‰‡
        """
        data_source_id = event['data_source_id']
        data_source_type = event['data_source_type']
        
        # æ ¹æ“šæ•¸æ“šæºé¡å‹å‰µå»ºé»˜èªè¦å‰‡
        if data_source_type == 'crawler':
            self._create_crawler_default_rules(data_source_id)
        elif data_source_type == 'api':
            self._create_api_default_rules(data_source_id)
        
        logger.info(f"Created default quality rules for {data_source_id}")
    
    def handle_datasource_schema_updated(self, event: Dict):
        """
        è™•ç†æ•¸æ“šæº Schema æ›´æ–°äº‹ä»¶
        
        ç•¶ Schema è®Šæ›´æ™‚ï¼Œé©—è­‰ç¾æœ‰è³ªé‡è¦å‰‡æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
        """
        data_source_id = event['data_source_id']
        new_schema = event['new_schema']
        
        # æª¢æŸ¥ç¾æœ‰è¦å‰‡
        rules = QualityRule.query.filter(
            QualityRule.data_sources.contains([data_source_id])
        ).all()
        
        for rule in rules:
            field = rule.rule_definition.get('field')
            
            # æª¢æŸ¥å­—æ®µæ˜¯å¦é‚„å­˜åœ¨
            if field and field not in new_schema['fields']:
                # ç¦ç”¨è¦å‰‡ä¸¦ç™¼é€å‘Šè­¦
                rule.enabled = False
                db.session.commit()
                
                self._send_alert({
                    'type': 'rule_invalidated',
                    'rule_id': rule.id,
                    'reason': f"Field '{field}' no longer exists in schema"
                })
    
    def emit_quality_issue(self, issue: Dict):
        """
        ç™¼é€è³ªé‡å•é¡Œäº‹ä»¶
        
        é€šçŸ¥å…¶ä»–æ¨¡çµ„æœ‰è³ªé‡å•é¡Œ
        """
        event = {
            'event_type': 'quality_issue_detected',
            'timestamp': datetime.utcnow().isoformat(),
            'data_source_id': issue['data_source'],
            'severity': issue['severity'],
            'issue_type': issue['issue_type'],
            'affected_records': issue['affected_records'],
            'details': issue
        }
        
        self.producer.send('quality.issues', value=event)
    
    def listen(self):
        """ç›£è½äº‹ä»¶"""
        for message in self.consumer:
            event = message.value
            event_type = event.get('event_type')
            
            if event_type == 'datasource.registered':
                self.handle_datasource_registered(event)
            elif event_type == 'datasource.schema_updated':
                self.handle_datasource_schema_updated(event)
```

---

## 2ï¸âƒ£ èˆ‡ Ch8 çˆ¬èŸ²æ•¸æ“šæ¡é›†ä¸­å¿ƒçš„äº¤äº’

### 2.1 å¯¦æ™‚è³ªé‡æª¢æŸ¥

```python
# app/quality_center/integration/crawler_integration.py

class CrawlerQualityIntegration:
    """èˆ‡çˆ¬èŸ²æ¨¡çµ„çš„é›†æˆ"""
    
    def __init__(self):
        self.kafka_consumer = KafkaConsumer(
            'crawler.raw_data',
            bootstrap_servers=['localhost:9092'],
            group_id='quality_checker_group',
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
        
        self.kafka_producer = KafkaProducer(
            bootstrap_servers=['localhost:9092'],
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
    
    def real_time_quality_check(self):
        """
        å¯¦æ™‚è³ªé‡æª¢æŸ¥çˆ¬èŸ²æ•¸æ“š
        
        å¾ Kafka æ¶ˆè²»çˆ¬èŸ²æ¡é›†çš„åŸå§‹æ•¸æ“šï¼Œ
        é€²è¡Œå¯¦æ™‚è³ªé‡æª¢æŸ¥å’Œæ¸…æ´—
        """
        for message in self.kafka_consumer:
            data = message.value
            
            # åŸ·è¡Œè³ªé‡æª¢æŸ¥
            check_result = self.check_crawler_data(data)
            
            if check_result['is_valid']:
                # æ•¸æ“šæœ‰æ•ˆï¼Œç™¼é€åˆ°æ¸…æ´—æµç¨‹
                self.kafka_producer.send(
                    'crawler.validated_data',
                    value=data
                )
            else:
                # æ•¸æ“šç„¡æ•ˆï¼Œç™¼é€åˆ°å•é¡ŒéšŠåˆ—
                self.kafka_producer.send(
                    'crawler.invalid_data',
                    value={
                        'data': data,
                        'issues': check_result['issues'],
                        'timestamp': datetime.utcnow().isoformat()
                    }
                )
                
                # é€šçŸ¥çˆ¬èŸ²æ¨¡çµ„èª¿æ•´ç­–ç•¥
                self.notify_crawler_adjustment(
                    crawler_id=data['crawler_id'],
                    issues=check_result['issues']
                )
    
    def check_crawler_data(self, data: Dict) -> Dict:
        """
        æª¢æŸ¥çˆ¬èŸ²æ•¸æ“šè³ªé‡
        
        Returns:
            {
                'is_valid': bool,
                'issues': List[str],
                'quality_score': float
            }
        """
        issues = []
        
        # 1. å¿…å¡«å­—æ®µæª¢æŸ¥
        required_fields = ['url', 'title', 'content', 'crawled_at']
        for field in required_fields:
            if not data.get(field):
                issues.append(f"Missing required field: {field}")
        
        # 2. URL æ ¼å¼æª¢æŸ¥
        if data.get('url'):
            if not self._is_valid_url(data['url']):
                issues.append("Invalid URL format")
        
        # 3. å…§å®¹é•·åº¦æª¢æŸ¥
        if data.get('content'):
            content_length = len(data['content'])
            if content_length < 100:
                issues.append(f"Content too short: {content_length} chars")
        
        # 4. æ™‚æ•ˆæ€§æª¢æŸ¥
        if data.get('crawled_at'):
            crawled_time = datetime.fromisoformat(data['crawled_at'])
            age = datetime.utcnow() - crawled_time
            if age.total_seconds() > 3600:  # 1å°æ™‚
                issues.append("Data too old")
        
        # è¨ˆç®—è³ªé‡åˆ†æ•¸
        quality_score = 100 - (len(issues) * 15)
        
        return {
            'is_valid': len(issues) == 0,
            'issues': issues,
            'quality_score': max(0, quality_score)
        }
    
    def notify_crawler_adjustment(self, crawler_id: str, issues: List[str]):
        """
        é€šçŸ¥çˆ¬èŸ²èª¿æ•´ç­–ç•¥
        
        æ ¹æ“šè³ªé‡å•é¡Œï¼Œå»ºè­°çˆ¬èŸ²èª¿æ•´æ¡é›†ç­–ç•¥
        """
        # åˆ†æå•é¡Œé¡å‹
        adjustments = []
        
        if any('Missing required field' in issue for issue in issues):
            adjustments.append({
                'type': 'selector_adjustment',
                'message': 'CSS/XPath é¸æ“‡å™¨å¯èƒ½å¤±æ•ˆï¼Œè«‹æª¢æŸ¥'
            })
        
        if any('Content too short' in issue for issue in issues):
            adjustments.append({
                'type': 'content_extraction',
                'message': 'å…§å®¹æå–ä¸å®Œæ•´ï¼Œå»ºè­°èª¿æ•´æå–è¦å‰‡'
            })
        
        # ç™¼é€èª¿æ•´å»ºè­°
        self.kafka_producer.send(
            'crawler.adjustment_suggestions',
            value={
                'crawler_id': crawler_id,
                'timestamp': datetime.utcnow().isoformat(),
                'adjustments': adjustments
            }
        )
```

### 2.2 çˆ¬èŸ²è³ªé‡è©•åˆ†

```python
def evaluate_crawler_performance(crawler_id: str, period_days: int = 7) -> Dict:
    """
    è©•ä¼°çˆ¬èŸ²æ€§èƒ½
    
    Args:
        crawler_id: çˆ¬èŸ²ID
        period_days: è©•ä¼°é€±æœŸï¼ˆå¤©ï¼‰
    
    Returns:
        çˆ¬èŸ²è³ªé‡è©•åˆ†å ±å‘Š
    """
    from_date = datetime.utcnow() - timedelta(days=period_days)
    
    # æŸ¥è©¢è³ªé‡æª¢æŸ¥è¨˜éŒ„
    checks = QualityCheck.query.filter(
        QualityCheck.data_source == crawler_id,
        QualityCheck.started_at >= from_date
    ).all()
    
    if not checks:
        return {'error': 'No quality checks found'}
    
    # è¨ˆç®—çµ±è¨ˆæŒ‡æ¨™
    total_records = sum(c.total_records for c in checks)
    valid_records = sum(c.valid_records for c in checks)
    avg_quality_score = sum(c.quality_score for c in checks) / len(checks)
    
    # å•é¡Œçµ±è¨ˆ
    common_issues = {}
    for check in checks:
        for issue in check.issues or []:
            issue_type = issue['rule_name']
            common_issues[issue_type] = common_issues.get(issue_type, 0) + 1
    
    # ç”Ÿæˆè©•åˆ†
    evaluation = {
        'crawler_id': crawler_id,
        'period': f"{period_days} days",
        'total_records': total_records,
        'valid_records': valid_records,
        'valid_rate': (valid_records / total_records * 100) if total_records > 0 else 0,
        'avg_quality_score': round(avg_quality_score, 2),
        'common_issues': sorted(
            common_issues.items(),
            key=lambda x: x[1],
            reverse=True
        )[:5],  # Top 5 å•é¡Œ
        'recommendations': []
    }
    
    # ç”Ÿæˆå»ºè­°
    if evaluation['valid_rate'] < 90:
        evaluation['recommendations'].append(
            "æ•¸æ“šæœ‰æ•ˆç‡ä½æ–¼ 90%ï¼Œå»ºè­°å„ªåŒ–çˆ¬èŸ²é‚è¼¯"
        )
    
    if 'content_too_short' in common_issues:
        evaluation['recommendations'].append(
            "å…§å®¹æå–ä¸å®Œæ•´ï¼Œå»ºè­°æª¢æŸ¥å…§å®¹é¸æ“‡å™¨"
        )
    
    return evaluation
```

---

## 3ï¸âƒ£ èˆ‡ Ch10 å¯¦æ™‚è™•ç†å¼•æ“çš„äº¤äº’

### 3.1 æµå¼è³ªé‡æª¢æŸ¥

```python
# app/quality_center/integration/streaming_integration.py

from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.functions import MapFunction, FilterFunction

class FlinkQualityChecker:
    """Flink æµå¼è³ªé‡æª¢æŸ¥"""
    
    def __init__(self):
        self.env = StreamExecutionEnvironment.get_execution_environment()
        self.env.set_parallelism(4)
    
    def create_quality_pipeline(self):
        """å‰µå»ºè³ªé‡æª¢æŸ¥ç®¡é“"""
        
        # 1. å¾ Kafka è®€å–æ•¸æ“šæµ
        data_stream = self.env.add_source(
            FlinkKafkaConsumer(
                topics='raw_data',
                deserialization_schema=SimpleStringSchema(),
                properties={'bootstrap.servers': 'localhost:9092'}
            )
        )
        
        # 2. è§£æ JSON
        parsed_stream = data_stream.map(lambda x: json.loads(x))
        
        # 3. è³ªé‡æª¢æŸ¥
        checked_stream = parsed_stream.map(QualityCheckFunction())
        
        # 4. åˆ†æµï¼šæœ‰æ•ˆæ•¸æ“š vs ç„¡æ•ˆæ•¸æ“š
        valid_stream = checked_stream.filter(lambda x: x['is_valid'])
        invalid_stream = checked_stream.filter(lambda x: not x['is_valid'])
        
        # 5. æœ‰æ•ˆæ•¸æ“šç™¼é€åˆ°ä¸‹æ¸¸
        valid_stream.add_sink(
            FlinkKafkaProducer(
                topic='validated_data',
                serialization_schema=SimpleStringSchema(),
                producer_config={'bootstrap.servers': 'localhost:9092'}
            )
        )
        
        # 6. ç„¡æ•ˆæ•¸æ“šç™¼é€åˆ°å•é¡ŒéšŠåˆ—
        invalid_stream.add_sink(
            FlinkKafkaProducer(
                topic='quality_issues',
                serialization_schema=SimpleStringSchema(),
                producer_config={'bootstrap.servers': 'localhost:9092'}
            )
        )
        
        # 7. è³ªé‡æŒ‡æ¨™èšåˆï¼ˆçª—å£ï¼‰
        quality_metrics = (checked_stream
            .key_by(lambda x: x['data_source'])
            .window(TumblingProcessingTimeWindows.of(Time.minutes(5)))
            .aggregate(QualityMetricsAggregator())
        )
        
        # 8. æŒ‡æ¨™å¯«å…¥ InfluxDB
        quality_metrics.add_sink(InfluxDBSink())
        
        return self.env


class QualityCheckFunction(MapFunction):
    """è³ªé‡æª¢æŸ¥å‡½æ•¸"""
    
    def map(self, value: Dict) -> Dict:
        """åŸ·è¡Œè³ªé‡æª¢æŸ¥"""
        issues = []
        
        # åŸ·è¡Œå„ç¨®æª¢æŸ¥...
        if not value.get('id'):
            issues.append('missing_id')
        
        if not value.get('timestamp'):
            issues.append('missing_timestamp')
        
        # è¨ˆç®—è³ªé‡åˆ†æ•¸
        quality_score = 100 - len(issues) * 10
        
        return {
            **value,
            'is_valid': len(issues) == 0,
            'quality_score': quality_score,
            'issues': issues,
            'checked_at': datetime.utcnow().isoformat()
        }
```

---

## 4ï¸âƒ£ èˆ‡ Ch11 èª¿åº¦ä¸­å¿ƒçš„äº¤äº’

### 4.1 å®šæ™‚è³ªé‡æª¢æŸ¥ä»»å‹™

```python
# app/quality_center/integration/scheduler_integration.py

class ScheduledQualityTasks:
    """èª¿åº¦çš„è³ªé‡ä»»å‹™"""
    
    @staticmethod
    def register_daily_quality_check():
        """è¨»å†Šæ¯æ—¥è³ªé‡æª¢æŸ¥ä»»å‹™"""
        from apscheduler.schedulers.background import BackgroundScheduler
        
        scheduler = BackgroundScheduler()
        
        # æ¯æ—¥å‡Œæ™¨ 2 é»åŸ·è¡Œå…¨é‡è³ªé‡æª¢æŸ¥
        scheduler.add_job(
            func=run_daily_quality_check,
            trigger='cron',
            hour=2,
            minute=0,
            id='daily_quality_check'
        )
        
        # æ¯å°æ™‚åŸ·è¡Œå¢é‡è³ªé‡æª¢æŸ¥
        scheduler.add_job(
            func=run_incremental_quality_check,
            trigger='interval',
            hours=1,
            id='hourly_quality_check'
        )
        
        # æ¯ 5 åˆ†é˜æ›´æ–°è³ªé‡çœ‹æ¿
        scheduler.add_job(
            func=update_quality_dashboard,
            trigger='interval',
            minutes=5,
            id='dashboard_update'
        )
        
        scheduler.start()


def run_daily_quality_check():
    """åŸ·è¡Œæ¯æ—¥è³ªé‡æª¢æŸ¥"""
    logger.info("Starting daily quality check...")
    
    # ç²å–æ‰€æœ‰æ¿€æ´»çš„æ•¸æ“šæº
    data_sources = get_active_datasources()
    
    for source in data_sources:
        try:
            # åŸ·è¡Œè³ªé‡æª¢æŸ¥
            result = perform_quality_check(source['id'], check_type='full')
            
            # ç”Ÿæˆè³ªé‡å ±å‘Š
            generate_quality_report(source['id'], result)
            
            # å¦‚æœè³ªé‡ä¸‹é™ï¼Œç™¼é€å‘Šè­¦
            if result['quality_score'] < source['min_quality_threshold']:
                send_quality_alert(source['id'], result)
        
        except Exception as e:
            logger.error(f"Quality check failed for {source['id']}: {e}")
```

### 4.2 å‹•æ…‹èª¿åº¦ç­–ç•¥

```python
class AdaptiveQualityScheduler:
    """è‡ªé©æ‡‰è³ªé‡èª¿åº¦å™¨"""
    
    def adjust_check_frequency(self, data_source_id: str):
        """
        æ ¹æ“šæ•¸æ“šæºè³ªé‡å‹•æ…‹èª¿æ•´æª¢æŸ¥é »ç‡
        
        è³ªé‡ç©©å®š â†’ é™ä½æª¢æŸ¥é »ç‡
        è³ªé‡æ³¢å‹• â†’ å¢åŠ æª¢æŸ¥é »ç‡
        """
        # ç²å–æœ€è¿‘çš„è³ªé‡åˆ†æ•¸
        recent_scores = self.get_recent_quality_scores(
            data_source_id,
            days=7
        )
        
        if not recent_scores:
            return
        
        # è¨ˆç®—ç©©å®šæ€§ï¼ˆæ¨™æº–å·®ï¼‰
        std_dev = np.std(recent_scores)
        avg_score = np.mean(recent_scores)
        
        # æ±ºå®šæª¢æŸ¥é »ç‡
        if std_dev < 2 and avg_score > 95:
            # è³ªé‡ç©©å®šä¸”é«˜ï¼Œé™ä½æª¢æŸ¥é »ç‡
            frequency = 'daily'
        elif std_dev < 5 and avg_score > 90:
            # è³ªé‡è¼ƒç©©å®šï¼Œä¿æŒæ­£å¸¸é »ç‡
            frequency = 'hourly'
        else:
            # è³ªé‡ä¸ç©©å®šï¼Œå¢åŠ æª¢æŸ¥é »ç‡
            frequency = 'every_30_minutes'
        
        # æ›´æ–°èª¿åº¦é…ç½®
        self.update_schedule(data_source_id, frequency)
        
        logger.info(
            f"Adjusted quality check frequency for {data_source_id} "
            f"to {frequency} (std: {std_dev:.2f}, avg: {avg_score:.2f})"
        )
```

---

## 5ï¸âƒ£ èˆ‡ Ch4 æ•¸æ“šè™•ç†å¼•æ“çš„äº¤äº’

### 5.1 æ¸…æ´—ç®¡é“é›†æˆ

```python
# app/quality_center/integration/processing_integration.py

class ProcessingPipelineIntegration:
    """èˆ‡æ•¸æ“šè™•ç†å¼•æ“çš„é›†æˆ"""
    
    def inject_quality_checks(self, pipeline_id: str):
        """
        åœ¨æ•¸æ“šè™•ç†ç®¡é“ä¸­æ³¨å…¥è³ªé‡æª¢æŸ¥é»
        
        Args:
            pipeline_id: è™•ç†ç®¡é“ID
        """
        # ç²å–ç®¡é“é…ç½®
        pipeline = self.get_pipeline_config(pipeline_id)
        
        # åœ¨é—œéµç¯€é»æ³¨å…¥è³ªé‡æª¢æŸ¥
        enhanced_pipeline = {
            **pipeline,
            'stages': []
        }
        
        for stage in pipeline['stages']:
            # åŸå§‹è™•ç†éšæ®µ
            enhanced_pipeline['stages'].append(stage)
            
            # æ³¨å…¥è³ªé‡æª¢æŸ¥éšæ®µ
            if stage['type'] in ['transform', 'aggregate']:
                enhanced_pipeline['stages'].append({
                    'type': 'quality_check',
                    'config': {
                        'rules': self.get_applicable_rules(
                            data_source=pipeline['data_source'],
                            stage=stage['type']
                        ),
                        'on_failure': 'log_and_continue'
                    }
                })
        
        return enhanced_pipeline
    
    def quality_gate(self, data: pd.DataFrame, rules: List[Dict]) -> bool:
        """
        è³ªé‡é–€æª»æª¢æŸ¥
        
        åœ¨æ•¸æ“šè™•ç†æµç¨‹ä¸­è¨­ç½®è³ªé‡é–€æª»ï¼Œ
        åªæœ‰é€šéè³ªé‡æª¢æŸ¥çš„æ•¸æ“šæ‰èƒ½ç¹¼çºŒè™•ç†
        
        Returns:
            True if data passes quality gate, False otherwise
        """
        for rule in rules:
            if not self._check_rule(data, rule):
                logger.warning(
                    f"Data failed quality gate: {rule['name']}"
                )
                return False
        
        return True
```

---

## ğŸ“Š å®Œæ•´é›†æˆæµç¨‹ç¤ºä¾‹

```python
# app/quality_center/integration/complete_flow.py

class CompleteQualityFlow:
    """å®Œæ•´çš„è³ªé‡ç®¡ç†æµç¨‹"""
    
    def __init__(self):
        self.datasource_integration = DataSourceIntegration(...)
        self.crawler_integration = CrawlerQualityIntegration()
        self.processing_integration = ProcessingPipelineIntegration()
    
    async def process_new_data(self, data_source_id: str):
        """
        è™•ç†æ–°æ•¸æ“šçš„å®Œæ•´æµç¨‹
        
        1. Ch8 çˆ¬èŸ²æ¡é›†æ•¸æ“š
        2. Ch12 å¯¦æ™‚è³ªé‡æª¢æŸ¥
        3. Ch12 æ•¸æ“šæ¸…æ´—
        4. Ch4 æ•¸æ“šè™•ç†
        5. Ch12 æœ€çµ‚è³ªé‡é©—è­‰
        6. Ch1 æ›´æ–°æ•¸æ“šæºè³ªé‡ç‹€æ…‹
        """
        
        # Step 1: å¾ Kafka æ¶ˆè²»çˆ¬èŸ²æ•¸æ“š
        data = await self.consume_crawler_data(data_source_id)
        
        # Step 2: å¯¦æ™‚è³ªé‡æª¢æŸ¥
        check_result = self.check_quality(data)
        
        if not check_result['is_valid']:
            # æ•¸æ“šä¸åˆæ ¼ï¼Œç™¼é€åˆ°å•é¡ŒéšŠåˆ—
            await self.send_to_issue_queue(data, check_result)
            
            # é€šçŸ¥çˆ¬èŸ²èª¿æ•´
            await self.notify_crawler(data_source_id, check_result)
            
            return
        
        # Step 3: æ•¸æ“šæ¸…æ´—
        cleaned_data = await self.clean_data(data)
        
        # Step 4: ç™¼é€åˆ°è™•ç†å¼•æ“
        await self.send_to_processing(cleaned_data)
        
        # Step 5: è™•ç†å®Œæˆå¾Œçš„è³ªé‡é©—è­‰
        # (ç”± Ch4 èª¿ç”¨ Ch12 çš„ API)
        
        # Step 6: æ›´æ–°æ•¸æ“šæºè³ªé‡ç‹€æ…‹
        quality_score = check_result['quality_score']
        quality_status = self._determine_status(quality_score)
        
        await self.datasource_integration.update_datasource_status(
            data_source_id,
            quality_status
        )
        
        # è¨˜éŒ„è³ªé‡æŒ‡æ¨™
        await self.record_quality_metrics(data_source_id, check_result)
    
    def _determine_status(self, score: float) -> str:
        """æ ¹æ“šè³ªé‡åˆ†æ•¸ç¢ºå®šç‹€æ…‹"""
        if score >= 95:
            return 'excellent'
        elif score >= 85:
            return 'good'
        elif score >= 70:
            return 'warning'
        else:
            return 'critical'
```

---

## ğŸ“‘ ç›¸é—œç« ç¯€

| å‰åº | ç•¶å‰ | å¾ŒçºŒ |
|-----|------|------|
| [12.8 å®‰å…¨è€ƒæ…®](ch12-8-å®‰å…¨è€ƒæ…®.md) | **12.9 èˆ‡å…¶ä»–æ¨¡çµ„çš„äº¤äº’** | [12.10 æœ€ä½³å¯¦è¸æŒ‡å—](ch12-10-æœ€ä½³å¯¦è¸æŒ‡å—.md) |

**å¿«é€Ÿéˆæ¥ï¼š**
- [12.1 æ¨¡çµ„æ¦‚è¿°](ch12-1-æ¨¡çµ„æ¦‚è¿°.md)
- [12.8 å®‰å…¨è€ƒæ…®](ch12-8-å®‰å…¨è€ƒæ…®.md)
- [12.10 æœ€ä½³å¯¦è¸æŒ‡å—](ch12-10-æœ€ä½³å¯¦è¸æŒ‡å—.md)
- [â† è¿”å›ç¬¬12ç« é¦–é ](ch12-index.md)

---

**æœ€å¾Œæ›´æ–°**: 2025-10-31  
**ç‰ˆæœ¬**: 1.0

