# ç¬¬10ç« ï¼šå¯¦æ™‚æ•¸æ“šè™•ç†å¼•æ“ (Real-time Data Processing Engine)

## 10.1 æ¨¡çµ„æ¦‚è¿°

**[â† è¿”å›ç¬¬10ç« é¦–é ](ch10-index.md)**

---

## ğŸ¯ æ¨¡çµ„å®šä½

å¯¦æ™‚æ•¸æ“šè™•ç†å¼•æ“æ˜¯é¡ç•Œå¹³å°çš„æ ¸å¿ƒæ“´å±•æ¨¡çµ„ï¼Œå°ˆæ³¨æ–¼è™•ç†æµå¼æ•¸æ“šã€å¯¦æ™‚è¨ˆç®—å’Œäº‹ä»¶é©…å‹•å ´æ™¯ã€‚å®ƒå¡«è£œäº†å¹³å°åœ¨å¯¦æ™‚è™•ç†èƒ½åŠ›ä¸Šçš„ç©ºç™½ï¼Œä½¿å¹³å°èƒ½å¤ è™•ç†å¾ç§’ç´šåˆ°æ¯«ç§’ç´šçš„å¯¦æ™‚æ•¸æ“šæµã€‚

### ç‚ºä»€éº¼éœ€è¦å¯¦æ™‚æ•¸æ“šè™•ç†ï¼Ÿ

**ç•¶å‰ç—›é»**:
- æ‰¹è™•ç†å»¶é²é«˜ï¼ˆåˆ†é˜åˆ°å°æ™‚ç´šåˆ¥ï¼‰
- ç„¡æ³•åŠæ™‚éŸ¿æ‡‰ç•°å¸¸äº‹ä»¶
- å¯¦æ™‚ç›£æ§èƒ½åŠ›ä¸è¶³
- ç¼ºå°‘æµå¼æ•¸æ“šåˆ†æ

**è§£æ±ºæ–¹æ¡ˆ**:
- æµå¼æ•¸æ“šè™•ç†ï¼ˆç§’ç´šå»¶é²ï¼‰
- å¯¦æ™‚äº‹ä»¶è§¸ç™¼å’Œå‘Šè­¦
- é€£çºŒæŸ¥è©¢å’Œå¯¦æ™‚èšåˆ
- è¤‡é›œäº‹ä»¶è™•ç†ï¼ˆCEPï¼‰

---

## ğŸ—ï¸ æ ¸å¿ƒæ¶æ§‹

### æ¶æ§‹å±¤æ¬¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ‡‰ç”¨å±¤                             â”‚
â”‚  å¯¦æ™‚çœ‹æ¿ â”‚ å¯¦æ™‚å‘Šè­¦ â”‚ æµé‡åˆ†æ â”‚ ç•°å¸¸æª¢æ¸¬           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   è™•ç†å±¤                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚        Apache Flink æµè™•ç†å¼•æ“                â”‚   â”‚
â”‚  â”‚  - CEP  - Window  - State  - Watermark       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   å­˜å„²å±¤                              â”‚
â”‚  Kafka â”‚ RocksDB â”‚ ClickHouse â”‚ InfluxDB           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   æ•¸æ“šæº                              â”‚
â”‚  Ch8çˆ¬èŸ² â”‚ Ch3ç›£æ¸¬ â”‚ Ch4å·¥ä½œæµ â”‚ å¤–éƒ¨ç³»çµ±           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ æ ¸å¿ƒåŠŸèƒ½

### 1. æµå¼æ•¸æ“šè™•ç†

#### æ•¸æ“šæºæ¥å…¥
- **Kafka Source**: å¾ Kafka topic è®€å–æ•¸æ“š
- **Socket Source**: TCP/UDP æ•¸æ“šæµ
- **File Source**: ç›£æ§æ–‡ä»¶è®ŠåŒ–
- **Custom Source**: è‡ªå®šç¾©æ•¸æ“šæº

#### æ•¸æ“šè½‰æ›
```python
# Flink æµè™•ç†ç¤ºä¾‹
DataStream<Event> stream = env
    .addSource(new FlinkKafkaConsumer<>(topic, schema, properties))
    .filter(event -> event.isValid())  # éæ¿¾
    .map(event -> transform(event))    # è½‰æ›
    .keyBy(event -> event.getKey())    # åˆ†çµ„
    .window(TumblingEventTimeWindows.of(Time.minutes(1)))  # çª—å£
    .aggregate(new CountAggregator())  # èšåˆ
    .addSink(new FlinkKafkaProducer<>()); # è¼¸å‡º
```

### 2. æ™‚é–“çª—å£è™•ç†

#### çª—å£é¡å‹

**æ»¾å‹•çª—å£ï¼ˆTumbling Windowï¼‰**
```
æ™‚é–“: 00:00  00:05  00:10  00:15  00:20
      [â”€â”€â”€â”€â”€][â”€â”€â”€â”€â”€][â”€â”€â”€â”€â”€][â”€â”€â”€â”€â”€]
       W1     W2     W3     W4
```
- å›ºå®šå¤§å°ï¼Œç„¡é‡ç–Š
- ç”¨æ–¼ï¼šæ¯5åˆ†é˜çµ±è¨ˆ

**æ»‘å‹•çª—å£ï¼ˆSliding Windowï¼‰**
```
æ™‚é–“: 00:00  00:02  00:04  00:06  00:08
      [â”€â”€â”€â”€â”€â”€â”€â”€]
         [â”€â”€â”€â”€â”€â”€â”€â”€]
            [â”€â”€â”€â”€â”€â”€â”€â”€]
               [â”€â”€â”€â”€â”€â”€â”€â”€]
```
- å›ºå®šå¤§å°ï¼Œæœ‰é‡ç–Š
- ç”¨æ–¼ï¼šæœ€è¿‘5åˆ†é˜ç§»å‹•å¹³å‡

**æœƒè©±çª—å£ï¼ˆSession Windowï¼‰**
```
äº‹ä»¶: E1  E2  E3      E4  E5         E6
      [â”€â”€â”€â”€â”€â”€â”€â”€â”€]     [â”€â”€â”€â”€â”€]        [â”€]
       Session1      Session2     Session3
```
- å‹•æ…‹å¤§å°ï¼ŒåŸºæ–¼æ´»å‹•é–“éš”
- ç”¨æ–¼ï¼šç”¨æˆ¶æœƒè©±åˆ†æ

### 3. è¤‡é›œäº‹ä»¶è™•ç†ï¼ˆCEPï¼‰

#### æ¨¡å¼åŒ¹é…

**æª¢æ¸¬ç•°å¸¸ç™»éŒ„æ¨¡å¼**
```java
Pattern<LoginEvent, ?> pattern = Pattern.<LoginEvent>begin("first")
    .where(new SimpleCondition<LoginEvent>() {
        @Override
        public boolean filter(LoginEvent event) {
            return event.isFailedLogin();
        }
    })
    .next("second")
    .where(new SimpleCondition<LoginEvent>() {
        @Override
        public boolean filter(LoginEvent event) {
            return event.isFailedLogin();
        }
    })
    .next("third")
    .where(new SimpleCondition<LoginEvent>() {
        @Override
        public boolean filter(LoginEvent event) {
            return event.isFailedLogin();
        }
    })
    .within(Time.minutes(5));  // 5åˆ†é˜å…§3æ¬¡å¤±æ•—ç™»éŒ„
```

### 4. ç‹€æ…‹ç®¡ç†

#### ç‹€æ…‹é¡å‹

**ValueState** - å–®å€‹å€¼
```java
ValueState<Long> countState = getRuntimeContext()
    .getState(new ValueStateDescriptor<>("count", Long.class));
```

**ListState** - åˆ—è¡¨
```java
ListState<Event> eventsState = getRuntimeContext()
    .getListState(new ListStateDescriptor<>("events", Event.class));
```

**MapState** - éµå€¼å°
```java
MapState<String, Long> mapState = getRuntimeContext()
    .getMapState(new MapStateDescriptor<>("map", String.class, Long.class));
```

---

## ğŸ¯ æ‡‰ç”¨å ´æ™¯

### å ´æ™¯ 1: å¯¦æ™‚ç›£æ§çˆ¬èŸ²ç‹€æ…‹

```python
# å¯¦æ™‚è¨ˆç®—çˆ¬èŸ² QPS
stream = env.add_source(KafkaSource("crawler-metrics"))
    .key_by(lambda x: x['crawler_id'])
    .window(TumblingProcessingTimeWindows.of(Time.seconds(10)))
    .aggregate(QPSAggregator())
    .filter(lambda x: x.qps < 10)  # QPS ç•°å¸¸ä½
    .add_sink(AlertSink())  # ç™¼é€å‘Šè­¦
```

### å ´æ™¯ 2: å¯¦æ™‚æ•¸æ“šè³ªé‡æª¢æ¸¬

```python
# æª¢æ¸¬é‡è¤‡æ•¸æ“š
stream = env.add_source(KafkaSource("raw-data"))
    .key_by(lambda x: x['url'])
    .process(DuplicateDetector())
    .filter(lambda x: x.is_duplicate)
    .add_sink(DuplicateStoreSink())
```

### å ´æ™¯ 3: å¯¦æ™‚æµé‡åˆ†æ

```python
# æ¯åˆ†é˜è¨ªå•é‡ TOP 10 ç¶²ç«™
stream = env.add_source(KafkaSource("access-logs"))
    .map(lambda x: (x['domain'], 1))
    .key_by(lambda x: x[0])
    .window(TumblingEventTimeWindows.of(Time.minutes(1)))
    .sum(1)
    .process(TopNProcessor(n=10))
    .add_sink(VisualizationSink())
```

---

## ğŸ“Š æ€§èƒ½æŒ‡æ¨™

### è™•ç†èƒ½åŠ›

| æŒ‡æ¨™ | ç›®æ¨™å€¼ | èªªæ˜ |
|------|--------|------|
| **ååé‡** | > 100è¬ events/ç§’ | å–®ç¯€é»è™•ç†èƒ½åŠ› |
| **å»¶é²** | < 100msï¼ˆP99ï¼‰ | ç«¯åˆ°ç«¯å»¶é² |
| **ç‹€æ…‹å¤§å°** | æ”¯æŒ TB ç´š | ä½¿ç”¨ RocksDB å­˜å„² |
| **å®¹éŒ¯æ¢å¾©** | < 1åˆ†é˜ | Checkpoint æ¢å¾©æ™‚é–“ |
| **æ°´å¹³æ“´å±•** | ç·šæ€§æ“´å±• | å¢åŠ ç¯€é»æå‡è™•ç†èƒ½åŠ› |

### å¯é æ€§

- **Exactly-Once**: ä¿è­‰ç²¾ç¢ºä¸€æ¬¡èªç¾©
- **Checkpoint**: åˆ†å¸ƒå¼å¿«ç…§æ©Ÿåˆ¶
- **Savepoint**: æ‰‹å‹•ä¿å­˜é»æ”¯æŒ
- **è‡ªå‹•æ¢å¾©**: å¤±æ•—å¾Œè‡ªå‹•é‡å•Ÿ

---

## ğŸ”— èˆ‡å…¶ä»–æ¨¡çµ„çš„é›†æˆ

### æ•¸æ“šæµå‘

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ch8 çˆ¬èŸ²   â”‚ â”€â”€Kafkaâ”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ch3 ç›£æ¸¬   â”‚ â”€â”€â†’ â”‚ Ch10 å¯¦æ™‚â”‚ â”€â”€â†’ â”‚ Ch3 ç›£æ¸¬å‘Šè­¦ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   è™•ç†   â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”œâ”€â”€â†’ ClickHouse (å¯¦æ™‚æŸ¥è©¢)
                         â”œâ”€â”€â†’ InfluxDB (æ™‚åºæ•¸æ“š)
                         â””â”€â”€â†’ Ch12 è³ªé‡ç®¡ç†
```

### æ¥å£è¦ç¯„

**è¼¸å…¥æ¥å£**
- Kafka Topic: `crawler.events`, `monitoring.metrics`
- REST API: `/api/v1/streams/publish`
- WebSocket: å¯¦æ™‚æ•¸æ“šæ¨é€

**è¼¸å‡ºæ¥å£**
- Kafka Topic: `processed.events`, `alerts`
- ClickHouse: å¯¦æ™‚OLAPæŸ¥è©¢
- REST API: `/api/v1/streams/query`

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### åŸºç¤ç¤ºä¾‹

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors import FlinkKafkaConsumer

# 1. å‰µå»ºåŸ·è¡Œç’°å¢ƒ
env = StreamExecutionEnvironment.get_execution_environment()
env.set_parallelism(4)

# 2. é…ç½® Checkpoint
env.enable_checkpointing(60000)  # æ¯åˆ†é˜

# 3. æ·»åŠ æ•¸æ“šæº
kafka_consumer = FlinkKafkaConsumer(
    topics='input-topic',
    deserialization_schema=JsonDeserializationSchema(),
    properties={'bootstrap.servers': 'localhost:9092'}
)
stream = env.add_source(kafka_consumer)

# 4. æ•¸æ“šè™•ç†
result = stream \
    .filter(lambda x: x['status'] == 'success') \
    .key_by(lambda x: x['source']) \
    .window(TumblingProcessingTimeWindows.of(Time.minutes(1))) \
    .aggregate(CountAggregator())

# 5. è¼¸å‡ºçµæœ
result.add_sink(FlinkKafkaProducer(...))

# 6. åŸ·è¡Œ
env.execute("Real-time Processing Job")
```

---

## ğŸ“š ç›¸é—œç« ç¯€

- [10.2 è©³ç´°åŠŸèƒ½æ¸…å–®](ch10-2-è©³ç´°åŠŸèƒ½æ¸…å–®.md)
- [10.3 æŠ€è¡“æ¶æ§‹](ch10-3-æŠ€è¡“æ¶æ§‹.md)
- [10.4 æ ¸å¿ƒçµ„ä»¶è©³ç´°å¯¦ç¾](ch10-4-æ ¸å¿ƒçµ„ä»¶è©³ç´°å¯¦ç¾.md)
- [â† è¿”å›ç¬¬10ç« é¦–é ](ch10-index.md)

---

**æœ€å¾Œæ›´æ–°**: 2025-10-31  
**ç‰ˆæœ¬**: 1.0


