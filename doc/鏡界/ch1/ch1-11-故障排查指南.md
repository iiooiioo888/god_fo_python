# ç¬¬1ç« ï¼šè³‡æ–™æºè¨»å†Šä¸­å¿ƒ (Data Source Registry)

## 1.11 æ•…éšœæ’æŸ¥æŒ‡å—

**[â† è¿”å›ç¬¬1ç« é¦–é ](ch1-index.md)**

---

æœ¬ç« ç¯€æä¾›è³‡æ–™æºè¨»å†Šä¸­å¿ƒå¸¸è¦‹å•é¡Œçš„è¨ºæ–·å’Œè§£æ±ºæ–¹æ¡ˆï¼Œå¹«åŠ©å¿«é€Ÿå®šä½å’Œä¿®å¾©å•é¡Œã€‚

## ğŸ” å•é¡Œè¨ºæ–·æµç¨‹

```mermaid
graph TD
    A[ç™¼ç¾å•é¡Œ] --> B{å•é¡Œé¡å‹?}
    B -->|æ•ˆèƒ½å•é¡Œ| C[æ•ˆèƒ½è¨ºæ–·]
    B -->|åŠŸèƒ½ç•°å¸¸| D[åŠŸèƒ½è¨ºæ–·]
    B -->|è³‡æ–™å•é¡Œ| E[è³‡æ–™è¨ºæ–·]
    C --> F[æª¢æŸ¥æŒ‡æ¨™]
    D --> G[æª¢æŸ¥æ—¥èªŒ]
    E --> H[æª¢æŸ¥è³‡æ–™åº«]
    F --> I[æ‡‰ç”¨è§£æ±ºæ–¹æ¡ˆ]
    G --> I
    H --> I
    I --> J{å•é¡Œè§£æ±º?}
    J -->|æ˜¯| K[è¨˜éŒ„è§£æ±ºæ–¹æ¡ˆ]
    J -->|å¦| L[å‡ç´šè™•ç†]
```

---

## ğŸ› å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ

### å•é¡Œ 1: è³‡æ–™æºæœå°‹é€Ÿåº¦æ…¢

#### ç—‡ç‹€
- æœå°‹è«‹æ±‚éŸ¿æ‡‰æ™‚é–“è¶…é 5 ç§’
- P99 å»¶é²è¶…é 10 ç§’
- Elasticsearch è² è¼‰éé«˜

#### å¯èƒ½åŸå› 
1. Elasticsearch ç´¢å¼•æœªå„ªåŒ–
2. æŸ¥è©¢æ¢ä»¶éæ–¼è¤‡é›œ
3. è³‡æ–™é‡éå¤§å°è‡´å…¨è¡¨æƒæ
4. åˆ†ç‰‡é…ç½®ä¸åˆç†

#### è¨ºæ–·æ­¥é©Ÿ

```bash
# 1. æª¢æŸ¥ Elasticsearch é›†ç¾¤å¥åº·ç‹€æ…‹
curl -X GET "localhost:9200/_cluster/health?pretty"

# 2. æŸ¥çœ‹ç´¢å¼•çµ±è¨ˆè³‡è¨Š
curl -X GET "localhost:9200/data_sources/_stats?pretty"

# 3. æª¢æŸ¥æ…¢æŸ¥è©¢æ—¥èªŒ
curl -X GET "localhost:9200/_cat/indices/data_sources?v"

# 4. åˆ†ææŸ¥è©¢æ•ˆèƒ½
curl -X GET "localhost:9200/data_sources/_search?explain=true" \
  -H 'Content-Type: application/json' \
  -d '{"query": {"match": {"name": "test"}}}'
```

#### è§£æ±ºæ–¹æ¡ˆ

**æ–¹æ¡ˆ 1: å„ªåŒ–ç´¢å¼•è¨­ç½®**
```python
# æ›´æ–°ç´¢å¼•è¨­ç½®
from elasticsearch import Elasticsearch

es = Elasticsearch(['localhost:9200'])

# å¢åŠ å‰¯æœ¬æ•¸ä»¥æå‡è®€å–æ•ˆèƒ½
es.indices.put_settings(
    index='data_sources',
    body={
        'number_of_replicas': 2,
        'refresh_interval': '30s'
    }
)

# å„ªåŒ–åˆ†ç‰‡ç­–ç•¥
es.indices.put_settings(
    index='data_sources',
    body={
        'routing.allocation.total_shards_per_node': 3
    }
)
```

**æ–¹æ¡ˆ 2: æ·»åŠ ç¼ºå¤±çš„ç´¢å¼•**
```sql
-- æª¢æŸ¥ç¼ºå¤±çš„ç´¢å¼•
SELECT 
    schemaname,
    tablename,
    indexname,
    indexdef
FROM pg_indexes
WHERE tablename = 'data_sources';

-- æ·»åŠ è¤‡åˆç´¢å¼•
CREATE INDEX CONCURRENTLY idx_data_sources_search 
ON data_sources USING GIN (
    to_tsvector('chinese', name || ' ' || description)
);

-- æ·»åŠ åˆ†é¡ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_data_sources_category 
ON data_sources (category, created_at DESC);
```

**æ–¹æ¡ˆ 3: ä½¿ç”¨æŸ¥è©¢ç·©å­˜**
```python
from functools import lru_cache
from datetime import datetime, timedelta

class SearchCache:
    def __init__(self, ttl_seconds=300):
        self.ttl = ttl_seconds
        self.cache = {}
    
    def get_cache_key(self, query_params):
        """ç”Ÿæˆç·©å­˜éµ"""
        return hash(frozenset(query_params.items()))
    
    def get(self, query_params):
        """ç²å–ç·©å­˜çµæœ"""
        key = self.get_cache_key(query_params)
        if key in self.cache:
            result, timestamp = self.cache[key]
            if datetime.now() - timestamp < timedelta(seconds=self.ttl):
                return result
            else:
                del self.cache[key]
        return None
    
    def set(self, query_params, result):
        """è¨­ç½®ç·©å­˜"""
        key = self.get_cache_key(query_params)
        self.cache[key] = (result, datetime.now())

# ä½¿ç”¨ç·©å­˜
search_cache = SearchCache(ttl_seconds=300)

def search_data_sources(query_params):
    # å…ˆæª¢æŸ¥ç·©å­˜
    cached_result = search_cache.get(query_params)
    if cached_result:
        return cached_result
    
    # åŸ·è¡Œæœå°‹
    result = es.search(index='data_sources', body=query_params)
    
    # å­˜å…¥ç·©å­˜
    search_cache.set(query_params, result)
    
    return result
```

#### é é˜²æªæ–½
- âœ… å®šæœŸé‡å»ºç´¢å¼•ï¼ˆæ¯é€±ä¸€æ¬¡ï¼‰
- âœ… ç›£æ§æŸ¥è©¢æ•ˆèƒ½æŒ‡æ¨™
- âœ… è¨­ç½®åˆç†çš„ç·©å­˜ç­–ç•¥
- âœ… é™åˆ¶è¤‡é›œæŸ¥è©¢çš„ä½¿ç”¨

---

### å•é¡Œ 2: è³‡æ–™æºè¨»å†Šå¤±æ•—

#### ç—‡ç‹€
- API è¿”å› 500 éŒ¯èª¤
- è³‡æ–™æºå‰µå»ºè«‹æ±‚è¶…æ™‚
- æ—¥èªŒä¸­å‡ºç¾è³‡æ–™åº«é€£æ¥éŒ¯èª¤

#### å¯èƒ½åŸå› 
1. è³‡æ–™åº«é€£æ¥æ± è€—ç›¡
2. è³‡æ–™é©—è­‰å¤±æ•—
3. é‡è¤‡çš„è³‡æ–™æº URL
4. æ¬Šé™ä¸è¶³

#### è¨ºæ–·æ­¥é©Ÿ

```python
# æª¢æŸ¥è³‡æ–™åº«é€£æ¥æ± ç‹€æ…‹
import psycopg2
from psycopg2 import pool

def check_connection_pool():
    """æª¢æŸ¥é€£æ¥æ± ç‹€æ…‹"""
    connection_pool = get_connection_pool()
    
    print(f"ç¸½é€£æ¥æ•¸: {connection_pool.maxconn}")
    print(f"æœ€å°é€£æ¥æ•¸: {connection_pool.minconn}")
    print(f"ç•¶å‰æ´»èºé€£æ¥: {len(connection_pool._used)}")
    print(f"ç©ºé–’é€£æ¥: {len(connection_pool._pool)}")
    
    # æª¢æŸ¥é•·æ™‚é–“é‹è¡Œçš„æŸ¥è©¢
    conn = connection_pool.getconn()
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT pid, now() - query_start as duration, query
        FROM pg_stat_activity
        WHERE state = 'active'
        AND now() - query_start > interval '30 seconds'
        ORDER BY duration DESC;
    """)
    
    long_running = cursor.fetchall()
    if long_running:
        print(f"ç™¼ç¾ {len(long_running)} å€‹é•·æ™‚é–“é‹è¡Œçš„æŸ¥è©¢:")
        for pid, duration, query in long_running:
            print(f"  PID {pid}: {duration} - {query[:100]}")
    
    connection_pool.putconn(conn)
```

#### è§£æ±ºæ–¹æ¡ˆ

**æ–¹æ¡ˆ 1: å¢åŠ é€£æ¥æ± å¤§å°**
```python
# config/database.py
DATABASE_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'mirror_realm',
    'user': 'mirror_realm',
    'password': os.getenv('DB_PASSWORD'),
    'minconn': 10,  # å¢åŠ æœ€å°é€£æ¥æ•¸
    'maxconn': 50,  # å¢åŠ æœ€å¤§é€£æ¥æ•¸
    'connect_timeout': 10,
    'options': '-c statement_timeout=30000'  # 30ç§’è¶…æ™‚
}
```

**æ–¹æ¡ˆ 2: æ·»åŠ è«‹æ±‚é‡è©¦æ©Ÿåˆ¶**
```python
import time
from functools import wraps

def retry_on_failure(max_retries=3, delay=1, backoff=2):
    """é‡è©¦è£é£¾å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            retries = 0
            current_delay = delay
            
            while retries < max_retries:
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    retries += 1
                    if retries >= max_retries:
                        raise
                    
                    print(f"è«‹æ±‚å¤±æ•—ï¼Œ{current_delay}ç§’å¾Œé‡è©¦ ({retries}/{max_retries})")
                    time.sleep(current_delay)
                    current_delay *= backoff
            
        return wrapper
    return decorator

@retry_on_failure(max_retries=3, delay=1)
def create_data_source(data):
    """å‰µå»ºè³‡æ–™æºï¼ˆå¸¶é‡è©¦ï¼‰"""
    return db.session.execute(
        insert(DataSource).values(**data)
    )
```

**æ–¹æ¡ˆ 3: æ·»åŠ è³‡æ–™é©—è­‰**
```python
from pydantic import BaseModel, validator, HttpUrl

class DataSourceCreate(BaseModel):
    """è³‡æ–™æºå‰µå»ºé©—è­‰æ¨¡å‹"""
    name: str
    url: HttpUrl
    category: str
    data_type: str
    
    @validator('name')
    def name_must_be_unique(cls, v):
        """é©—è­‰åç¨±å”¯ä¸€æ€§"""
        if DataSource.query.filter_by(name=v).first():
            raise ValueError(f'è³‡æ–™æºåç¨± {v} å·²å­˜åœ¨')
        return v
    
    @validator('url')
    def url_must_be_unique(cls, v):
        """é©—è­‰URLå”¯ä¸€æ€§"""
        if DataSource.query.filter_by(url=str(v)).first():
            raise ValueError(f'è³‡æ–™æºURL {v} å·²å­˜åœ¨')
        return v
    
    @validator('category')
    def category_must_be_valid(cls, v):
        """é©—è­‰åˆ†é¡æœ‰æ•ˆæ€§"""
        valid_categories = ['web', 'api', 'database', 'file', 'social']
        if v not in valid_categories:
            raise ValueError(f'ç„¡æ•ˆçš„åˆ†é¡: {v}')
        return v

# ä½¿ç”¨é©—è­‰
@app.post('/api/v1/data-sources')
def create_data_source_api(data: DataSourceCreate):
    try:
        # Pydantic è‡ªå‹•é©—è­‰
        result = create_data_source(data.dict())
        return {'id': result.id, 'status': 'success'}
    except ValueError as e:
        return {'error': str(e)}, 400
```

#### é é˜²æªæ–½
- âœ… è¨­ç½®åˆç†çš„é€£æ¥æ± å¤§å°
- âœ… æ·»åŠ è«‹æ±‚é©—è­‰å’Œé‡è©¦æ©Ÿåˆ¶
- âœ… ç›£æ§è³‡æ–™åº«é€£æ¥ç‹€æ…‹
- âœ… å®šæœŸæ¸…ç†ç„¡æ•ˆé€£æ¥

---

### å•é¡Œ 3: è³‡æ–™æºç‰ˆæœ¬è¡çª

#### ç—‡ç‹€
- æ›´æ–°è³‡æ–™æºæ™‚è¿”å›è¡çªéŒ¯èª¤
- è³‡æ–™ç‰ˆæœ¬è™Ÿä¸ä¸€è‡´
- ä¸¦ç™¼æ›´æ–°å°è‡´è³‡æ–™ä¸Ÿå¤±

#### å¯èƒ½åŸå› 
1. æ¨‚è§€é–å¤±æ•ˆ
2. ä¸¦ç™¼æ›´æ–°æœªè™•ç†
3. ç‰ˆæœ¬è™Ÿæœªæ­£ç¢ºæ›´æ–°

#### è§£æ±ºæ–¹æ¡ˆ

**ä½¿ç”¨æ¨‚è§€é–**
```python
from sqlalchemy import Column, Integer, String, DateTime
from sqlalchemy.orm import Session

class DataSource(Base):
    __tablename__ = 'data_sources'
    
    id = Column(String, primary_key=True)
    name = Column(String, nullable=False)
    version = Column(Integer, default=1, nullable=False)  # ç‰ˆæœ¬è™Ÿ
    updated_at = Column(DateTime, default=func.now(), onupdate=func.now())

def update_data_source(session: Session, ds_id: str, updates: dict, current_version: int):
    """ä½¿ç”¨æ¨‚è§€é–æ›´æ–°è³‡æ–™æº"""
    result = session.execute(
        update(DataSource)
        .where(DataSource.id == ds_id)
        .where(DataSource.version == current_version)  # ç‰ˆæœ¬æª¢æŸ¥
        .values(
            **updates,
            version=DataSource.version + 1,  # ç‰ˆæœ¬éå¢
            updated_at=func.now()
        )
    )
    
    if result.rowcount == 0:
        raise ConflictError(f'è³‡æ–™æº {ds_id} å·²è¢«å…¶ä»–ç”¨æˆ¶ä¿®æ”¹ï¼Œè«‹é‡æ–°ç²å–æœ€æ–°ç‰ˆæœ¬')
    
    session.commit()
    return get_data_source(session, ds_id)
```

---

## ğŸ“Š ç›£æ§èˆ‡å‘Šè­¦

### é—œéµæŒ‡æ¨™ç›£æ§

```yaml
# prometheus ç›£æ§è¦å‰‡
groups:
  - name: data_source_registry
    rules:
      # API éŸ¿æ‡‰æ™‚é–“
      - alert: SlowAPIResponse
        expr: histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m])) > 1
        for: 5m
        annotations:
          summary: "API éŸ¿æ‡‰éæ…¢"
          description: "P99 å»¶é²è¶…é 1 ç§’"
      
      # æœå°‹å¤±æ•—ç‡
      - alert: HighSearchErrorRate
        expr: rate(search_errors_total[5m]) / rate(search_requests_total[5m]) > 0.05
        for: 5m
        annotations:
          summary: "æœå°‹éŒ¯èª¤ç‡éé«˜"
          description: "æœå°‹å¤±æ•—ç‡è¶…é 5%"
      
      # è³‡æ–™åº«é€£æ¥æ± 
      - alert: ConnectionPoolNearlyFull
        expr: db_connection_pool_active / db_connection_pool_max > 0.9
        for: 2m
        annotations:
          summary: "é€£æ¥æ± æ¥è¿‘é£½å’Œ"
          description: "é€£æ¥æ± ä½¿ç”¨ç‡è¶…é 90%"
```

---

## ğŸ”§ ç¶­è­·å·¥å…·

### è³‡æ–™åº«ç¶­è­·è…³æœ¬

```bash
#!/bin/bash
# db_maintenance.sh - å®šæœŸç¶­è­·è…³æœ¬

# 1. é‡å»ºç´¢å¼•
psql -U mirror_realm -d mirror_realm -c "REINDEX TABLE data_sources;"

# 2. æ›´æ–°çµ±è¨ˆè³‡è¨Š
psql -U mirror_realm -d mirror_realm -c "ANALYZE data_sources;"

# 3. æ¸…ç†èˆŠç‰ˆæœ¬è³‡æ–™
psql -U mirror_realm -d mirror_realm -c "
    DELETE FROM data_source_versions 
    WHERE created_at < NOW() - INTERVAL '90 days'
    AND id NOT IN (
        SELECT MAX(id) FROM data_source_versions GROUP BY data_source_id
    );
"

# 4. å‚™ä»½è³‡æ–™åº«
pg_dump -U mirror_realm mirror_realm > backup_$(date +%Y%m%d).sql

echo "ç¶­è­·å®Œæˆ: $(date)"
```

---

## ğŸ“ ç²å–å¹«åŠ©

å¦‚æœä»¥ä¸Šè§£æ±ºæ–¹æ¡ˆç„¡æ³•è§£æ±ºå•é¡Œï¼Œè«‹ï¼š

1. æ”¶é›†è©³ç´°çš„éŒ¯èª¤æ—¥èªŒå’Œç³»çµ±æŒ‡æ¨™
2. è¨˜éŒ„å•é¡Œå¾©ç¾æ­¥é©Ÿ
3. è¯ç¹«æŠ€è¡“æ”¯æŒåœ˜éšŠ
4. åƒè€ƒ [API æ–‡æª”](ch1-6-APIè©³ç´°è¦ç¯„.md) å’Œ [æœ€ä½³å¯¦è¸](ch1-10-æœ€ä½³å¯¦è¸æŒ‡å—.md)

---

**ç›¸é—œç« ç¯€**:
- [1.7 æ•ˆèƒ½å„ªåŒ–ç­–ç•¥](ch1-7-æ•ˆèƒ½å„ªåŒ–ç­–ç•¥.md)
- [1.8 å®‰å…¨è€ƒæ…®](ch1-8-å®‰å…¨è€ƒæ…®.md)
- [1.10 æœ€ä½³å¯¦è¸æŒ‡å—](ch1-10-æœ€ä½³å¯¦è¸æŒ‡å—.md)
- [â† è¿”å›ç¬¬1ç« é¦–é ](ch1-index.md)

