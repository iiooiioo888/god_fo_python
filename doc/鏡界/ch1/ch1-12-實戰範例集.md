# ç¬¬1ç« ï¼šè³‡æ–™æºè¨»å†Šä¸­å¿ƒ (Data Source Registry)

## 1.12 å¯¦æˆ°ç¯„ä¾‹é›†

**[â† è¿”å›ç¬¬1ç« é¦–é ](ch1-index.md)**

---

æœ¬ç« ç¯€æä¾›è³‡æ–™æºè¨»å†Šä¸­å¿ƒçš„å¯¦éš›æ‡‰ç”¨ç¯„ä¾‹ï¼Œæ¶µè“‹å¸¸è¦‹ä½¿ç”¨å ´æ™¯å’Œæœ€ä½³å¯¦è¸ã€‚

## ğŸ¯ ç¯„ä¾‹ç´¢å¼•

| ç¯„ä¾‹ç·¨è™Ÿ | åç¨± | é›£åº¦ | å ´æ™¯ |
|---------|------|------|------|
| 1 | æ‰¹é‡å°å…¥è³‡æ–™æº | â­â­ | è³‡æ–™é·ç§» |
| 2 | è‡ªå®šç¾©åˆ†é¡ç³»çµ± | â­â­â­ | çµ„ç¹”ç®¡ç† |
| 3 | é«˜ç´šæœå°‹éæ¿¾ | â­â­â­ | è³‡æ–™ç™¼ç¾ |
| 4 | è³‡æ–™æºå¥åº·ç›£æ§ | â­â­â­â­ | é‹ç¶­ç›£æ§ |
| 5 | æ¬Šé™ç®¡ç†ç³»çµ± | â­â­â­â­ | å®‰å…¨æ§åˆ¶ |
| 6 | è³‡æ–™æºæ¨è–¦å¼•æ“ | â­â­â­â­â­ | æ™ºèƒ½æ¨è–¦ |

---

## ç¯„ä¾‹ 1: æ‰¹é‡å°å…¥è³‡æ–™æº

### å ´æ™¯æè¿°
éœ€è¦å°‡ Excel æ–‡ä»¶ä¸­çš„ 1000+ å€‹è³‡æ–™æºæ‰¹é‡å°å…¥åˆ°ç³»çµ±ä¸­ã€‚

### å®Œæ•´å¯¦ç¾

```python
import pandas as pd
import requests
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataSourceBatchImporter:
    """è³‡æ–™æºæ‰¹é‡å°å…¥å™¨"""
    
    def __init__(self, api_base_url: str, api_token: str):
        self.api_base_url = api_base_url
        self.headers = {
            'Authorization': f'Bearer {api_token}',
            'Content-Type': 'application/json'
        }
        self.results = {'success': 0, 'failed': 0, 'errors': []}
    
    def read_excel(self, file_path: str) -> pd.DataFrame:
        """è®€å– Excel æ–‡ä»¶"""
        logger.info(f"è®€å–æ–‡ä»¶: {file_path}")
        df = pd.read_excel(file_path)
        logger.info(f"å…±è®€å– {len(df)} æ¢è¨˜éŒ„")
        return df
    
    def validate_row(self, row: pd.Series) -> tuple[bool, str]:
        """é©—è­‰å–®è¡Œè³‡æ–™"""
        required_fields = ['name', 'url', 'category', 'data_type']
        
        for field in required_fields:
            if pd.isna(row.get(field)):
                return False, f"ç¼ºå°‘å¿…å¡«æ¬„ä½: {field}"
        
        # é©—è­‰ URL æ ¼å¼
        url = row['url']
        if not url.startswith(('http://', 'https://')):
            return False, f"ç„¡æ•ˆçš„ URL æ ¼å¼: {url}"
        
        return True, ""
    
    def transform_row(self, row: pd.Series) -> Dict:
        """è½‰æ›è³‡æ–™æ ¼å¼"""
        return {
            'name': str(row['name']).strip(),
            'display_name': str(row.get('display_name', row['name'])).strip(),
            'url': str(row['url']).strip(),
            'category': str(row['category']).strip().lower(),
            'data_type': str(row['data_type']).strip().lower(),
            'description': str(row.get('description', '')).strip(),
            'tags': str(row.get('tags', '')).split(',') if pd.notna(row.get('tags')) else [],
            'metadata': {
                'source': 'batch_import',
                'import_date': pd.Timestamp.now().isoformat()
            }
        }
    
    def create_data_source(self, data: Dict) -> tuple[bool, str]:
        """å‰µå»ºå–®å€‹è³‡æ–™æº"""
        try:
            response = requests.post(
                f"{self.api_base_url}/api/v1/data-sources",
                json=data,
                headers=self.headers,
                timeout=10
            )
            
            if response.status_code == 201:
                return True, response.json()['id']
            else:
                error_msg = response.json().get('error', 'Unknown error')
                return False, error_msg
                
        except requests.exceptions.RequestException as e:
            return False, str(e)
    
    def import_batch(self, df: pd.DataFrame, batch_size: int = 10, max_workers: int = 5):
        """æ‰¹é‡å°å…¥ï¼ˆä¸¦ç™¼è™•ç†ï¼‰"""
        total = len(df)
        logger.info(f"é–‹å§‹æ‰¹é‡å°å…¥ï¼Œå…± {total} æ¢è¨˜éŒ„")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []
            
            for idx, row in df.iterrows():
                # é©—è­‰è³‡æ–™
                is_valid, error_msg = self.validate_row(row)
                if not is_valid:
                    logger.warning(f"è¡Œ {idx + 2}: é©—è­‰å¤±æ•— - {error_msg}")
                    self.results['failed'] += 1
                    self.results['errors'].append({
                        'row': idx + 2,
                        'name': row.get('name', 'N/A'),
                        'error': error_msg
                    })
                    continue
                
                # è½‰æ›è³‡æ–™
                data = self.transform_row(row)
                
                # æäº¤ä»»å‹™
                future = executor.submit(self.create_data_source, data)
                futures.append((idx + 2, row['name'], future))
            
            # æ”¶é›†çµæœ
            for row_num, name, future in futures:
                try:
                    success, result = future.result()
                    if success:
                        self.results['success'] += 1
                        logger.info(f"âœ“ è¡Œ {row_num}: {name} - æˆåŠŸ (ID: {result})")
                    else:
                        self.results['failed'] += 1
                        self.results['errors'].append({
                            'row': row_num,
                            'name': name,
                            'error': result
                        })
                        logger.error(f"âœ— è¡Œ {row_num}: {name} - å¤±æ•— ({result})")
                except Exception as e:
                    self.results['failed'] += 1
                    self.results['errors'].append({
                        'row': row_num,
                        'name': name,
                        'error': str(e)
                    })
                    logger.error(f"âœ— è¡Œ {row_num}: {name} - ç•°å¸¸ ({e})")
        
        # è¼¸å‡ºçµ±è¨ˆ
        self.print_summary()
    
    def print_summary(self):
        """æ‰“å°å°å…¥æ‘˜è¦"""
        print("\n" + "=" * 60)
        print("æ‰¹é‡å°å…¥å®Œæˆæ‘˜è¦")
        print("=" * 60)
        print(f"âœ“ æˆåŠŸ: {self.results['success']}")
        print(f"âœ— å¤±æ•—: {self.results['failed']}")
        
        if self.results['errors']:
            print(f"\nå¤±æ•—è©³æƒ…:")
            for error in self.results['errors'][:10]:  # åªé¡¯ç¤ºå‰ 10 å€‹
                print(f"  è¡Œ {error['row']}: {error['name']} - {error['error']}")
            
            if len(self.results['errors']) > 10:
                print(f"  ... é‚„æœ‰ {len(self.results['errors']) - 10} å€‹éŒ¯èª¤")
    
    def export_errors(self, output_file: str = 'import_errors.xlsx'):
        """å°å‡ºéŒ¯èª¤è¨˜éŒ„"""
        if self.results['errors']:
            df_errors = pd.DataFrame(self.results['errors'])
            df_errors.to_excel(output_file, index=False)
            logger.info(f"éŒ¯èª¤è¨˜éŒ„å·²å°å‡ºåˆ°: {output_file}")

# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    # é…ç½®
    API_BASE_URL = "http://localhost:8000"
    API_TOKEN = "your_api_token_here"
    EXCEL_FILE = "data_sources.xlsx"
    
    # å‰µå»ºå°å…¥å™¨
    importer = DataSourceBatchImporter(API_BASE_URL, API_TOKEN)
    
    # è®€å–ä¸¦å°å…¥
    df = importer.read_excel(EXCEL_FILE)
    importer.import_batch(df, batch_size=10, max_workers=5)
    
    # å°å‡ºéŒ¯èª¤
    importer.export_errors()
```

### Excel æ–‡ä»¶æ ¼å¼ç¤ºä¾‹

| name | display_name | url | category | data_type | description | tags |
|------|--------------|-----|----------|-----------|-------------|------|
| github-api | GitHub API | https://api.github.com | api | json | GitHub REST API | api,github,code |
| wiki-data | Wikipedia | https://www.wikipedia.org | web | html | Online encyclopedia | wiki,knowledge |

---

## ç¯„ä¾‹ 2: è‡ªå®šç¾©åˆ†é¡ç³»çµ±

### å ´æ™¯æè¿°
éœ€è¦å»ºç«‹ä¸€å€‹å¤šå±¤æ¬¡çš„è³‡æ–™æºåˆ†é¡ç³»çµ±ï¼Œæ”¯æŒè‡ªå®šç¾©åˆ†é¡å’Œæ¨™ç±¤ã€‚

### å®Œæ•´å¯¦ç¾

```python
from typing import List, Dict, Optional
from dataclasses import dataclass
from enum import Enum

class CategoryType(Enum):
    """åˆ†é¡é¡å‹"""
    STANDARD = "standard"  # æ¨™æº–åˆ†é¡
    CUSTOM = "custom"      # è‡ªå®šç¾©åˆ†é¡
    SYSTEM = "system"      # ç³»çµ±åˆ†é¡

@dataclass
class Category:
    """åˆ†é¡æ•¸æ“šé¡"""
    id: str
    name: str
    parent_id: Optional[str]
    type: CategoryType
    icon: Optional[str]
    color: Optional[str]
    description: str
    metadata: Dict

class CategoryManager:
    """åˆ†é¡ç®¡ç†å™¨"""
    
    def __init__(self, db_session):
        self.db = db_session
    
    def create_category(
        self, 
        name: str, 
        parent_id: Optional[str] = None,
        type: CategoryType = CategoryType.CUSTOM,
        **kwargs
    ) -> Category:
        """å‰µå»ºåˆ†é¡"""
        # é©—è­‰çˆ¶åˆ†é¡
        if parent_id:
            parent = self.get_category(parent_id)
            if not parent:
                raise ValueError(f"çˆ¶åˆ†é¡ä¸å­˜åœ¨: {parent_id}")
        
        # å‰µå»ºåˆ†é¡
        category = Category(
            id=generate_id(),
            name=name,
            parent_id=parent_id,
            type=type,
            icon=kwargs.get('icon'),
            color=kwargs.get('color'),
            description=kwargs.get('description', ''),
            metadata=kwargs.get('metadata', {})
        )
        
        self.db.add(category)
        self.db.commit()
        
        return category
    
    def get_category_tree(self, root_id: Optional[str] = None) -> Dict:
        """ç²å–åˆ†é¡æ¨¹"""
        categories = self.db.query(Category).filter_by(parent_id=root_id).all()
        
        tree = []
        for category in categories:
            node = {
                'id': category.id,
                'name': category.name,
                'type': category.type.value,
                'icon': category.icon,
                'color': category.color,
                'children': self.get_category_tree(category.id)
            }
            tree.append(node)
        
        return tree
    
    def assign_category(self, data_source_id: str, category_ids: List[str]):
        """ç‚ºè³‡æ–™æºåˆ†é…åˆ†é¡"""
        # é©—è­‰åˆ†é¡å­˜åœ¨
        for cat_id in category_ids:
            if not self.get_category(cat_id):
                raise ValueError(f"åˆ†é¡ä¸å­˜åœ¨: {cat_id}")
        
        # åˆªé™¤èˆŠåˆ†é¡
        self.db.execute(
            delete(DataSourceCategory)
            .where(DataSourceCategory.data_source_id == data_source_id)
        )
        
        # æ·»åŠ æ–°åˆ†é¡
        for cat_id in category_ids:
            self.db.execute(
                insert(DataSourceCategory)
                .values(data_source_id=data_source_id, category_id=cat_id)
            )
        
        self.db.commit()

# ä½¿ç”¨ç¯„ä¾‹
category_manager = CategoryManager(db_session)

# å‰µå»ºåˆ†é¡å±¤æ¬¡çµæ§‹
tech = category_manager.create_category(
    name="æŠ€è¡“",
    icon="ğŸ’»",
    color="#0066cc"
)

web = category_manager.create_category(
    name="Web é–‹ç™¼",
    parent_id=tech.id,
    icon="ğŸŒ",
    color="#00cc66"
)

api = category_manager.create_category(
    name="API",
    parent_id=web.id,
    icon="ğŸ”Œ",
    color="#cc6600"
)

# ç‚ºè³‡æ–™æºåˆ†é…åˆ†é¡
category_manager.assign_category(
    data_source_id="ds-123",
    category_ids=[api.id, web.id]
)

# ç²å–åˆ†é¡æ¨¹
tree = category_manager.get_category_tree()
print(json.dumps(tree, indent=2, ensure_ascii=False))
```

---

## ç¯„ä¾‹ 3: é«˜ç´šæœå°‹éæ¿¾å™¨

### å ´æ™¯æè¿°
å¯¦ç¾ä¸€å€‹æ”¯æŒå¤šæ¢ä»¶çµ„åˆçš„é«˜ç´šæœå°‹åŠŸèƒ½ã€‚

### å®Œæ•´å¯¦ç¾

```python
from typing import List, Dict, Optional, Any
from enum import Enum
from dataclasses import dataclass
from elasticsearch import Elasticsearch

class FilterOperator(Enum):
    """éæ¿¾é‹ç®—ç¬¦"""
    EQUALS = "eq"
    NOT_EQUALS = "ne"
    CONTAINS = "contains"
    IN = "in"
    NOT_IN = "not_in"
    GREATER_THAN = "gt"
    LESS_THAN = "lt"
    BETWEEN = "between"
    EXISTS = "exists"

@dataclass
class SearchFilter:
    """æœå°‹éæ¿¾å™¨"""
    field: str
    operator: FilterOperator
    value: Any

class AdvancedSearchBuilder:
    """é«˜ç´šæœå°‹æ§‹å»ºå™¨"""
    
    def __init__(self, es_client: Elasticsearch):
        self.es = es_client
        self.filters = []
        self.sort_fields = []
        self.page = 1
        self.page_size = 20
    
    def add_filter(self, field: str, operator: FilterOperator, value: Any):
        """æ·»åŠ éæ¿¾æ¢ä»¶"""
        self.filters.append(SearchFilter(field, operator, value))
        return self
    
    def add_sort(self, field: str, order: str = "asc"):
        """æ·»åŠ æ’åº"""
        self.sort_fields.append({field: {"order": order}})
        return self
    
    def set_pagination(self, page: int, page_size: int):
        """è¨­ç½®åˆ†é """
        self.page = page
        self.page_size = page_size
        return self
    
    def build_query(self) -> Dict:
        """æ§‹å»º Elasticsearch æŸ¥è©¢"""
        query = {
            "bool": {
                "must": [],
                "filter": [],
                "should": [],
                "must_not": []
            }
        }
        
        for filter in self.filters:
            condition = self._build_condition(filter)
            if condition:
                query["bool"]["filter"].append(condition)
        
        return query
    
    def _build_condition(self, filter: SearchFilter) -> Dict:
        """æ§‹å»ºå–®å€‹æ¢ä»¶"""
        if filter.operator == FilterOperator.EQUALS:
            return {"term": {filter.field: filter.value}}
        
        elif filter.operator == FilterOperator.CONTAINS:
            return {"match": {filter.field: filter.value}}
        
        elif filter.operator == FilterOperator.IN:
            return {"terms": {filter.field: filter.value}}
        
        elif filter.operator == FilterOperator.NOT_IN:
            return {"bool": {"must_not": {"terms": {filter.field: filter.value}}}}
        
        elif filter.operator == FilterOperator.GREATER_THAN:
            return {"range": {filter.field: {"gt": filter.value}}}
        
        elif filter.operator == FilterOperator.LESS_THAN:
            return {"range": {filter.field: {"lt": filter.value}}}
        
        elif filter.operator == FilterOperator.BETWEEN:
            return {"range": {filter.field: {"gte": filter.value[0], "lte": filter.value[1]}}}
        
        elif filter.operator == FilterOperator.EXISTS:
            return {"exists": {"field": filter.field}}
        
        return {}
    
    def execute(self) -> Dict:
        """åŸ·è¡Œæœå°‹"""
        query = self.build_query()
        
        body = {
            "query": query,
            "from": (self.page - 1) * self.page_size,
            "size": self.page_size
        }
        
        if self.sort_fields:
            body["sort"] = self.sort_fields
        
        response = self.es.search(index="data_sources", body=body)
        
        return {
            "total": response["hits"]["total"]["value"],
            "results": [hit["_source"] for hit in response["hits"]["hits"]],
            "page": self.page,
            "page_size": self.page_size
        }

# ä½¿ç”¨ç¯„ä¾‹
es = Elasticsearch(['localhost:9200'])
search = AdvancedSearchBuilder(es)

# æ§‹å»ºè¤‡é›œæŸ¥è©¢
results = (search
    .add_filter("category", FilterOperator.IN, ["api", "web"])
    .add_filter("data_type", FilterOperator.EQUALS, "json")
    .add_filter("created_at", FilterOperator.GREATER_THAN, "2024-01-01")
    .add_filter("tags", FilterOperator.CONTAINS, "python")
    .add_sort("created_at", "desc")
    .set_pagination(page=1, page_size=20)
    .execute())

print(f"æ‰¾åˆ° {results['total']} å€‹çµæœ")
for item in results['results']:
    print(f"- {item['name']}: {item['url']}")
```

---

## ğŸ”— ç›¸é—œè³‡æº

- [1.4 æ ¸å¿ƒçµ„ä»¶è©³ç´°å¯¦ç¾](ch1-4-æ ¸å¿ƒçµ„ä»¶è©³ç´°å¯¦ç¾.md)
- [1.6 APIè©³ç´°è¦ç¯„](ch1-6-APIè©³ç´°è¦ç¯„.md)
- [1.11 æ•…éšœæ’æŸ¥æŒ‡å—](ch1-11-æ•…éšœæ’æŸ¥æŒ‡å—.md)
- [â† è¿”å›ç¬¬1ç« é¦–é ](ch1-index.md)

