# Á¨¨1Á´†ÔºöË≥áÊñôÊ∫êË®ªÂÜä‰∏≠ÂøÉ (Data Source Registry)

## 1.12 ÂØ¶Êà∞ÁØÑ‰æãÈõÜ

**[‚Üê ËøîÂõûÁ¨¨1Á´†È¶ñÈ†Å](ch1-index.md)**

---

Êú¨Á´†ÁØÄÊèê‰æõË≥áÊñôÊ∫êË®ªÂÜä‰∏≠ÂøÉÁöÑÂØ¶ÈöõÊáâÁî®ÁØÑ‰æãÔºåÊ∂µËìãÂ∏∏Ë¶ã‰ΩøÁî®Â†¥ÊôØÂíåÊúÄ‰Ω≥ÂØ¶Ë∏ê„ÄÇ

## üéØ ÁØÑ‰æãÁ¥¢Âºï

| ÁØÑ‰æãÁ∑®Ëôü | ÂêçÁ®± | Èõ£Â∫¶ | Â†¥ÊôØ |
|---------|------|------|------|
| 1 | ÊâπÈáèÂ∞éÂÖ•Ë≥áÊñôÊ∫ê | ‚≠ê‚≠ê | Ë≥áÊñôÈÅ∑Áßª |
| 2 | Ëá™ÂÆöÁæ©ÂàÜÈ°ûÁ≥ªÁµ± | ‚≠ê‚≠ê‚≠ê | ÁµÑÁπîÁÆ°ÁêÜ |
| 3 | È´òÁ¥öÊêúÂ∞ãÈÅéÊøæ | ‚≠ê‚≠ê‚≠ê | Ë≥áÊñôÁôºÁèæ |
| 4 | Ë≥áÊñôÊ∫êÂÅ•Â∫∑Áõ£Êéß | ‚≠ê‚≠ê‚≠ê‚≠ê | ÈÅãÁ∂≠Áõ£Êéß |
| 5 | Ê¨äÈôêÁÆ°ÁêÜÁ≥ªÁµ± | ‚≠ê‚≠ê‚≠ê‚≠ê | ÂÆâÂÖ®ÊéßÂà∂ |
| 6 | Ë≥áÊñôÊ∫êÊé®Ëñ¶ÂºïÊìé | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Êô∫ËÉΩÊé®Ëñ¶ |

---

## ÁØÑ‰æã 1: ÊâπÈáèÂ∞éÂÖ•Ë≥áÊñôÊ∫ê

### Â†¥ÊôØÊèèËø∞
ÈúÄË¶ÅÂ∞á Excel Êñá‰ª∂‰∏≠ÁöÑ 1000+ ÂÄãË≥áÊñôÊ∫êÊâπÈáèÂ∞éÂÖ•Âà∞Á≥ªÁµ±‰∏≠„ÄÇ

### ÂÆåÊï¥ÂØ¶Áèæ

```python
import pandas as pd
import requests
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataSourceBatchImporter:
    """Ë≥áÊñôÊ∫êÊâπÈáèÂ∞éÂÖ•Âô®"""
    
    def __init__(self, api_base_url: str, api_token: str):
        self.api_base_url = api_base_url
        self.headers = {
            'Authorization': f'Bearer {api_token}',
            'Content-Type': 'application/json'
        }
        self.results = {'success': 0, 'failed': 0, 'errors': []}
    
    def read_excel(self, file_path: str) -> pd.DataFrame:
        """ËÆÄÂèñ Excel Êñá‰ª∂"""
        logger.info(f"ËÆÄÂèñÊñá‰ª∂: {file_path}")
        df = pd.read_excel(file_path)
        logger.info(f"ÂÖ±ËÆÄÂèñ {len(df)} Ê¢ùË®òÈåÑ")
        return df
    
    def validate_row(self, row: pd.Series) -> tuple[bool, str]:
        """È©óË≠âÂñÆË°åË≥áÊñô"""
        required_fields = ['name', 'url', 'category', 'data_type']
        
        for field in required_fields:
            if pd.isna(row.get(field)):
                return False, f"Áº∫Â∞ëÂøÖÂ°´Ê¨Ñ‰Ωç: {field}"
        
        # È©óË≠â URL Ê†ºÂºè
        url = row['url']
        if not url.startswith(('http://', 'https://')):
            return False, f"ÁÑ°ÊïàÁöÑ URL Ê†ºÂºè: {url}"
        
        return True, ""
    
    def transform_row(self, row: pd.Series) -> Dict:
        """ËΩâÊèõË≥áÊñôÊ†ºÂºè"""
        return {
            'name': str(row['name']).strip(),
            'display_name': str(row.get('display_name', row['name'])).strip(),
            'url': str(row['url']).strip(),
            'category': str(row['category']).strip().lower(),
            'data_type': str(row['data_type']).strip().lower(),
            'description': str(row.get('description', '')).strip(),
            'tags': str(row.get('tags', '')).split(',') if pd.notna(row.get('tags')) else [],
            'metadata': {
                'source': 'batch_import',
                'import_date': pd.Timestamp.now().isoformat()
            }
        }
    
    def create_data_source(self, data: Dict) -> tuple[bool, str]:
        """ÂâµÂª∫ÂñÆÂÄãË≥áÊñôÊ∫ê"""
        try:
            response = requests.post(
                f"{self.api_base_url}/api/v1/data-sources",
                json=data,
                headers=self.headers,
                timeout=10
            )
            
            if response.status_code == 201:
                return True, response.json()['id']
            else:
                error_msg = response.json().get('error', 'Unknown error')
                return False, error_msg
                
        except requests.exceptions.RequestException as e:
            return False, str(e)
    
    def import_batch(self, df: pd.DataFrame, batch_size: int = 10, max_workers: int = 5):
        """ÊâπÈáèÂ∞éÂÖ•Ôºà‰∏¶ÁôºËôïÁêÜÔºâ"""
        total = len(df)
        logger.info(f"ÈñãÂßãÊâπÈáèÂ∞éÂÖ•ÔºåÂÖ± {total} Ê¢ùË®òÈåÑ")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []
            
            for idx, row in df.iterrows():
                # È©óË≠âË≥áÊñô
                is_valid, error_msg = self.validate_row(row)
                if not is_valid:
                    logger.warning(f"Ë°å {idx + 2}: È©óË≠âÂ§±Êïó - {error_msg}")
                    self.results['failed'] += 1
                    self.results['errors'].append({
                        'row': idx + 2,
                        'name': row.get('name', 'N/A'),
                        'error': error_msg
                    })
                    continue
                
                # ËΩâÊèõË≥áÊñô
                data = self.transform_row(row)
                
                # Êèê‰∫§‰ªªÂãô
                future = executor.submit(self.create_data_source, data)
                futures.append((idx + 2, row['name'], future))
            
            # Êî∂ÈõÜÁµêÊûú
            for row_num, name, future in futures:
                try:
                    success, result = future.result()
                    if success:
                        self.results['success'] += 1
                        logger.info(f"‚úì Ë°å {row_num}: {name} - ÊàêÂäü (ID: {result})")
                    else:
                        self.results['failed'] += 1
                        self.results['errors'].append({
                            'row': row_num,
                            'name': name,
                            'error': result
                        })
                        logger.error(f"‚úó Ë°å {row_num}: {name} - Â§±Êïó ({result})")
                except Exception as e:
                    self.results['failed'] += 1
                    self.results['errors'].append({
                        'row': row_num,
                        'name': name,
                        'error': str(e)
                    })
                    logger.error(f"‚úó Ë°å {row_num}: {name} - Áï∞Â∏∏ ({e})")
        
        # Ëº∏Âá∫Áµ±Ë®à
        self.print_summary()
    
    def print_summary(self):
        """ÊâìÂç∞Â∞éÂÖ•ÊëòË¶Å"""
        print("\n" + "=" * 60)
        print("ÊâπÈáèÂ∞éÂÖ•ÂÆåÊàêÊëòË¶Å")
        print("=" * 60)
        print(f"‚úì ÊàêÂäü: {self.results['success']}")
        print(f"‚úó Â§±Êïó: {self.results['failed']}")
        
        if self.results['errors']:
            print(f"\nÂ§±ÊïóË©≥ÊÉÖ:")
            for error in self.results['errors'][:10]:  # Âè™È°ØÁ§∫Ââç 10 ÂÄã
                print(f"  Ë°å {error['row']}: {error['name']} - {error['error']}")
            
            if len(self.results['errors']) > 10:
                print(f"  ... ÈÇÑÊúâ {len(self.results['errors']) - 10} ÂÄãÈåØË™§")
    
    def export_errors(self, output_file: str = 'import_errors.xlsx'):
        """Â∞éÂá∫ÈåØË™§Ë®òÈåÑ"""
        if self.results['errors']:
            df_errors = pd.DataFrame(self.results['errors'])
            df_errors.to_excel(output_file, index=False)
            logger.info(f"ÈåØË™§Ë®òÈåÑÂ∑≤Â∞éÂá∫Âà∞: {output_file}")

# ‰ΩøÁî®ÁØÑ‰æã
if __name__ == "__main__":
    # ÈÖçÁΩÆ
    API_BASE_URL = "http://localhost:8000"
    API_TOKEN = "your_api_token_here"
    EXCEL_FILE = "data_sources.xlsx"
    
    # ÂâµÂª∫Â∞éÂÖ•Âô®
    importer = DataSourceBatchImporter(API_BASE_URL, API_TOKEN)
    
    # ËÆÄÂèñ‰∏¶Â∞éÂÖ•
    df = importer.read_excel(EXCEL_FILE)
    importer.import_batch(df, batch_size=10, max_workers=5)
    
    # Â∞éÂá∫ÈåØË™§
    importer.export_errors()
```

### Excel Êñá‰ª∂Ê†ºÂºèÁ§∫‰æã

| name | display_name | url | category | data_type | description | tags |
|------|--------------|-----|----------|-----------|-------------|------|
| github-api | GitHub API | https://api.github.com | api | json | GitHub REST API | api,github,code |
| wiki-data | Wikipedia | https://www.wikipedia.org | web | html | Online encyclopedia | wiki,knowledge |

---

## ÁØÑ‰æã 2: Ëá™ÂÆöÁæ©ÂàÜÈ°ûÁ≥ªÁµ±

### Â†¥ÊôØÊèèËø∞
ÈúÄË¶ÅÂª∫Á´ã‰∏ÄÂÄãÂ§öÂ±§Ê¨°ÁöÑË≥áÊñôÊ∫êÂàÜÈ°ûÁ≥ªÁµ±ÔºåÊîØÊåÅËá™ÂÆöÁæ©ÂàÜÈ°ûÂíåÊ®ôÁ±§„ÄÇ

### ÂÆåÊï¥ÂØ¶Áèæ

```python
from typing import List, Dict, Optional
from dataclasses import dataclass
from enum import Enum

class CategoryType(Enum):
    """ÂàÜÈ°ûÈ°ûÂûã"""
    STANDARD = "standard"  # Ê®ôÊ∫ñÂàÜÈ°û
    CUSTOM = "custom"      # Ëá™ÂÆöÁæ©ÂàÜÈ°û
    SYSTEM = "system"      # Á≥ªÁµ±ÂàÜÈ°û

@dataclass
class Category:
    """ÂàÜÈ°ûÊï∏ÊìöÈ°û"""
    id: str
    name: str
    parent_id: Optional[str]
    type: CategoryType
    icon: Optional[str]
    color: Optional[str]
    description: str
    metadata: Dict

class CategoryManager:
    """ÂàÜÈ°ûÁÆ°ÁêÜÂô®"""
    
    def __init__(self, db_session):
        self.db = db_session
    
    def create_category(
        self, 
        name: str, 
        parent_id: Optional[str] = None,
        type: CategoryType = CategoryType.CUSTOM,
        **kwargs
    ) -> Category:
        """ÂâµÂª∫ÂàÜÈ°û"""
        # È©óË≠âÁà∂ÂàÜÈ°û
        if parent_id:
            parent = self.get_category(parent_id)
            if not parent:
                raise ValueError(f"Áà∂ÂàÜÈ°û‰∏çÂ≠òÂú®: {parent_id}")
        
        # ÂâµÂª∫ÂàÜÈ°û
        category = Category(
            id=generate_id(),
            name=name,
            parent_id=parent_id,
            type=type,
            icon=kwargs.get('icon'),
            color=kwargs.get('color'),
            description=kwargs.get('description', ''),
            metadata=kwargs.get('metadata', {})
        )
        
        self.db.add(category)
        self.db.commit()
        
        return category
    
    def get_category_tree(self, root_id: Optional[str] = None) -> Dict:
        """Áç≤ÂèñÂàÜÈ°ûÊ®π"""
        categories = self.db.query(Category).filter_by(parent_id=root_id).all()
        
        tree = []
        for category in categories:
            node = {
                'id': category.id,
                'name': category.name,
                'type': category.type.value,
                'icon': category.icon,
                'color': category.color,
                'children': self.get_category_tree(category.id)
            }
            tree.append(node)
        
        return tree
    
    def assign_category(self, data_source_id: str, category_ids: List[str]):
        """ÁÇ∫Ë≥áÊñôÊ∫êÂàÜÈÖçÂàÜÈ°û"""
        # È©óË≠âÂàÜÈ°ûÂ≠òÂú®
        for cat_id in category_ids:
            if not self.get_category(cat_id):
                raise ValueError(f"ÂàÜÈ°û‰∏çÂ≠òÂú®: {cat_id}")
        
        # Âà™Èô§ËàäÂàÜÈ°û
        self.db.execute(
            delete(DataSourceCategory)
            .where(DataSourceCategory.data_source_id == data_source_id)
        )
        
        # Ê∑ªÂä†Êñ∞ÂàÜÈ°û
        for cat_id in category_ids:
            self.db.execute(
                insert(DataSourceCategory)
                .values(data_source_id=data_source_id, category_id=cat_id)
            )
        
        self.db.commit()

# ‰ΩøÁî®ÁØÑ‰æã
category_manager = CategoryManager(db_session)

# ÂâµÂª∫ÂàÜÈ°ûÂ±§Ê¨°ÁµêÊßã
tech = category_manager.create_category(
    name="ÊäÄË°ì",
    icon="üíª",
    color="#0066cc"
)

web = category_manager.create_category(
    name="Web ÈñãÁôº",
    parent_id=tech.id,
    icon="üåê",
    color="#00cc66"
)

api = category_manager.create_category(
    name="API",
    parent_id=web.id,
    icon="üîå",
    color="#cc6600"
)

# ÁÇ∫Ë≥áÊñôÊ∫êÂàÜÈÖçÂàÜÈ°û
category_manager.assign_category(
    data_source_id="ds-123",
    category_ids=[api.id, web.id]
)

# Áç≤ÂèñÂàÜÈ°ûÊ®π
tree = category_manager.get_category_tree()
print(json.dumps(tree, indent=2, ensure_ascii=False))
```

---

## ÁØÑ‰æã 3: È´òÁ¥öÊêúÂ∞ãÈÅéÊøæÂô®

### Â†¥ÊôØÊèèËø∞
ÂØ¶Áèæ‰∏ÄÂÄãÊîØÊåÅÂ§öÊ¢ù‰ª∂ÁµÑÂêàÁöÑÈ´òÁ¥öÊêúÂ∞ãÂäüËÉΩ„ÄÇ

### ÂÆåÊï¥ÂØ¶Áèæ

```python
from typing import List, Dict, Optional, Any
from enum import Enum
from dataclasses import dataclass
from elasticsearch import Elasticsearch

class FilterOperator(Enum):
    """ÈÅéÊøæÈÅãÁÆóÁ¨¶"""
    EQUALS = "eq"
    NOT_EQUALS = "ne"
    CONTAINS = "contains"
    IN = "in"
    NOT_IN = "not_in"
    GREATER_THAN = "gt"
    LESS_THAN = "lt"
    BETWEEN = "between"
    EXISTS = "exists"

@dataclass
class SearchFilter:
    """ÊêúÂ∞ãÈÅéÊøæÂô®"""
    field: str
    operator: FilterOperator
    value: Any

class AdvancedSearchBuilder:
    """È´òÁ¥öÊêúÂ∞ãÊßãÂª∫Âô®"""
    
    def __init__(self, es_client: Elasticsearch):
        self.es = es_client
        self.filters = []
        self.sort_fields = []
        self.page = 1
        self.page_size = 20
    
    def add_filter(self, field: str, operator: FilterOperator, value: Any):
        """Ê∑ªÂä†ÈÅéÊøæÊ¢ù‰ª∂"""
        self.filters.append(SearchFilter(field, operator, value))
        return self
    
    def add_sort(self, field: str, order: str = "asc"):
        """Ê∑ªÂä†ÊéíÂ∫è"""
        self.sort_fields.append({field: {"order": order}})
        return self
    
    def set_pagination(self, page: int, page_size: int):
        """Ë®≠ÁΩÆÂàÜÈ†Å"""
        self.page = page
        self.page_size = page_size
        return self
    
    def build_query(self) -> Dict:
        """ÊßãÂª∫ Elasticsearch Êü•Ë©¢"""
        query = {
            "bool": {
                "must": [],
                "filter": [],
                "should": [],
                "must_not": []
            }
        }
        
        for filter in self.filters:
            condition = self._build_condition(filter)
            if condition:
                query["bool"]["filter"].append(condition)
        
        return query
    
    def _build_condition(self, filter: SearchFilter) -> Dict:
        """ÊßãÂª∫ÂñÆÂÄãÊ¢ù‰ª∂"""
        if filter.operator == FilterOperator.EQUALS:
            return {"term": {filter.field: filter.value}}
        
        elif filter.operator == FilterOperator.CONTAINS:
            return {"match": {filter.field: filter.value}}
        
        elif filter.operator == FilterOperator.IN:
            return {"terms": {filter.field: filter.value}}
        
        elif filter.operator == FilterOperator.NOT_IN:
            return {"bool": {"must_not": {"terms": {filter.field: filter.value}}}}
        
        elif filter.operator == FilterOperator.GREATER_THAN:
            return {"range": {filter.field: {"gt": filter.value}}}
        
        elif filter.operator == FilterOperator.LESS_THAN:
            return {"range": {filter.field: {"lt": filter.value}}}
        
        elif filter.operator == FilterOperator.BETWEEN:
            return {"range": {filter.field: {"gte": filter.value[0], "lte": filter.value[1]}}}
        
        elif filter.operator == FilterOperator.EXISTS:
            return {"exists": {"field": filter.field}}
        
        return {}
    
    def execute(self) -> Dict:
        """Âü∑Ë°åÊêúÂ∞ã"""
        query = self.build_query()
        
        body = {
            "query": query,
            "from": (self.page - 1) * self.page_size,
            "size": self.page_size
        }
        
        if self.sort_fields:
            body["sort"] = self.sort_fields
        
        response = self.es.search(index="data_sources", body=body)
        
        return {
            "total": response["hits"]["total"]["value"],
            "results": [hit["_source"] for hit in response["hits"]["hits"]],
            "page": self.page,
            "page_size": self.page_size
        }

# ‰ΩøÁî®ÁØÑ‰æã
es = Elasticsearch(['localhost:9200'])
search = AdvancedSearchBuilder(es)

# ÊßãÂª∫Ë§áÈõúÊü•Ë©¢
results = (search
    .add_filter("category", FilterOperator.IN, ["api", "web"])
    .add_filter("data_type", FilterOperator.EQUALS, "json")
    .add_filter("created_at", FilterOperator.GREATER_THAN, "2024-01-01")
    .add_filter("tags", FilterOperator.CONTAINS, "python")
    .add_sort("created_at", "desc")
    .set_pagination(page=1, page_size=20)
    .execute())

print(f"ÊâæÂà∞ {results['total']} ÂÄãÁµêÊûú")
for item in results['results']:
    print(f"- {item['name']}: {item['url']}")
```

---

## üîó Áõ∏ÈóúË≥áÊ∫ê

- [1.4 Ê†∏ÂøÉÁµÑ‰ª∂Ë©≥Á¥∞ÂØ¶Áèæ](ch1-4-Ê†∏ÂøÉÁµÑ‰ª∂Ë©≥Á¥∞ÂØ¶Áèæ.md)
- [1.6 APIË©≥Á¥∞Ë¶èÁØÑ](ch1-6-APIË©≥Á¥∞Ë¶èÁØÑ.md)
- [1.11 ÊïÖÈöúÊéíÊü•ÊåáÂçó](ch1-11-ÊïÖÈöúÊéíÊü•ÊåáÂçó.md)
- [‚Üê ËøîÂõûÁ¨¨1Á´†È¶ñÈ†Å](ch1-index.md)

