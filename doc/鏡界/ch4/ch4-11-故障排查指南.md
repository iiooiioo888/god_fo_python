# ç¬¬4ç« ï¼šè³‡æ–™è™•ç†å·¥ä½œæµå¼•æ“ (Data Processing Workflow Engine)

## 4.11 æ•…éšœæ’æŸ¥æŒ‡å—

**[â† è¿”å›ç¬¬4ç« é¦–é ](ch4-index.md)**

---

æœ¬ç« ç¯€æä¾›è³‡æ–™è™•ç†å·¥ä½œæµå¼•æ“å¸¸è¦‹å•é¡Œçš„è¨ºæ–·å’Œè§£æ±ºæ–¹æ¡ˆã€‚

## ğŸ” å•é¡Œè¨ºæ–·æµç¨‹

```mermaid
graph TD
    A[å·¥ä½œæµç•°å¸¸] --> B{å•é¡Œé¡å‹?}
    B -->|åŸ·è¡Œå¤±æ•—| C[æª¢æŸ¥ä»»å‹™æ—¥èªŒ]
    B -->|æ•ˆèƒ½å•é¡Œ| D[åˆ†æç“¶é ¸]
    B -->|è³‡æºä¸è¶³| E[æª¢æŸ¥è³‡æºä½¿ç”¨]
    C --> F[å®šä½å¤±æ•—ç¯€é»]
    D --> G[å„ªåŒ–è™•ç†é‚è¼¯]
    E --> H[æ“´å±•è³‡æº]
    F --> I[æ‡‰ç”¨è§£æ±ºæ–¹æ¡ˆ]
    G --> I
    H --> I
```

---

## ğŸ› å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ

### å•é¡Œ 1: å·¥ä½œæµåŸ·è¡Œå¡ä½

#### ç—‡ç‹€
- å·¥ä½œæµé•·æ™‚é–“è™•æ–¼é‹è¡Œç‹€æ…‹
- ç‰¹å®šç¯€é»ç„¡éŸ¿æ‡‰
- ä»»å‹™éšŠåˆ—ç©å£“

#### è¨ºæ–·å·¥å…·

```python
class WorkflowDiagnostics:
    """å·¥ä½œæµè¨ºæ–·å·¥å…·"""
    
    def __init__(self, workflow_engine):
        self.engine = workflow_engine
        self.logger = logging.getLogger(__name__)
    
    def diagnose_stuck_workflow(self, workflow_id: str) -> Dict:
        """è¨ºæ–·å¡ä½çš„å·¥ä½œæµ"""
        workflow = self.engine.get_workflow(workflow_id)
        
        diagnostics = {
            'workflow_id': workflow_id,
            'status': workflow.status,
            'duration': (datetime.now() - workflow.started_at).total_seconds(),
            'stuck_nodes': [],
            'resource_usage': {},
            'suggestions': []
        }
        
        # æª¢æŸ¥æ¯å€‹ç¯€é»
        for node in workflow.nodes:
            if node.status == 'running':
                duration = (datetime.now() - node.started_at).total_seconds()
                
                # å¦‚æœé‹è¡Œæ™‚é–“è¶…éé æœŸ
                if duration > node.expected_duration * 2:
                    diagnostics['stuck_nodes'].append({
                        'node_id': node.id,
                        'type': node.type,
                        'duration': duration,
                        'expected': node.expected_duration,
                        'logs': self._get_recent_logs(node)
                    })
        
        # æª¢æŸ¥è³‡æºä½¿ç”¨
        diagnostics['resource_usage'] = self._check_resource_usage(workflow)
        
        # ç”Ÿæˆå»ºè­°
        diagnostics['suggestions'] = self._generate_suggestions(diagnostics)
        
        return diagnostics
    
    def _get_recent_logs(self, node, lines=50) -> List[str]:
        """ç²å–æœ€è¿‘çš„æ—¥èªŒ"""
        logs = self.engine.get_node_logs(node.id, limit=lines)
        return [log.message for log in logs]
    
    def _check_resource_usage(self, workflow) -> Dict:
        """æª¢æŸ¥è³‡æºä½¿ç”¨"""
        return {
            'cpu_usage': workflow.metrics.cpu_usage,
            'memory_usage': workflow.metrics.memory_usage,
            'io_wait': workflow.metrics.io_wait,
            'network_io': workflow.metrics.network_io
        }
    
    def _generate_suggestions(self, diagnostics: Dict) -> List[str]:
        """ç”Ÿæˆå»ºè­°"""
        suggestions = []
        
        # æª¢æŸ¥å¡ä½çš„ç¯€é»
        if diagnostics['stuck_nodes']:
            suggestions.append("ç™¼ç¾å¡ä½çš„ç¯€é»ï¼Œå»ºè­°æª¢æŸ¥ç¯€é»æ—¥èªŒ")
            suggestions.append("è€ƒæ…®è¨­ç½®ç¯€é»è¶…æ™‚ä¸¦é‡è©¦")
        
        # æª¢æŸ¥è³‡æºä½¿ç”¨
        resources = diagnostics['resource_usage']
        if resources.get('cpu_usage', 0) > 90:
            suggestions.append("CPUä½¿ç”¨ç‡éé«˜ï¼Œè€ƒæ…®å„ªåŒ–è™•ç†é‚è¼¯æˆ–å¢åŠ è³‡æº")
        
        if resources.get('io_wait', 0) > 50:
            suggestions.append("IOç­‰å¾…æ™‚é–“éé•·ï¼Œæª¢æŸ¥ç£ç¢Ÿæˆ–ç¶²è·¯ç‹€æ³")
        
        return suggestions

# ä½¿ç”¨ç¯„ä¾‹
diagnostics = WorkflowDiagnostics(workflow_engine)
result = diagnostics.diagnose_stuck_workflow("workflow-123")

print(f"å·¥ä½œæµç‹€æ…‹: {result['status']}")
print(f"é‹è¡Œæ™‚é–“: {result['duration']}ç§’")
print(f"å¡ä½çš„ç¯€é»æ•¸: {len(result['stuck_nodes'])}")

for node in result['stuck_nodes']:
    print(f"\nç¯€é» {node['node_id']}:")
    print(f"  é¡å‹: {node['type']}")
    print(f"  é‹è¡Œæ™‚é–“: {node['duration']}ç§’ (é æœŸ: {node['expected']}ç§’)")
    print(f"  æœ€è¿‘æ—¥èªŒ: {node['logs'][-5:]}")

print(f"\nå»ºè­°:")
for suggestion in result['suggestions']:
    print(f"  - {suggestion}")
```

#### è§£æ±ºæ–¹æ¡ˆ

**æ–¹æ¡ˆ 1: å¯¦ç¾ç¯€é»è¶…æ™‚æ©Ÿåˆ¶**

```python
from celery import Task
from celery.exceptions import SoftTimeLimitExceeded
import signal

class TimeoutTask(Task):
    """å¸¶è¶…æ™‚çš„ä»»å‹™"""
    
    def __call__(self, *args, **kwargs):
        # è¨­ç½®è»Ÿè¶…æ™‚å’Œç¡¬è¶…æ™‚
        soft_timeout = kwargs.pop('soft_timeout', 300)  # 5åˆ†é˜
        hard_timeout = kwargs.pop('hard_timeout', 600)  # 10åˆ†é˜
        
        try:
            # è¨­ç½®ä¿¡è™Ÿè™•ç†
            signal.signal(signal.SIGALRM, self._timeout_handler)
            signal.alarm(hard_timeout)
            
            # åŸ·è¡Œä»»å‹™
            result = self.run(*args, **kwargs)
            
            # å–æ¶ˆè¶…æ™‚
            signal.alarm(0)
            
            return result
            
        except SoftTimeLimitExceeded:
            self.logger.warning(f"ä»»å‹™é”åˆ°è»Ÿè¶…æ™‚é™åˆ¶: {soft_timeout}ç§’")
            # å˜—è©¦å„ªé›…åœ°åœæ­¢
            self.cleanup()
            raise
        
        except Exception as e:
            signal.alarm(0)
            self.logger.error(f"ä»»å‹™åŸ·è¡Œå¤±æ•—: {str(e)}")
            raise
    
    def _timeout_handler(self, signum, frame):
        """è¶…æ™‚è™•ç†"""
        raise TimeoutError("ä»»å‹™åŸ·è¡Œè¶…æ™‚")
    
    def cleanup(self):
        """æ¸…ç†è³‡æº"""
        pass

# ä½¿ç”¨ç¯„ä¾‹
@app.task(base=TimeoutTask, bind=True)
def process_data(self, data, soft_timeout=300, hard_timeout=600):
    """è™•ç†è³‡æ–™ï¼ˆå¸¶è¶…æ™‚ï¼‰"""
    try:
        # è™•ç†é‚è¼¯
        result = heavy_processing(data)
        return result
    except TimeoutError:
        self.logger.error("è™•ç†è¶…æ™‚ï¼Œä¿å­˜ä¸­é–“çµæœ")
        # ä¿å­˜é€²åº¦
        self.update_state(state='TIMEOUT', meta={'progress': self.progress})
        raise
```

**æ–¹æ¡ˆ 2: å¯¦ç¾æ–·é»çºŒå‚³**

```python
class ResumableWorkflow:
    """å¯æ¢å¾©çš„å·¥ä½œæµ"""
    
    def __init__(self, workflow_id: str, checkpoint_interval: int = 60):
        self.workflow_id = workflow_id
        self.checkpoint_interval = checkpoint_interval
        self.last_checkpoint = time.time()
    
    def execute(self):
        """åŸ·è¡Œå·¥ä½œæµ"""
        # æª¢æŸ¥æ˜¯å¦æœ‰æª¢æŸ¥é»
        checkpoint = self.load_checkpoint()
        
        if checkpoint:
            self.logger.info(f"å¾æª¢æŸ¥é»æ¢å¾©: {checkpoint['node_id']}")
            start_node = checkpoint['node_id']
            context = checkpoint['context']
        else:
            start_node = self.workflow.start_node
            context = {}
        
        # å¾æŒ‡å®šç¯€é»é–‹å§‹åŸ·è¡Œ
        current_node = start_node
        
        while current_node:
            try:
                # åŸ·è¡Œç¯€é»
                result = self.execute_node(current_node, context)
                context.update(result)
                
                # å®šæœŸä¿å­˜æª¢æŸ¥é»
                if time.time() - self.last_checkpoint > self.checkpoint_interval:
                    self.save_checkpoint(current_node.id, context)
                    self.last_checkpoint = time.time()
                
                # ç§»å‹•åˆ°ä¸‹ä¸€å€‹ç¯€é»
                current_node = self.get_next_node(current_node, result)
                
            except Exception as e:
                # å‡ºéŒ¯æ™‚ä¿å­˜æª¢æŸ¥é»
                self.save_checkpoint(current_node.id, context)
                raise
        
        # å®Œæˆæ™‚æ¸…ç†æª¢æŸ¥é»
        self.clear_checkpoint()
    
    def save_checkpoint(self, node_id: str, context: Dict):
        """ä¿å­˜æª¢æŸ¥é»"""
        checkpoint = {
            'workflow_id': self.workflow_id,
            'node_id': node_id,
            'context': context,
            'timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜åˆ° Redis
        redis_client.setex(
            f"checkpoint:{self.workflow_id}",
            3600,  # 1å°æ™‚éæœŸ
            json.dumps(checkpoint)
        )
    
    def load_checkpoint(self) -> Optional[Dict]:
        """è¼‰å…¥æª¢æŸ¥é»"""
        checkpoint_data = redis_client.get(f"checkpoint:{self.workflow_id}")
        
        if checkpoint_data:
            return json.loads(checkpoint_data)
        
        return None
    
    def clear_checkpoint(self):
        """æ¸…é™¤æª¢æŸ¥é»"""
        redis_client.delete(f"checkpoint:{self.workflow_id}")
```

---

### å•é¡Œ 2: ä¸¦è¡Œç¯€é»è³‡æºç«¶çˆ­

#### ç—‡ç‹€
- ä¸¦è¡ŒåŸ·è¡Œçš„ç¯€é»äº’ç›¸å½±éŸ¿
- è³‡æ–™åº«é€£æ¥æ± è€—ç›¡
- è³‡æºçˆ­ç”¨å°è‡´æ•ˆèƒ½ä¸‹é™

#### è§£æ±ºæ–¹æ¡ˆ

**æ–¹æ¡ˆ 1: å¯¦ç¾è³‡æºæ± ç®¡ç†**

```python
from contextlib import contextmanager
from queue import Queue
import threading

class ResourcePool:
    """è³‡æºæ± ç®¡ç†å™¨"""
    
    def __init__(self, resource_factory, pool_size=10):
        self.resource_factory = resource_factory
        self.pool_size = pool_size
        self.pool = Queue(maxsize=pool_size)
        self.lock = threading.Lock()
        self.stats = {
            'total_created': 0,
            'total_acquired': 0,
            'total_released': 0,
            'current_usage': 0
        }
        
        # åˆå§‹åŒ–è³‡æºæ± 
        self._initialize_pool()
    
    def _initialize_pool(self):
        """åˆå§‹åŒ–è³‡æºæ± """
        for _ in range(self.pool_size):
            resource = self.resource_factory()
            self.pool.put(resource)
            self.stats['total_created'] += 1
    
    @contextmanager
    def acquire(self, timeout=30):
        """ç²å–è³‡æº"""
        resource = None
        try:
            # å¾æ± ä¸­ç²å–è³‡æº
            resource = self.pool.get(timeout=timeout)
            
            with self.lock:
                self.stats['total_acquired'] += 1
                self.stats['current_usage'] += 1
            
            yield resource
            
        except Queue.Empty:
            raise TimeoutError(f"ç„¡æ³•åœ¨ {timeout} ç§’å…§ç²å–è³‡æº")
        
        finally:
            if resource:
                # è¿”é‚„è³‡æº
                self.pool.put(resource)
                
                with self.lock:
                    self.stats['total_released'] += 1
                    self.stats['current_usage'] -= 1
    
    def get_stats(self) -> Dict:
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        with self.lock:
            return self.stats.copy()

# è³‡æ–™åº«é€£æ¥æ± 
db_pool = ResourcePool(
    resource_factory=lambda: create_db_connection(),
    pool_size=20
)

# åœ¨å·¥ä½œæµç¯€é»ä¸­ä½¿ç”¨
def process_node_with_db(data):
    """ä½¿ç”¨è³‡æ–™åº«é€£æ¥çš„ç¯€é»è™•ç†"""
    with db_pool.acquire(timeout=30) as conn:
        # åŸ·è¡Œè³‡æ–™åº«æ“ä½œ
        result = conn.execute_query(data)
        return result
```

**æ–¹æ¡ˆ 2: å¯¦ç¾ç¯€é»å„ªå…ˆç´šèª¿åº¦**

```python
import heapq
from dataclasses import dataclass, field
from typing import Any

@dataclass(order=True)
class PrioritizedNode:
    """å¸¶å„ªå…ˆç´šçš„ç¯€é»"""
    priority: int
    node: Any = field(compare=False)
    timestamp: float = field(default_factory=time.time, compare=False)

class PriorityScheduler:
    """å„ªå…ˆç´šèª¿åº¦å™¨"""
    
    def __init__(self, max_concurrent=10):
        self.max_concurrent = max_concurrent
        self.queue = []
        self.running = {}
        self.lock = threading.Lock()
    
    def submit(self, node, priority=5):
        """æäº¤ç¯€é»"""
        with self.lock:
            heapq.heappush(
                self.queue,
                PrioritizedNode(priority=priority, node=node)
            )
    
    def schedule(self):
        """èª¿åº¦ç¯€é»"""
        with self.lock:
            # å¦‚æœæœ‰ç©ºé–’æ§½ä½
            while len(self.running) < self.max_concurrent and self.queue:
                # ç²å–æœ€é«˜å„ªå…ˆç´šçš„ç¯€é»
                prioritized = heapq.heappop(self.queue)
                node = prioritized.node
                
                # åŸ·è¡Œç¯€é»
                future = executor.submit(self._execute_node, node)
                self.running[node.id] = future
                
                # è¨­ç½®å®Œæˆå›èª¿
                future.add_done_callback(
                    lambda f, nid=node.id: self._on_complete(nid)
                )
    
    def _execute_node(self, node):
        """åŸ·è¡Œç¯€é»"""
        try:
            return node.execute()
        except Exception as e:
            self.logger.error(f"ç¯€é»åŸ·è¡Œå¤±æ•— {node.id}: {e}")
            raise
    
    def _on_complete(self, node_id: str):
        """ç¯€é»å®Œæˆå›èª¿"""
        with self.lock:
            if node_id in self.running:
                del self.running[node_id]
        
        # è§¸ç™¼æ–°ä¸€è¼ªèª¿åº¦
        self.schedule()
```

---

### å•é¡Œ 3: å·¥ä½œæµç‰ˆæœ¬è¡çª

#### ç—‡ç‹€
- æ›´æ–°å·¥ä½œæµå®šç¾©å¾ŒèˆŠç‰ˆæœ¬ä»åœ¨é‹è¡Œ
- ç‰ˆæœ¬æ··ç”¨å°è‡´éŒ¯èª¤
- ç„¡æ³•å›æ»¾åˆ°ç©©å®šç‰ˆæœ¬

#### è§£æ±ºæ–¹æ¡ˆ

```python
class WorkflowVersionManager:
    """å·¥ä½œæµç‰ˆæœ¬ç®¡ç†å™¨"""
    
    def __init__(self, storage):
        self.storage = storage
    
    def create_version(self, workflow_id: str, definition: Dict) -> str:
        """å‰µå»ºæ–°ç‰ˆæœ¬"""
        # ç”Ÿæˆç‰ˆæœ¬è™Ÿ
        current_version = self.get_latest_version(workflow_id)
        new_version = self._increment_version(current_version)
        
        # ä¿å­˜ç‰ˆæœ¬
        version_data = {
            'workflow_id': workflow_id,
            'version': new_version,
            'definition': definition,
            'created_at': datetime.now(),
            'created_by': self.get_current_user()
        }
        
        self.storage.save_version(version_data)
        
        return new_version
    
    def deploy_version(
        self,
        workflow_id: str,
        version: str,
        strategy: str = 'blue_green'
    ):
        """éƒ¨ç½²ç‰ˆæœ¬"""
        if strategy == 'blue_green':
            self._blue_green_deploy(workflow_id, version)
        elif strategy == 'canary':
            self._canary_deploy(workflow_id, version)
        elif strategy == 'immediate':
            self._immediate_deploy(workflow_id, version)
    
    def _blue_green_deploy(self, workflow_id: str, version: str):
        """è—ç¶ éƒ¨ç½²"""
        # 1. éƒ¨ç½²æ–°ç‰ˆæœ¬åˆ°ç¶ è‰²ç’°å¢ƒ
        self._deploy_to_environment(workflow_id, version, 'green')
        
        # 2. é©—è­‰ç¶ è‰²ç’°å¢ƒ
        if not self._validate_environment(workflow_id, 'green'):
            raise Exception("ç¶ è‰²ç’°å¢ƒé©—è­‰å¤±æ•—")
        
        # 3. åˆ‡æ›æµé‡åˆ°ç¶ è‰²ç’°å¢ƒ
        self._switch_traffic(workflow_id, 'green')
        
        # 4. ä¿æŒè—è‰²ç’°å¢ƒä¸€æ®µæ™‚é–“ä»¥ä¾¿å›æ»¾
        self._schedule_cleanup(workflow_id, 'blue', delay=3600)
    
    def rollback(self, workflow_id: str, target_version: str):
        """å›æ»¾åˆ°æŒ‡å®šç‰ˆæœ¬"""
        # ç²å–ç›®æ¨™ç‰ˆæœ¬å®šç¾©
        definition = self.storage.get_version(workflow_id, target_version)
        
        # ç«‹å³éƒ¨ç½²
        self._immediate_deploy(workflow_id, target_version)
        
        # è¨˜éŒ„å›æ»¾æ“ä½œ
        self._log_rollback(workflow_id, target_version)
```

---

## ğŸ“Š æ•ˆèƒ½ç›£æ§

```yaml
# Prometheus ç›£æ§è¦å‰‡
groups:
  - name: workflow_engine
    rules:
      - alert: HighWorkflowFailureRate
        expr: rate(workflow_failures_total[5m]) / rate(workflow_executions_total[5m]) > 0.1
        annotations:
          summary: "å·¥ä½œæµå¤±æ•—ç‡éé«˜"
      
      - alert: WorkflowQueueBacklog
        expr: workflow_queue_size > 1000
        for: 10m
        annotations:
          summary: "å·¥ä½œæµéšŠåˆ—ç©å£“"
      
      - alert: LongRunningWorkflow
        expr: workflow_duration_seconds > 3600
        annotations:
          summary: "å·¥ä½œæµé‹è¡Œæ™‚é–“éé•·"
```

---

**ç›¸é—œç« ç¯€**:
- [4.7 æ•ˆèƒ½å„ªåŒ–ç­–ç•¥](ch4-7-æ•ˆèƒ½å„ªåŒ–ç­–ç•¥.md)
- [4.10 æœ€ä½³å¯¦è¸æŒ‡å—](ch4-10-æœ€ä½³å¯¦è¸æŒ‡å—.md)
- [â† è¿”å›ç¬¬4ç« é¦–é ](ch4-index.md)

