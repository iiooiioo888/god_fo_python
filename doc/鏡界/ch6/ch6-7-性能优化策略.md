# ç¬¬6ç« ï¼šAIè¼”åŠ©é–‹å‘ç³»çµ± (AI-Assisted Development System)

## 6.7 æ•ˆèƒ½å„ªåŒ–ç­–ç•¥

**[â† è¿”å›ç¬¬6ç« é¦–é ](ch6-index.md)**

---

#### 6.7.1 LLMèª¿ç”¨å„ªåŒ–

1. **ç¼“å­˜æ©Ÿåˆ¶**
   ```python
   class LLMCachingClient:
       """å¸¦ç¼“å­˜çš„LLMå®¢æˆ·ç«¯"""
       
       def __init__(self, llm_client, cache_ttl=3600):
           self.llm_client = llm_client
           self.cache = TTLCache(maxsize=1000, ttl=cache_ttl)
           self.logger = logging.getLogger(__name__)
       
       def generate(self, prompt: str) -> str:
           """ç”Ÿæˆæ–‡æœ¬ï¼Œä½¿ç”¨ç¼“å­˜"""
           # ç”Ÿæˆç¼“å­˜éµï¼ˆæç¤ºè¯çš„å“ˆå¸Œï¼‰
           cache_key = self._generate_cache_key(prompt)
           
           # æ£€æŸ¥ç¼“å­˜
           if cache_key in self.cache:
               self.logger.info("LLM response from cache")
               return self.cache[cache_key]
           
           # èª¿ç”¨LLM
           start_time = time.time()
           response = self.llm_client.generate(prompt)
           duration = time.time() - start_time
           
           # è¨˜éŒ„æŒ‡æ¨™
           self.logger.info("LLM call completed in %.2f seconds", duration)
           
           # ç¼“å­˜ç»“æœ
           self.cache[cache_key] = response
           
           return response
       
       def _generate_cache_key(self, prompt: str) -> str:
           """ç”Ÿæˆç¼“å­˜éµ"""
           return hashlib.md5(prompt.encode('utf-8')).hexdigest()
   ```

2. **æç¤ºè¯å„ªåŒ–**
   ```python
   class PromptOptimizer:
       """æç¤ºè¯å„ªåŒ–å™¨ï¼Œå‡å°‘tokenä½¿ç”¨"""
       
       def optimize(self, prompt: str) -> str:
           """å„ªåŒ–æç¤ºè¯"""
           # 1. ç§»é™¤å†—ä½™ç©ºæ ¼å’Œæ¢è¡Œ
           optimized = re.sub(r'\s+', ' ', prompt).strip()
           
           # 2. ç¼©çŸ­å¸¸è§çŸ­è¯­
           replacements = {
               "please": "plz",
               "information": "info",
               "approximately": "approx",
               "example": "ex",
               "solution": "sol"
           }
           
           for old, new in replacements.items():
               optimized = re.sub(r'\b' + old + r'\b', new, optimized, flags=re.IGNORECASE)
           
           # 3. æˆªæ–­éé•·çš„éƒ¨åˆ†
           if len(optimized) > 2000:
               # ä¿ç•™é–‹å¤´å’Œç»“å°¾
               optimized = optimized[:1000] + "...[TRUNCATED]..." + optimized[-1000:]
           
           return optimized
   ```

3. **æ‰¹è™•ç†è¯·æ±‚**
   ```python
   class BatchLLMClient:
       """æ‰¹è™•ç†LLMå®¢æˆ·ç«¯"""
       
       def __init__(self, llm_client, batch_size=5, max_wait=2.0):
           self.llm_client = llm_client
           self.batch_size = batch_size
           self.max_wait = max_wait
           self.request_queue = []
           self.lock = threading.Lock()
           self.thread = threading.Thread(target=self._process_queue, daemon=True)
           self.thread.start()
       
       def _process_queue(self):
           """è™•ç†è¯·æ±‚éšŠåˆ—"""
           while True:
               with self.lock:
                   if len(self.request_queue) >= self.batch_size or (self.request_queue and time.time() - self.request_queue[0][2] > self.max_wait):
                       batch = self.request_queue[:self.batch_size]
                       self.request_queue = self.request_queue[self.batch_size:]
                   else:
                       batch = None
               
               if batch:
                   self._process_batch(batch)
               
               time.sleep(0.1)
       
       def _process_batch(self, batch):
           """è™•ç†ä¸€æ‰¹è¯·æ±‚"""
           prompts = [item[0] for item in batch]
           callbacks = [item[1] for item in batch]
           
           try:
               # èª¿ç”¨LLMè™•ç†æ‰¹é‡è¯·æ±‚
               responses = self.llm_client.generate_batch(prompts)
               
               # èª¿ç”¨å›è°ƒ
               for callback, response in zip(callbacks, responses):
                   callback(response)
           except Exception as e:
               for callback in callbacks:
                   callback(None, str(e))
       
       def generate(self, prompt: str, callback: Callable):
           """å¼‚æ­¥ç”Ÿæˆæ–‡æœ¬"""
           with self.lock:
               self.request_queue.append((prompt, callback, time.time()))
   ```

#### 6.7.2 ä¸Šä¸‹æ–‡ç®¡ç†å„ªåŒ–

1. **ä¸Šä¸‹æ–‡å‹ç¼©**
   ```python
   class ContextCompressor:
       """ä¸Šä¸‹æ–‡å‹ç¼©å™¨ï¼Œå‡å°‘ä¸Šä¸‹æ–‡tokenæ•¸é‡"""
       
       def compress(self, context: Dict, max_tokens: int = 2000) -> Dict:
           """
           å‹ç¼©ä¸Šä¸‹æ–‡åˆ°æŒ‡å®štokené™åˆ¶
           
           :param context: åŸå§‹ä¸Šä¸‹æ–‡
           :param max_tokens: æœ€å¤§tokenæ•¸
           :return: å‹ç¼©å¾Œçš„ä¸Šä¸‹æ–‡
           """
           # 1. è®¡ç®—ç•¶å‰tokenæ•¸
           current_tokens = self._estimate_tokens(context)
           
           # 2. å¦‚æœä¸éœ€è¦å‹ç¼©ï¼Œç›´æ¥è¿”å›
           if current_tokens <= max_tokens:
               return context
           
           # 3. æŒ‰é‡è¦æ€§æ’åº
           important_keys = ["error_log", "user_request", "recent_messages"]
           less_important_keys = [k for k in context.keys() if k not in important_keys]
           
           # 4. ä¼˜å…ˆä¿ç•™é‡è¦è³‡è¨Š
           compressed = {k: context[k] for k in important_keys if k in context}
           
           # 5. é€æ­¥æ·»åŠ æ¬¡è¦è³‡è¨Šç›´åˆ°è¾¾åˆ°tokené™åˆ¶
           remaining_tokens = max_tokens - self._estimate_tokens(compressed)
           
           for key in less_important_keys:
               if remaining_tokens <= 0:
                   break
               
               # å‹ç¼©å•å€‹å­—æ®µ
               compressed_value = self._compress_field(context[key], remaining_tokens)
               compressed[key] = compressed_value
               
               # æ›´æ–°å‰©ä½™token
               remaining_tokens -= self._estimate_tokens({key: compressed_value})
           
           return compressed
       
       def _compress_field(self, value: Any, max_tokens: int) -> Any:
           """å‹ç¼©å•å€‹å­—æ®µ"""
           if isinstance(value, str):
               # ç®€å•å¯¦ç¾ï¼šæˆªæ–­å­—ç¬¦ä¸²
               tokens = self._estimate_tokens(value)
               if tokens > max_tokens:
                   # ä¿ç•™é–‹å¤´å’Œç»“å°¾
                   return value[:max_tokens//2] + "...[TRUNCATED]..." + value[-max_tokens//2:]
               return value
           
           elif isinstance(value, list):
               # ä¿ç•™å‰Nå€‹å…ƒç´ 
               if len(value) > 5:
                   return value[:5]
               return value
           
           elif isinstance(value, dict):
               # ä¿ç•™æœ€é‡è¦çš„å­—æ®µ
               important_fields = ["root_cause", "solutions", "error_message"]
               return {k: v for k, v in value.items() if k in important_fields}
           
           return value
       
       def _estimate_tokens(self, obj: Any) -> int:
           """ä¼°è®¡ç‰©ä»¶çš„tokenæ•¸é‡"""
           if isinstance(obj, str):
               # ç®€å•ä¼°è®¡ï¼šæ¯å€‹å­—ç¬¦çº¦0.25å€‹token
               return max(1, len(obj) // 4)
           elif isinstance(obj, list):
               return sum(self._estimate_tokens(item) for item in obj)
           elif isinstance(obj, dict):
               return sum(self._estimate_tokens(v) for v in obj.values())
           return 1
   ```

2. **ä¸Šä¸‹æ–‡æ‘˜è¦**
   ```python
   class ContextSummarizer:
       """ä¸Šä¸‹æ–‡æ‘˜è¦ç”Ÿæˆå™¨"""
       
       def __init__(self, llm_client):
           self.llm_client = llm_client
       
       def summarize(self, context: Dict) -> str:
           """
           ç”Ÿæˆä¸Šä¸‹æ–‡æ‘˜è¦
           
           :param context: åŸå§‹ä¸Šä¸‹æ–‡
           :return: ä¸Šä¸‹æ–‡æ‘˜è¦
           """
           # æ§‹å»ºæ‘˜è¦æç¤ºè¯
           prompt = f"""
           è¯·å°‡ä»¥ä¸‹å°è¯ä¸Šä¸‹æ–‡æ€»ç»“ç‚ºç®€æ´çš„æ‘˜è¦ï¼Œä¿ç•™é—œéµè³‡è¨Šï¼Œä¸è¶…é100å€‹è¯ï¼š

           {json.dumps(context, indent=2)}

           æ‘˜è¦:
           """
           
           # ç”Ÿæˆæ‘˜è¦
           summary = self.llm_client.generate(prompt)
           
           # æ¸…ç†ç»“æœ
           return summary.strip()
   ```

#### 6.7.3 è³‡æºç®¡ç†ç­–ç•¥

1. **è³‡æºé…é¢ç®¡ç†**
   ```python
   class ResourceQuotaManager:
       """è³‡æºé…é¢ç®¡ç†å™¨"""
       
       def __init__(self, db: Database, config: Config):
           self.db = db
           self.config = config
           self.logger = logging.getLogger(__name__)
           self.quota_cache = TTLCache(maxsize=1000, ttl=300)  # 5åˆ†é’Ÿç¼“å­˜
       
       def check_quota(
           self,
           user_id: str,
           resource_type: str,
           amount: int
       ) -> Tuple[bool, str]:
           """
           æ£€æŸ¥è³‡æºé…é¢
           
           :param user_id: ç”¨æˆ¶ID
           :param resource_type: è³‡æºé¡å‹ (llm_calls, processing_timeç­‰)
           :param amount: è¯·æ±‚çš„è³‡æºé‡
           :return: (æ˜¯å¦å…è®¸, æ¶ˆæ¯)
           """
           # 1. ç²å–ç”¨æˆ¶é…é¢
           quota = self._get_user_quota(user_id)
           
           # 2. ç²å–å·²ç”¨è³‡æº
           used = self._get_used_resources(user_id, resource_type)
           
           # 3. æ£€æŸ¥æ˜¯å¦è¶…å‡ºé…é¢
           if used + amount > quota[resource_type]:
               return False, f"è¶…å‡º{resource_type}é…é¢ ({used}/{quota[resource_type]})"
           
           # 4. é æ‰£è³‡æº
           self._reserve_resources(user_id, resource_type, amount)
           
           return True, f"å·²é ç•™{amount}å•ä½{resource_type}"
       
       def _get_user_quota(self, user_id: str) -> Dict:
           """ç²å–ç”¨æˆ¶é…é¢"""
           # å¾ç¼“å­˜ç²å–
           cache_key = f"{user_id}:quota"
           if cache_key in self.quota_cache:
               return self.quota_cache[cache_key]
           
           # å¾è³‡æ–™åº«ç²å–
           sql = """
           SELECT llm_calls, processing_time, storage 
           FROM user_quotas 
           WHERE user_id = %(user_id)s
           """
           row = self.db.fetchone(sql, {"user_id": user_id})
           
           if not row:
               # é»˜è®¤é…é¢
               quota = {
                   "llm_calls": self.config.default_llm_calls,
                   "processing_time": self.config.default_processing_time,
                   "storage": self.config.default_storage
               }
           else:
               quota = {
                   "llm_calls": row["llm_calls"],
                   "processing_time": row["processing_time"],
                   "storage": row["storage"]
               }
           
           # ç¼“å­˜ç»“æœ
           self.quota_cache[cache_key] = quota
           return quota
       
       def _get_used_resources(self, user_id: str, resource_type: str) -> int:
           """ç²å–å·²ç”¨è³‡æº"""
           # å¯¦ç¾è³‡æºä½¿ç”¨ç»Ÿè®¡
           # é€™é‡Œç®€åŒ–ç‚ºè¿”å›0
           return 0
       
       def _reserve_resources(
           self,
           user_id: str,
           resource_type: str,
           amount: int
       ):
           """é æ‰£è³‡æº"""
           # å¯¦ç¾è³‡æºé ç•™
           pass
   ```

---

## ğŸ“‘ ç›¸é—œç« ç¯€

| å‰åº | ç•¶å‰ | å¾ŒçºŒ |
|-----|------|------|
| [6.6 APIè©³ç´°è¦ç¯„](ch6-6-APIè©³ç´°è¦ç¯„.md) | **6.7 æ•ˆèƒ½å„ªåŒ–ç­–ç•¥** | [6.8 å®‰å…¨è€ƒæ…®](ch6-8-å®‰å…¨è€ƒæ…®.md) |

**å¿«é€Ÿéˆæ¥ï¼š**
- [6.6 APIè©³ç´°è¦ç¯„](ch6-6-APIè©³ç´°è¦ç¯„.md)
- [6.8 å®‰å…¨è€ƒæ…®](ch6-8-å®‰å…¨è€ƒæ…®.md)
- [â† è¿”å›ç¬¬6ç« é¦–é ](ch6-index.md)
